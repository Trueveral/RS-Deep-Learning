{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torchvision import datasets, transforms\n",
    "# import Datasets\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "# import plotting tools\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from PIL import Image\n",
    "from tqdm import tqdm\n",
    "import cv2 as cv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\n",
    "    'cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
    "train_data_path = '../../../Datasets/Model/train'\n",
    "test_data_path = '../../../Datasets/Model/test'\n",
    "image_path = '../../../Datasets/Model/Images'\n",
    "data_save_path = '../../../Save/AdvancedDeliverables/DVBPR'\n",
    "analytics_path = '../../../Analysis/AdvancedDeliverables'\n",
    "lr = 0.005\n",
    "batch_size = 32\n",
    "max_epoch = 100\n",
    "\n",
    "train_users = np.load(f'{train_data_path}/train_users.npy')\n",
    "train_items = np.load(f'{train_data_path}/train_items.npy')\n",
    "train_ratings = np.load(f'{train_data_path}/train_ratings.npy')\n",
    "\n",
    "test_users = np.load(f'{test_data_path}/test_users.npy')\n",
    "test_items = np.load(f'{test_data_path}/test_items.npy')\n",
    "test_ratings = np.load(f'{test_data_path}/test_ratings.npy')\n",
    "\n",
    "# images = []\n",
    "\n",
    "# items = np.unique(np.concatenate((train_items, test_items)))\n",
    "# for item in tqdm(items):\n",
    "\n",
    "def load_image(item):\n",
    "  try: \n",
    "    img = Image.open(f'{image_path}/{item}.jpg')\n",
    "    img = img.resize((224, 224))\n",
    "    # normalize the image\n",
    "    img = np.array(img) / 255\n",
    "    # convert the image to list\n",
    "    img = img.tolist()\n",
    "  except:\n",
    "    # use the black number as the image if the image is not found\n",
    "    img = torch.zeros(224, 224, 3)\n",
    "    # convert the image to list\n",
    "    img = img.tolist()\n",
    "  return img\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''function sample: input a user id, and return an item the user has rated, and another item the user has not rated'''\n",
    "def sample_user(user, users, items, ratings):\n",
    "    user_indices = np.where(users == user)[0]\n",
    "    user_index = np.random.choice(user_indices)\n",
    "    user_item = items[user_index]\n",
    "    user_rating = ratings[user_index]\n",
    "    # load the image of the user_item\n",
    "    user_image = load_image(user_item)\n",
    "\n",
    "    # select an item the user has not rated\n",
    "    other_items = np.where(users != user)[0]\n",
    "    other_item_index = np.random.choice(other_items)\n",
    "    other_item = items[other_item_index]\n",
    "    other_rating = ratings[other_item_index]\n",
    "    # load the image of the other_item\n",
    "    other_image = load_image(other_item)\n",
    "\n",
    "    return user, user_item, user_rating, user_image, other_item, other_rating, other_image\n",
    "\n",
    "def get_batch(users, items, ratings, batch_size=32, fixed_users_set = None):\n",
    "  # get a batch of users\n",
    "  if fixed_users_set is None:\n",
    "    batch_users = np.random.choice(users, size=batch_size)\n",
    "  else:\n",
    "    batch_users = fixed_users_set\n",
    "  # get the batch of samples\n",
    "  batch = [list(sample_user(user, users, items, ratings)) for user in batch_users]\n",
    "  # transform each column of the batch to a pytorch tensor\n",
    "  batch = [torch.tensor(sample) for sample in zip(*batch)]\n",
    "  return batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading: \"https://github.com/pytorch/vision/zipball/v0.10.0\" to /Users/zhouzihan/.cache/torch/hub/v0.10.0.zip\n"
     ]
    }
   ],
   "source": [
    "model = torch.hub.load('pytorch/vision:v0.10.0', 'resnet18', pretrained=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ResNet(\n",
       "  (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
       "  (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (relu): ReLU(inplace=True)\n",
       "  (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
       "  (layer1): Sequential(\n",
       "    (0): BasicBlock(\n",
       "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (1): BasicBlock(\n",
       "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "  )\n",
       "  (layer2): Sequential(\n",
       "    (0): BasicBlock(\n",
       "      (conv1): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "        (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): BasicBlock(\n",
       "      (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "  )\n",
       "  (layer3): Sequential(\n",
       "    (0): BasicBlock(\n",
       "      (conv1): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): BasicBlock(\n",
       "      (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "  )\n",
       "  (layer4): Sequential(\n",
       "    (0): BasicBlock(\n",
       "      (conv1): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): BasicBlock(\n",
       "      (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "  )\n",
       "  (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))\n",
       "  (fc): Linear(in_features=512, out_features=1000, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a Siamese CNN network using AlexNet architecture\n",
    "def init_weights(m):\n",
    "    if isinstance(m, nn.Conv2d):\n",
    "        torch.nn.init.kaiming_normal_(m.weight)\n",
    "        m.bias.data.fill_(0.01)\n",
    "    elif isinstance(m, nn.Linear):\n",
    "        torch.nn.init.kaiming_normal_(m.weight)\n",
    "        m.bias.data.fill_(0.01)\n",
    "\n",
    "class FM(nn.Module):\n",
    "    def __init__(self, latent_dim, fea_num):\n",
    "        super().__init__()\n",
    "\n",
    "        self.latent_dim = latent_dim\n",
    "        self.w0 = nn.Parameter(torch.zeros([1, ]))\n",
    "        self.w1 = nn.Parameter(torch.rand([fea_num, 1]))\n",
    "        self.w2 = nn.Parameter(torch.rand([fea_num, latent_dim]))\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        # inputs = inputs.long()\n",
    "        first_order = self.w0 + torch.mm(inputs, self.w1)\n",
    "        second_order = 1/2 * torch.sum(\n",
    "            torch.pow(torch.mm(inputs, self.w2), 2) -\n",
    "            torch.mm(torch.pow(inputs, 2), torch.pow(self.w2, 2)),\n",
    "\n",
    "            dim=1,\n",
    "            keepdim=True\n",
    "        )\n",
    "\n",
    "        result = first_order + second_order\n",
    "        # flatten the result\n",
    "        result = result.squeeze(1)\n",
    "\n",
    "        return result\n",
    "\n",
    "# AlexNet architecture\n",
    "class SiameseCNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(SiameseCNN, self).__init__()\n",
    "        # input size is 3 x 224 x 224\n",
    "        self.net = nn.Sequential(\n",
    "          nn.Conv2d(3, 64, kernel_size=11, stride=4, padding=2),\n",
    "          # size: 64 x 55 x 55\n",
    "          nn.ReLU(inplace=True),\n",
    "          nn.MaxPool2d(kernel_size=3, stride=2),\n",
    "          # size: 64 x 27 x 27\n",
    "          nn.Conv2d(64, 192, kernel_size=5, padding=2),\n",
    "          # size: 192 x 27 x 27\n",
    "          nn.ReLU(inplace=True),\n",
    "          nn.MaxPool2d(kernel_size=3, stride=2),\n",
    "          # size: 192 x 13 x 13\n",
    "          nn.Conv2d(192, 384, kernel_size=3, padding=1),\n",
    "          # size: 384 x 13 x 13\n",
    "          nn.ReLU(inplace=True),\n",
    "          nn.Conv2d(384, 256, kernel_size=3, padding=1),\n",
    "          # size: 256 x 13 x 13\n",
    "          nn.ReLU(inplace=True),\n",
    "          nn.Conv2d(256, 256, kernel_size=3, padding=1),\n",
    "          # size: 256 x 13 x 13\n",
    "          nn.ReLU(inplace=True),\n",
    "          nn.MaxPool2d(kernel_size=3, stride=2),\n",
    "          # size: 256 x 6 x 6\n",
    "        )\n",
    "\n",
    "        self.fc = nn.Sequential(\n",
    "          nn.Linear(256 * 6 * 6, 4096),\n",
    "          nn.ReLU(inplace=True),\n",
    "          nn.Dropout(0.5),\n",
    "          nn.Linear(4096, 4096),\n",
    "          nn.ReLU(inplace=True),\n",
    "          nn.Dropout(0.5),\n",
    "          nn.Linear(4096, 100),\n",
    "        )\n",
    "\n",
    "    def forward(self, x1, x2):\n",
    "        x1 = x1.permute(0, 3, 1, 2)\n",
    "        x2 = x2.permute(0, 3, 1, 2)\n",
    "        output1 = self.net(x1)\n",
    "        output1 = output1.view(output1.size(0), -1)\n",
    "        output1 = self.fc(output1)\n",
    "        output2 = self.net(x2)\n",
    "        output2 = output2.view(output2.size(0), -1)\n",
    "        output2 = self.fc(output2)\n",
    "        return output1, output2\n",
    "\n",
    "DVBPR = SiameseCNN().apply(init_weights).to(device)\n",
    "predictor = FM(latent_dim=10, fea_num=100).to(device)\n",
    "\n",
    "optimiser_predictor = torch.optim.Adam(predictor.parameters(), lr=lr, weight_decay=0.0001)\n",
    "optimiser_DVBPR = optim.Adam(DVBPR.parameters(), lr=lr, weight_decay=0.001)\n",
    "\n",
    "def save_training(path = f'{data_save_path}/'):\n",
    "    torch.save(DVBPR.state_dict(), path + 'DVBPR.pt')\n",
    "    torch.save(predictor.state_dict(), path + 'predictor.pt')\n",
    "\n",
    "def load_training(path = f'{data_save_path}/'):\n",
    "    DVBPR.load_state_dict(torch.load(path + 'DVBPR.pt'))\n",
    "    predictor.load_state_dict(torch.load(path + 'predictor.pt'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluation(test_batch_size = 32):\n",
    "  test_users_unique = list(set(test_users))\n",
    "  mse_arr = []\n",
    "\n",
    "  for i in tqdm(range(0, len(test_users_unique), test_batch_size)):\n",
    "    test_users_batch = test_users_unique[i:i+test_batch_size]\n",
    "    batch_test = get_batch(test_users, test_items, test_ratings, fixed_users_set=test_users_batch)\n",
    "    user, user_item, user_rating, user_image, other_item, other_rating, other_image = batch_test\n",
    "    user, user_item, user_rating, user_image, other_item, other_rating, other_image = user.to(device), user_item.to(device), user_rating.to(device), user_image.to(device), other_item.to(device), other_rating.to(device), other_image.to(device)\n",
    "\n",
    "    img_result_1, img_result_2 = DVBPR(user_image, other_image)\n",
    "\n",
    "    pred_1 = predictor(img_result_1)\n",
    "    pred_2 = predictor(img_result_2)\n",
    "\n",
    "    mse_1 = torch.mean(torch.pow(pred_1 - user_rating, 2))\n",
    "    mse_2 = torch.mean(torch.pow(pred_2 - other_rating, 2))\n",
    "\n",
    "    mse_avg = (mse_1 + mse_2) / 2\n",
    "    mse_arr.append(mse_avg.item())\n",
    "  \n",
    "  return np.mean(mse_arr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating on test set...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 3/3609 [00:10<3:24:24,  3.40s/it]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/Users/zhouzihan/Documents/DurhamWorks/Y3/Codes/y3project.nosync/Models/AdvancedDeliverables/DVBPR/DVBPR.ipynb Cell 8\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/zhouzihan/Documents/DurhamWorks/Y3/Codes/y3project.nosync/Models/AdvancedDeliverables/DVBPR/DVBPR.ipynb#ch0000009?line=0'>1</a>\u001b[0m best_rmse \u001b[39m=\u001b[39m \u001b[39m1e6\u001b[39m\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/zhouzihan/Documents/DurhamWorks/Y3/Codes/y3project.nosync/Models/AdvancedDeliverables/DVBPR/DVBPR.ipynb#ch0000009?line=2'>3</a>\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m'\u001b[39m\u001b[39mEvaluating on test set...\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/zhouzihan/Documents/DurhamWorks/Y3/Codes/y3project.nosync/Models/AdvancedDeliverables/DVBPR/DVBPR.ipynb#ch0000009?line=4'>5</a>\u001b[0m best_rmse \u001b[39m=\u001b[39m evaluation()\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/zhouzihan/Documents/DurhamWorks/Y3/Codes/y3project.nosync/Models/AdvancedDeliverables/DVBPR/DVBPR.ipynb#ch0000009?line=6'>7</a>\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m'\u001b[39m\u001b[39mBaseline RMSE: \u001b[39m\u001b[39m{\u001b[39;00mbest_rmse\u001b[39m}\u001b[39;00m\u001b[39m'\u001b[39m)\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/zhouzihan/Documents/DurhamWorks/Y3/Codes/y3project.nosync/Models/AdvancedDeliverables/DVBPR/DVBPR.ipynb#ch0000009?line=8'>9</a>\u001b[0m \u001b[39m'''training the model'''\u001b[39;00m\n",
      "\u001b[1;32m/Users/zhouzihan/Documents/DurhamWorks/Y3/Codes/y3project.nosync/Models/AdvancedDeliverables/DVBPR/DVBPR.ipynb Cell 8\u001b[0m in \u001b[0;36mevaluation\u001b[0;34m(test_batch_size)\u001b[0m\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/zhouzihan/Documents/DurhamWorks/Y3/Codes/y3project.nosync/Models/AdvancedDeliverables/DVBPR/DVBPR.ipynb#ch0000009?line=7'>8</a>\u001b[0m user, user_item, user_rating, user_image, other_item, other_rating, other_image \u001b[39m=\u001b[39m batch_test\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/zhouzihan/Documents/DurhamWorks/Y3/Codes/y3project.nosync/Models/AdvancedDeliverables/DVBPR/DVBPR.ipynb#ch0000009?line=8'>9</a>\u001b[0m user, user_item, user_rating, user_image, other_item, other_rating, other_image \u001b[39m=\u001b[39m user\u001b[39m.\u001b[39mto(device), user_item\u001b[39m.\u001b[39mto(device), user_rating\u001b[39m.\u001b[39mto(device), user_image\u001b[39m.\u001b[39mto(device), other_item\u001b[39m.\u001b[39mto(device), other_rating\u001b[39m.\u001b[39mto(device), other_image\u001b[39m.\u001b[39mto(device)\n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/zhouzihan/Documents/DurhamWorks/Y3/Codes/y3project.nosync/Models/AdvancedDeliverables/DVBPR/DVBPR.ipynb#ch0000009?line=10'>11</a>\u001b[0m img_result_1, img_result_2 \u001b[39m=\u001b[39m DVBPR(user_image, other_image)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/zhouzihan/Documents/DurhamWorks/Y3/Codes/y3project.nosync/Models/AdvancedDeliverables/DVBPR/DVBPR.ipynb#ch0000009?line=12'>13</a>\u001b[0m pred_1 \u001b[39m=\u001b[39m predictor(img_result_1)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/zhouzihan/Documents/DurhamWorks/Y3/Codes/y3project.nosync/Models/AdvancedDeliverables/DVBPR/DVBPR.ipynb#ch0000009?line=13'>14</a>\u001b[0m pred_2 \u001b[39m=\u001b[39m predictor(img_result_2)\n",
      "File \u001b[0;32m~/miniconda3/envs/y3project/lib/python3.8/site-packages/torch/nn/modules/module.py:1131\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1127\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1128\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1129\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1130\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1131\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1132\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1133\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "\u001b[1;32m/Users/zhouzihan/Documents/DurhamWorks/Y3/Codes/y3project.nosync/Models/AdvancedDeliverables/DVBPR/DVBPR.ipynb Cell 8\u001b[0m in \u001b[0;36mSiameseCNN.forward\u001b[0;34m(self, x1, x2)\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/zhouzihan/Documents/DurhamWorks/Y3/Codes/y3project.nosync/Models/AdvancedDeliverables/DVBPR/DVBPR.ipynb#ch0000009?line=78'>79</a>\u001b[0m output1 \u001b[39m=\u001b[39m output1\u001b[39m.\u001b[39mview(output1\u001b[39m.\u001b[39msize(\u001b[39m0\u001b[39m), \u001b[39m-\u001b[39m\u001b[39m1\u001b[39m)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/zhouzihan/Documents/DurhamWorks/Y3/Codes/y3project.nosync/Models/AdvancedDeliverables/DVBPR/DVBPR.ipynb#ch0000009?line=79'>80</a>\u001b[0m output1 \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mfc(output1)\n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/zhouzihan/Documents/DurhamWorks/Y3/Codes/y3project.nosync/Models/AdvancedDeliverables/DVBPR/DVBPR.ipynb#ch0000009?line=80'>81</a>\u001b[0m output2 \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mnet(x2)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/zhouzihan/Documents/DurhamWorks/Y3/Codes/y3project.nosync/Models/AdvancedDeliverables/DVBPR/DVBPR.ipynb#ch0000009?line=81'>82</a>\u001b[0m output2 \u001b[39m=\u001b[39m output2\u001b[39m.\u001b[39mview(output2\u001b[39m.\u001b[39msize(\u001b[39m0\u001b[39m), \u001b[39m-\u001b[39m\u001b[39m1\u001b[39m)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/zhouzihan/Documents/DurhamWorks/Y3/Codes/y3project.nosync/Models/AdvancedDeliverables/DVBPR/DVBPR.ipynb#ch0000009?line=82'>83</a>\u001b[0m output2 \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mfc(output2)\n",
      "File \u001b[0;32m~/miniconda3/envs/y3project/lib/python3.8/site-packages/torch/nn/modules/module.py:1131\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1127\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1128\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1129\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1130\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1131\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1132\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1133\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/miniconda3/envs/y3project/lib/python3.8/site-packages/torch/nn/modules/container.py:139\u001b[0m, in \u001b[0;36mSequential.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    137\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m):\n\u001b[1;32m    138\u001b[0m     \u001b[39mfor\u001b[39;00m module \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m:\n\u001b[0;32m--> 139\u001b[0m         \u001b[39minput\u001b[39m \u001b[39m=\u001b[39m module(\u001b[39minput\u001b[39;49m)\n\u001b[1;32m    140\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39minput\u001b[39m\n",
      "File \u001b[0;32m~/miniconda3/envs/y3project/lib/python3.8/site-packages/torch/nn/modules/module.py:1131\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1127\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1128\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1129\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1130\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1131\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1132\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1133\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/miniconda3/envs/y3project/lib/python3.8/site-packages/torch/nn/modules/conv.py:459\u001b[0m, in \u001b[0;36mConv2d.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    458\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m: Tensor) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tensor:\n\u001b[0;32m--> 459\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_conv_forward(\u001b[39minput\u001b[39;49m, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mweight, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mbias)\n",
      "File \u001b[0;32m~/miniconda3/envs/y3project/lib/python3.8/site-packages/torch/nn/modules/conv.py:455\u001b[0m, in \u001b[0;36mConv2d._conv_forward\u001b[0;34m(self, input, weight, bias)\u001b[0m\n\u001b[1;32m    451\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpadding_mode \u001b[39m!=\u001b[39m \u001b[39m'\u001b[39m\u001b[39mzeros\u001b[39m\u001b[39m'\u001b[39m:\n\u001b[1;32m    452\u001b[0m     \u001b[39mreturn\u001b[39;00m F\u001b[39m.\u001b[39mconv2d(F\u001b[39m.\u001b[39mpad(\u001b[39minput\u001b[39m, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_reversed_padding_repeated_twice, mode\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpadding_mode),\n\u001b[1;32m    453\u001b[0m                     weight, bias, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstride,\n\u001b[1;32m    454\u001b[0m                     _pair(\u001b[39m0\u001b[39m), \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdilation, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mgroups)\n\u001b[0;32m--> 455\u001b[0m \u001b[39mreturn\u001b[39;00m F\u001b[39m.\u001b[39;49mconv2d(\u001b[39minput\u001b[39;49m, weight, bias, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mstride,\n\u001b[1;32m    456\u001b[0m                 \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mpadding, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdilation, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mgroups)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "best_rmse = 1e6\n",
    "\n",
    "print('Evaluating on test set...')\n",
    "\n",
    "best_rmse = evaluation()\n",
    "\n",
    "print(f'Baseline RMSE: {best_rmse}')\n",
    "\n",
    "'''training the model'''\n",
    "\n",
    "loss_arr = []\n",
    "rmse_arr = []\n",
    "\n",
    "rmse_arr.append(best_rmse)\n",
    "\n",
    "for epoch in range (max_epoch):\n",
    "  for i in tqdm(range(1000)):\n",
    "    user, user_item, user_rating, user_image, other_item, other_rating, other_image = get_batch(train_users, train_items, train_ratings, batch_size=32)\n",
    "    user, user_item, user_rating, user_image, other_item, other_rating, other_image = user.to(device), user_item.to(device), user_rating.to(device), user_image.to(device), other_item.to(device), other_rating.to(device), other_image.to(device)\n",
    "    img_result_1, img_result_2 = DVBPR(user_image, other_image)\n",
    "\n",
    "    rating_diff = user_rating - other_rating\n",
    "    pred_diff = predictor(img_result_1) - predictor(img_result_2)\n",
    "    loss = rating_diff - pred_diff\n",
    "    # average loss\n",
    "    loss = torch.mean(loss)\n",
    "\n",
    "    optimiser_predictor.zero_grad()\n",
    "    optimiser_DVBPR.zero_grad()\n",
    "    loss.backward()\n",
    "    optimiser_predictor.step()\n",
    "    optimiser_DVBPR.step()\n",
    "\n",
    "  with torch.no_grad():\n",
    "    loss_arr.append(loss.item())\n",
    "    rmse = evaluation()\n",
    "    print(f'Epoch {epoch+1} RMSE: {rmse}')\n",
    "    rmse_arr.append(rmse)\n",
    "    if rmse < best_rmse:\n",
    "      best_rmse = rmse\n",
    "      save_training()\n",
    "      print('Saved model')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.11 ('y3project')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.11"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "9af667c24e890afcb270fe85ca560e1aa788703788da50f20e13467161b45801"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
