{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "from transformers import AlbertTokenizerFast, AlbertModel\n",
    "# from gensim.models import Word2Vec\n",
    "# from nets import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://zh-v2.d2l.ai/chapter_natural-language-processing-applications/sentiment-analysis-cnn.html\n",
    "class GloveEmbedding:\n",
    "    \"\"\"Token Embedding.\"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        self.idx_to_token, self.idx_to_vec = self._load_embedding()\n",
    "        self.unknown_idx = 0\n",
    "        # a dictionary like {'best':1} from idx_to_token\n",
    "        self.token_to_idx = {\n",
    "            token: idx for idx, token in enumerate(self.idx_to_token)}\n",
    "\n",
    "    def _load_embedding(self):\n",
    "        idx_to_token, idx_to_vec = ['<unk>'], []\n",
    "        data_dir = '../../Datasets/Model/nlp/'\n",
    "        # GloVe website: https://nlp.stanford.edu/projects/glove/\n",
    "        # fastText website: https://fasttext.cc/\n",
    "        with open(os.path.join(data_dir, 'vec.txt'), encoding='gb18030', errors='ignore') as f:\n",
    "            for line in f:\n",
    "                elems = line.rstrip().split(' ')\n",
    "                # Skip header information, such as the top row in fastText\n",
    "                # structure: hello  [0.1, 0.2, 0.3, ...]\n",
    "                token, elems = elems[0], [float(elem) for elem in elems[1:]]\n",
    "                if len(elems) > 1:\n",
    "                    idx_to_token.append(token)\n",
    "                    idx_to_vec.append(elems)\n",
    "        idx_to_vec = [[0] * len(idx_to_vec[0])] + idx_to_vec\n",
    "        return idx_to_token, torch.tensor(idx_to_vec)\n",
    "\n",
    "    def __getitem__(self, tokens):\n",
    "        indices = [\n",
    "            self.token_to_idx.get(token, self.unknown_idx)\n",
    "            for token in tokens]\n",
    "        vecs = self.idx_to_vec[torch.tensor(indices)]\n",
    "        return vecs\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.idx_to_token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''CONSTANTS'''\n",
    "batch_size = 32\n",
    "max_epoch = 40\n",
    "lr = 0.001\n",
    "\n",
    "LD = 30\n",
    "LW = 20\n",
    "\n",
    "device = torch.device(\n",
    "    'cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
    "train_data_path = '../../Datasets/Model/train'\n",
    "test_data_path = '../../Datasets/Model/test'\n",
    "image_path = '../../Datasets/Images'\n",
    "data_save_path = '../../Save/Experiment5'\n",
    "analytics_path = '../../Analysis/Experiment5'\n",
    "nlp_path = '../../Datasets/Model/nlp'\n",
    "\n",
    "'''NLP'''\n",
    "# load word2vec models from nlp_path\n",
    "# model_review = Word2Vec.load(f'{nlp_path}/model_review.model')\n",
    "# model_title = Word2Vec.load(f'{nlp_path}/model_title.model')\n",
    "\n",
    "# get the vocabulary of reviews titles\n",
    "# vocab_review = model_review.wv.index_to_key\n",
    "# vocab_title = model_title.wv.index_to_key\n",
    "\n",
    "# # get the vectors of reviews and titles\n",
    "# vecs_review = torch.tensor(model_review.wv[vocab_review])\n",
    "# vecs_title = torch.tensor(model_title.wv[vocab_title])\n",
    "\n",
    "# get the glove embeddings\n",
    "# print(\"Loading glove embeddings...\")\n",
    "tokens = []\n",
    "with open('../../Datasets/Model/nlp/tokens.txt', 'r') as f:\n",
    "  for line in f:\n",
    "    tokens.append(line.strip())\n",
    "glove = GloveEmbedding()\n",
    "\n",
    "\n",
    "'''ANALYTICS'''\n",
    "\n",
    "rmse_arr = []\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_users = np.load(f'{train_data_path}/train_users.npy')\n",
    "train_items = np.load(f'{train_data_path}/train_items.npy')\n",
    "train_ratings = np.load(f'{train_data_path}/train_ratings.npy')\n",
    "train_reviews = np.load(\n",
    "    f'{train_data_path}/train_reviews.npy', allow_pickle=True)\n",
    "train_descriptions = np.load(\n",
    "    f'{train_data_path}/train_descriptions.npy', allow_pickle=True)\n",
    "train_titles = np.load(\n",
    "    f'{train_data_path}/train_titles.npy', allow_pickle=True)\n",
    "train_prices = np.load(f'{train_data_path}/train_prices.npy')\n",
    "train_categories = np.load(f'{train_data_path}/train_categories.npy')\n",
    "\n",
    "test_users = np.load(f'{test_data_path}/test_users.npy')\n",
    "test_items = np.load(f'{test_data_path}/test_items.npy')\n",
    "test_ratings = np.load(f'{test_data_path}/test_ratings.npy')\n",
    "test_reviews = np.load(f'{test_data_path}/test_reviews.npy', allow_pickle=True)\n",
    "test_descriptions = np.load(\n",
    "    f'{test_data_path}/test_descriptions.npy', allow_pickle=True)\n",
    "test_titles = np.load(\n",
    "    f'{test_data_path}/test_titles.npy', allow_pickle=True)\n",
    "test_prices = np.load(f'{test_data_path}/test_prices.npy')\n",
    "test_categories = np.load(f'{test_data_path}/test_categories.npy')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this function accepts a sequence, and pads the setences in the sequence to LW length, then returns the padded sequence\n",
    "def pad_sequence(sequence, lw = LW, ld = LD, placeholder=0):\n",
    "    arr = []\n",
    "    for sentence in sequence:\n",
    "      if len(sentence) < lw:\n",
    "        arr.append(sentence + [placeholder] * (lw - len(sentence)))\n",
    "      else:\n",
    "        arr.append(sentence[:lw])\n",
    "\n",
    "    if len(arr) > ld:\n",
    "      arr = arr[:ld]\n",
    "    else:\n",
    "      for i in range (ld-len(arr)):\n",
    "        arr.append([placeholder] * lw)\n",
    "        \n",
    "    return arr\n",
    "\n",
    "'''sample user'''\n",
    "'''this function takes a user, who is from a set of users, a set of items, a set of reviews, a set of titles, \n",
    "a set of categories, a set of prices, and a set of ratings.'''\n",
    "def sample_user(user, users, items, reviews, ratings):\n",
    "  # find out all the indicies of the user in users\n",
    "  user_indices = np.where(users == user)[0]\n",
    "  # randomly select an index from these indicies.\n",
    "  user_index = np.random.choice(user_indices)\n",
    "\n",
    "  # get the item that the user has purchased.\n",
    "  item = items[user_index]\n",
    "\n",
    "  # get the rating that the user has given to that item.\n",
    "  rating = ratings[user_index]\n",
    "\n",
    "  # find out the indices of the item in items\n",
    "  item_indices = np.where(items == item)[0]\n",
    "\n",
    "  # get the reviews of that item\n",
    "  item_reviews = [reviews[i] for i in item_indices if i!= user_index]\n",
    "  user_reviews = [reviews[i] for i in user_indices if i!= user_index]\n",
    "\n",
    "  user_reviews = pad_sequence(user_reviews)\n",
    "  item_reviews = pad_sequence(item_reviews)\n",
    "\n",
    "  return user, item, rating, item_reviews, user_reviews\n",
    "\n",
    "\n",
    "'''function to get a batch of training samples.\n",
    "this function selects a random user from a set of users\n",
    "and gets his information by function sample_user, repeat it for batch_size times and \n",
    "gets the batch of samples. Each element in the batch tuple is then transormed to a pytorch tensor and returned.'''\n",
    "def get_batch(users, items, reviews, ratings, batch_size=32, fixed_users_set = None):\n",
    "  # get a batch of users\n",
    "  if fixed_users_set is None:\n",
    "    batch_users = np.random.choice(users, size=batch_size)\n",
    "  else:\n",
    "    batch_users = fixed_users_set\n",
    "  # get the batch of samples\n",
    "  batch = [list(sample_user(user, users, items, reviews, ratings)) for user in batch_users]\n",
    "  # transform each column of the batch to a pytorch tensor\n",
    "  batch = [torch.tensor(sample) for sample in zip(*batch)]\n",
    "  return batch\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get a batch of training samples\n",
    "batch = get_batch(train_users, train_items, train_reviews, train_ratings)\n",
    "user, item, rating, item_reviews, user_reviews = batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def weights_init(m):\n",
    "  if isinstance(m, nn.Linear):\n",
    "    nn.init.normal_(m.weight.data)\n",
    "    nn.init.constant_(m.bias.data, 0.0)\n",
    "\n",
    "class FM(nn.Module):\n",
    "    def __init__(self, latent_dim, fea_num):\n",
    "        super().__init__()\n",
    "\n",
    "        self.latent_dim = latent_dim\n",
    "        self.w0 = nn.Parameter(torch.zeros([1, ], requires_grad=True))\n",
    "        self.w1 = nn.Parameter(torch.rand([fea_num, 1], requires_grad=True))\n",
    "        self.w2 = nn.Parameter(torch.rand([fea_num, latent_dim], requires_grad=True))\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        # inputs = inputs.long()\n",
    "        first_order = self.w0 + torch.mm(inputs, self.w1)\n",
    "        second_order = 1/2 * torch.sum(\n",
    "            torch.pow(torch.mm(inputs, self.w2), 2) -\n",
    "            torch.mm(torch.pow(inputs, 2), torch.pow(self.w2, 2)),\n",
    "\n",
    "            dim=1,\n",
    "            keepdim=True\n",
    "        )\n",
    "\n",
    "        return first_order + second_order\n",
    "\n",
    "\n",
    "class MPCN(nn.Module):\n",
    "  def __init__(self, vocab_size = len(tokens), embed_size = 50, lw = LW, fm_dim = 8, np = 1):\n",
    "      super().__init__()\n",
    "      self.np = np\n",
    "      self.embedding = nn.Embedding(vocab_size, embed_size)\n",
    "      self.embedding.weight.data.copy_(glove[tokens])\n",
    "      # initialise a weighting matrix of size (50 x 50)\n",
    "      self.W_g = nn.Parameter(torch.rand(embed_size, embed_size, requires_grad=True) * 0.001)\n",
    "      # initialise a bias vector of size (50)\n",
    "      self.b_g = nn.Parameter(torch.zeros(embed_size, requires_grad=True))\n",
    "      # initialise a weighting matrix of size (50 x 50)\n",
    "      self.W_u = nn.Parameter(torch.rand(embed_size, embed_size, requires_grad=True) * 0.001)\n",
    "      # initialise a bias vector of size (50)\n",
    "      self.b_u = nn.Parameter(torch.zeros(embed_size, requires_grad=True))\n",
    "      self.sigmoid = nn.Sigmoid()\n",
    "      self.tanh = nn.Tanh()\n",
    "\n",
    "      self.F_review = nn.Sequential(\n",
    "        nn.Linear(embed_size, 2*embed_size, bias=True),\n",
    "        nn.ReLU(),\n",
    "        nn.Dropout(0.2),\n",
    "        nn.Linear(2*embed_size, embed_size, bias=True),\n",
    "        nn.ReLU(),\n",
    "        nn.Dropout(0.2),\n",
    "      )\n",
    "\n",
    "      self.F_word = nn.Sequential(\n",
    "        nn.Linear(embed_size, 2*embed_size, bias=True),\n",
    "        nn.ReLU(),\n",
    "        nn.Dropout(0.2),\n",
    "        nn.Linear(2*embed_size, embed_size, bias=True),\n",
    "        nn.ReLU(),\n",
    "        nn.Dropout(0.2),\n",
    "      )\n",
    "\n",
    "      self.pointer_nn = nn.Linear(np * embed_size, embed_size,bias=True)\n",
    "\n",
    "      # initialise a weighting matrix of size (50 x 50)\n",
    "      self.M = nn.Parameter(torch.rand(embed_size, embed_size, requires_grad=True) * 0.001)\n",
    "      self.M_w = nn.Parameter(torch.rand(embed_size, embed_size, requires_grad=True) * 0.001)\n",
    "\n",
    "      self.fm = FM(fm_dim, 2 * embed_size)\n",
    "\n",
    "  def forward(self, a, b):\n",
    "    a = self.embedding(a.long())\n",
    "    b = self.embedding(b.long())\n",
    "    # size of a: (32, 30, 20, 50)\n",
    "    # size of b: (32, 30, 20, 50)\n",
    "\n",
    "    a_sumed = torch.sum(a, dim=2)\n",
    "    b_sumed = torch.sum(b, dim=2)\n",
    "    # size of a_sumed: (32, 30, 50)\n",
    "    # size of b_sumed: (32, 30, 50)\n",
    "\n",
    "    a_bar = self.sigmoid(a_sumed @ self.W_u) + self.b_g * self.tanh(a_sumed @ self.W_g + self.b_u)\n",
    "    b_bar = self.sigmoid(b_sumed @ self.W_u) + self.b_g * self.tanh(b_sumed @ self.W_g + self.b_u)\n",
    "    # size of a_bar: (32, 30, 50)\n",
    "    # size of b_bar: (32, 30, 50)\n",
    "\n",
    "    a_prime_arr = []\n",
    "    b_prime_arr = []\n",
    "\n",
    "    for i in range (self.np):\n",
    "      S = self.F_review(a_bar) @ self.M @ self.F_review(b_bar).permute(0,2,1)\n",
    "      # get the maximum of S columnwise\n",
    "      max_col_S = torch.max(S, dim=1)[0]\n",
    "      # get the maximum of S rowwise\n",
    "      max_row_S = torch.max(S, dim=2)[0]\n",
    "\n",
    "      # size of S: (32, 30, 30)\n",
    "      # size of max_col_S: (32, 30)\n",
    "      # size of max_row_S: (32, 30)\n",
    "\n",
    "      p_a = nn.functional.gumbel_softmax(logits=max_col_S, hard=True, dim=-1, tau=0.1).unsqueeze(1).unsqueeze(2)\n",
    "      p_b = nn.functional.gumbel_softmax(logits=max_row_S, hard=True, dim=-1, tau=0.1).unsqueeze(1).unsqueeze(2)\n",
    "      # size of p_a: (32, 1, 1, 30)\n",
    "      # size of p_b: (32, 1, 1, 30)\n",
    "\n",
    "      ''' This step implements the pointer selection mechanism in the paper \n",
    "      \n",
    "      p_a is originally a two-dimensional tensor of size (32 x 30(LW)), while a, b \\\n",
    "      are originally a four-dimensional tensor of size (32 x 30(LW) x 20 x 50) \\\n",
    "      we need to reshape p_a and p_b to a four-dimensional tensor of size (32, 1, 1, 30) \\\n",
    "      and multiply with reshaped a and b to (32, 20, 30, 50) (a.permute(0,2,1,3)) \\\n",
    "      then we get the correct but not ideally-shaped selected review of size a' = (32, 1, 20, 50) \\\n",
    "      then we squeeze the second dimension (a'.squeeze(1)) of a' to get (32, 20, 50)\n",
    "      \n",
    "      ''' \n",
    "\n",
    "      a_prime = (p_a @ a.permute(0,2,1,3)).permute(0,2,1,3).squeeze(1)\n",
    "      b_prime = (p_b @ b.permute(0,2,1,3)).permute(0,2,1,3).squeeze(1)\n",
    "      # size of a_prime: (32, lw, 50)\n",
    "      # size of b_prime: (32, lw, 50)\n",
    "\n",
    "      a_prime_arr.append(a_prime)\n",
    "      b_prime_arr.append(b_prime)\n",
    "\n",
    "    # concatenate the elements of a_prime_arr and b_prime_arr\n",
    "    a_prime = torch.cat(a_prime_arr, dim=1)\n",
    "    b_prime = torch.cat(b_prime_arr, dim=1)\n",
    "\n",
    "    \n",
    "    W = self.F_word(a_prime) @ self.M_w @ self.F_word(b_prime).permute(0,2,1)\n",
    "    # size of W: (batch_size, lw * np, lw * np)\n",
    "\n",
    "    # get the column average of W\n",
    "    avg_col_W = torch.mean(W, dim=1).unsqueeze(2)\n",
    "    # get the row average of W\n",
    "    avg_row_W = torch.mean(W, dim=2).unsqueeze(2)\n",
    "    # size of avg_col_W: (batch_size, lw * np, 1)\n",
    "    # size of avg_row_W: (batch_size, lw * np, 1)\n",
    "    \n",
    "    a_bar_prime = self.sigmoid(avg_col_W).permute(0,2,1) @ a_prime\n",
    "    b_bar_prime = self.sigmoid(avg_row_W).permute(0,2,1) @ a_prime\n",
    "    # size of a_bar_prime: (batch_size, 1, emb_size)\n",
    "    # size of b_bar_prime: (batch_size, 1, emb_size)\n",
    "\n",
    "    a_bar_prime = a_bar_prime.squeeze(1)\n",
    "    b_bar_prime = b_bar_prime.squeeze(1)\n",
    "\n",
    "    # concatenate a_bar_prime and b_bar_prime\n",
    "    a_bar_prime_bar_prime = torch.cat((a_bar_prime, b_bar_prime), dim=1)\n",
    "    \n",
    "    pred = self.fm(a_bar_prime_bar_prime)\n",
    "\n",
    "    # flatten the predictions\n",
    "    pred = pred.squeeze(1)\n",
    "\n",
    "    return pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "65891"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "mpcn = MPCN().apply(weights_init).to(device)\n",
    "optimiser_mpcn = optim.Adam(mpcn.parameters(), lr=lr, weight_decay=1e-6)\n",
    "\n",
    "def save_training(path = f'{data_save_path}/'):\n",
    "  torch.save(mpcn.state_dict(), path + 'mpcn_GloVe_p1.pt')\n",
    "  torch.save(optimiser_mpcn.state_dict(), path + 'mpcn_GloVe_optimiser_p1.pt')\n",
    "  print('Saved model and optimiser')\n",
    "\n",
    "def load_training(path = f'{data_save_path}/'):\n",
    "  mpcn.load_state_dict(torch.load(path + 'mpcn_GloVe_p1.pt'))\n",
    "  optimiser_mpcn.load_state_dict(torch.load(path + 'mpcn_GloVe_optimiser_p1.pt'))\n",
    "  print('Loaded model and optimiser')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 6030.9258,  2909.1482, 27792.8887,  4945.6060, 29688.2617, 29688.2617,\n",
       "        29688.2617, 28738.8281, 28738.8281, 29688.2617, 29688.2617, 29688.2617,\n",
       "        29688.2617, 26850.4297, 29688.2617, 13725.5830, 29688.2617, 24976.0039,\n",
       "        29688.2617, 29688.2617, 17154.5664, 27792.8887,  5675.0830, 29688.2617,\n",
       "        27792.8887, 21295.8340, 27792.8867, 29688.2617, 29688.2617, 24976.0039,\n",
       "        29688.2617, 27792.8887], grad_fn=<SqueezeBackward1>)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mpcn(user_reviews, item_reviews)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Program: Evaluating the baseline RMSE of the model on the test set\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3609/3609 [01:27<00:00, 41.07it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Baseline RMSE: 23869.477338013472\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1000/1000 [01:09<00:00, 14.42it/s]\n",
      "100%|██████████| 3609/3609 [01:33<00:00, 38.75it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0 Loss: 19.89178466796875 Test RMSE: 4.376226421471413\n",
      "Saved model and optimiser\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1000/1000 [01:05<00:00, 15.23it/s]\n",
      "100%|██████████| 3609/3609 [01:34<00:00, 38.17it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1 Loss: 20.180095672607422 Test RMSE: 4.365999220282327\n",
      "Saved model and optimiser\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1000/1000 [01:03<00:00, 15.66it/s]\n",
      "100%|██████████| 3609/3609 [01:36<00:00, 37.56it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 2 Loss: 16.979753494262695 Test RMSE: 4.348151881729807\n",
      "Saved model and optimiser\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1000/1000 [01:04<00:00, 15.42it/s]\n",
      "100%|██████████| 3609/3609 [01:34<00:00, 38.22it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 3 Loss: 19.794273376464844 Test RMSE: 4.319161050700851\n",
      "Saved model and optimiser\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1000/1000 [01:06<00:00, 14.99it/s]\n",
      "100%|██████████| 3609/3609 [01:33<00:00, 38.60it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 4 Loss: 17.656112670898438 Test RMSE: 4.271269194998547\n",
      "Saved model and optimiser\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1000/1000 [01:08<00:00, 14.55it/s]\n",
      "100%|██████████| 3609/3609 [01:33<00:00, 38.78it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 5 Loss: 18.808788299560547 Test RMSE: 4.193429800373393\n",
      "Saved model and optimiser\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1000/1000 [01:05<00:00, 15.35it/s]\n",
      "100%|██████████| 3609/3609 [01:36<00:00, 37.56it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 6 Loss: 18.492324829101562 Test RMSE: 4.06731152745884\n",
      "Saved model and optimiser\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1000/1000 [01:04<00:00, 15.50it/s]\n",
      "100%|██████████| 3609/3609 [01:34<00:00, 38.04it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 7 Loss: 14.13482666015625 Test RMSE: 3.8767355516961683\n",
      "Saved model and optimiser\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1000/1000 [01:07<00:00, 14.90it/s]\n",
      "100%|██████████| 3609/3609 [01:32<00:00, 38.95it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 8 Loss: 13.01166820526123 Test RMSE: 3.590277198808416\n",
      "Saved model and optimiser\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1000/1000 [01:08<00:00, 14.67it/s]\n",
      "100%|██████████| 3609/3609 [01:32<00:00, 39.09it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 9 Loss: 9.532758712768555 Test RMSE: 3.199975230020643\n",
      "Saved model and optimiser\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1000/1000 [01:06<00:00, 14.96it/s]\n",
      "100%|██████████| 3609/3609 [01:34<00:00, 38.17it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 10 Loss: 7.408608913421631 Test RMSE: 2.7207176457418565\n",
      "Saved model and optimiser\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1000/1000 [01:04<00:00, 15.43it/s]\n",
      "100%|██████████| 3609/3609 [01:35<00:00, 37.82it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 11 Loss: 5.871913909912109 Test RMSE: 2.2035953092522225\n",
      "Saved model and optimiser\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1000/1000 [01:05<00:00, 15.28it/s]\n",
      "100%|██████████| 3609/3609 [01:33<00:00, 38.60it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 12 Loss: 2.586378335952759 Test RMSE: 1.7301112697804262\n",
      "Saved model and optimiser\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1000/1000 [01:07<00:00, 14.83it/s]\n",
      "100%|██████████| 3609/3609 [01:32<00:00, 39.09it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 13 Loss: 1.79399836063385 Test RMSE: 1.3836160294069548\n",
      "Saved model and optimiser\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1000/1000 [01:08<00:00, 14.59it/s]\n",
      "100%|██████████| 3609/3609 [01:33<00:00, 38.58it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 14 Loss: 2.3971822261810303 Test RMSE: 1.2099115894454509\n",
      "Saved model and optimiser\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1000/1000 [01:05<00:00, 15.23it/s]\n",
      "100%|██████████| 3609/3609 [01:35<00:00, 37.85it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 15 Loss: 1.044924020767212 Test RMSE: 1.1588604494487522\n",
      "Saved model and optimiser\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1000/1000 [01:04<00:00, 15.45it/s]\n",
      "100%|██████████| 3609/3609 [01:33<00:00, 38.44it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 16 Loss: 1.2187628746032715 Test RMSE: 1.1590642587700588\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1000/1000 [01:05<00:00, 15.35it/s]\n",
      "100%|██████████| 3609/3609 [01:32<00:00, 38.98it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 17 Loss: 1.6556785106658936 Test RMSE: 1.1595511829532656\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1000/1000 [01:07<00:00, 14.87it/s]\n",
      "100%|██████████| 3609/3609 [01:32<00:00, 38.91it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 18 Loss: 1.2976622581481934 Test RMSE: 1.1588499679001258\n",
      "Saved model and optimiser\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1000/1000 [01:07<00:00, 14.89it/s]\n",
      "100%|██████████| 3609/3609 [01:34<00:00, 38.04it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 19 Loss: 0.9607446193695068 Test RMSE: 1.1557196297121035\n",
      "Saved model and optimiser\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1000/1000 [01:04<00:00, 15.62it/s]\n",
      "100%|██████████| 3609/3609 [01:35<00:00, 37.89it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 20 Loss: 1.6241024732589722 Test RMSE: 1.1637068527429908\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1000/1000 [01:04<00:00, 15.58it/s]\n",
      "100%|██████████| 3609/3609 [01:32<00:00, 38.97it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 21 Loss: 1.0673630237579346 Test RMSE: 1.15722388622007\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1000/1000 [01:06<00:00, 15.11it/s]\n",
      "100%|██████████| 3609/3609 [01:32<00:00, 39.16it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 22 Loss: 0.8384554386138916 Test RMSE: 1.1579008816549672\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1000/1000 [01:05<00:00, 15.22it/s]\n",
      "100%|██████████| 3609/3609 [01:34<00:00, 38.32it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 23 Loss: 1.5470759868621826 Test RMSE: 1.1577505367090908\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1000/1000 [01:05<00:00, 15.35it/s]\n",
      "100%|██████████| 3609/3609 [01:34<00:00, 38.06it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 24 Loss: 1.139744520187378 Test RMSE: 1.1598941451560496\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1000/1000 [01:04<00:00, 15.58it/s]\n",
      "100%|██████████| 3609/3609 [01:33<00:00, 38.65it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 25 Loss: 1.8110547065734863 Test RMSE: 1.1546407172012012\n",
      "Saved model and optimiser\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1000/1000 [01:06<00:00, 14.97it/s]\n",
      "100%|██████████| 3609/3609 [01:32<00:00, 39.17it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 26 Loss: 2.0307483673095703 Test RMSE: 1.1585707906442655\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1000/1000 [01:06<00:00, 15.05it/s]\n",
      "100%|██████████| 3609/3609 [01:32<00:00, 38.93it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 27 Loss: 1.4534952640533447 Test RMSE: 1.1570834504266498\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1000/1000 [01:05<00:00, 15.26it/s]\n",
      "100%|██████████| 3609/3609 [01:34<00:00, 38.35it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 28 Loss: 1.2338168621063232 Test RMSE: 1.1613555412670356\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1000/1000 [01:04<00:00, 15.56it/s]\n",
      "100%|██████████| 3609/3609 [01:34<00:00, 38.32it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 29 Loss: 2.156033515930176 Test RMSE: 1.1551874517965066\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1000/1000 [01:05<00:00, 15.31it/s]\n",
      "100%|██████████| 3609/3609 [01:33<00:00, 38.76it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 30 Loss: 1.1721270084381104 Test RMSE: 1.1593783282612657\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1000/1000 [01:05<00:00, 15.31it/s]\n",
      "100%|██████████| 3609/3609 [01:32<00:00, 38.86it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 31 Loss: 1.9084382057189941 Test RMSE: 1.1585609007489486\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1000/1000 [01:05<00:00, 15.38it/s]\n",
      "100%|██████████| 3609/3609 [01:33<00:00, 38.65it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 32 Loss: 0.5864557027816772 Test RMSE: 1.1597896779663643\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1000/1000 [01:04<00:00, 15.40it/s]\n",
      "100%|██████████| 3609/3609 [01:33<00:00, 38.68it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 33 Loss: 1.4988024234771729 Test RMSE: 1.1617190201515701\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1000/1000 [01:05<00:00, 15.35it/s]\n",
      "100%|██████████| 3609/3609 [01:33<00:00, 38.55it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 34 Loss: 1.0576894283294678 Test RMSE: 1.1596875779844777\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1000/1000 [01:04<00:00, 15.42it/s]\n",
      "100%|██████████| 3609/3609 [01:32<00:00, 39.04it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 35 Loss: 1.4839138984680176 Test RMSE: 1.160179731339819\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1000/1000 [01:05<00:00, 15.38it/s]\n",
      "100%|██████████| 3609/3609 [01:32<00:00, 38.97it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 36 Loss: 1.9201328754425049 Test RMSE: 1.1551056486412772\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1000/1000 [01:05<00:00, 15.18it/s]\n",
      "100%|██████████| 3609/3609 [01:32<00:00, 39.17it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 37 Loss: 1.1536513566970825 Test RMSE: 1.1615061345408253\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1000/1000 [01:05<00:00, 15.25it/s]\n",
      "100%|██████████| 3609/3609 [01:32<00:00, 38.91it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 38 Loss: 1.0336813926696777 Test RMSE: 1.1576454447358888\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1000/1000 [01:06<00:00, 14.93it/s]\n",
      "100%|██████████| 3609/3609 [01:33<00:00, 38.65it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 39 Loss: 1.0173463821411133 Test RMSE: 1.1547704489484127\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "'''Training'''\n",
    "RMSE = []\n",
    "best_rmse = 1e6\n",
    "\n",
    "def evaluation(test_batch_size = 32):\n",
    "  test_users_unique = list(set(test_users))\n",
    "  rmse_arr = []\n",
    "  for i in tqdm(range(0, len(test_users_unique), test_batch_size)):\n",
    "    test_users_batch = test_users_unique[i:i+test_batch_size]\n",
    "    batch_test = get_batch(test_users, test_items, test_reviews, test_ratings, fixed_users_set=test_users_batch)\n",
    "    user, item, rating, item_reviews, user_reviews = batch_test\n",
    "    user, item, rating, item_reviews, user_reviews = user.to(device), item.to(device), rating.to(device), item_reviews.to(device), user_reviews.to(device)\n",
    "\n",
    "    pred = mpcn(item_reviews, user_reviews)\n",
    "\n",
    "    # calculate rooted mean square error\n",
    "    rmse_arr.append(torch.sqrt(torch.mean((pred - rating)**2)).item())\n",
    "  \n",
    "  # return the mean of the rmse_arr\n",
    "  return np.mean(rmse_arr)\n",
    "\n",
    "print(\"Program: Evaluating the baseline RMSE of the model on the test set\")\n",
    "\n",
    "# set best_rmse to be the largest possible value\n",
    "best_rmse = evaluation()\n",
    "RMSE.append(best_rmse)\n",
    "\n",
    "# print baseline best_rmse\n",
    "print('Baseline RMSE:', best_rmse)\n",
    "\n",
    "for epoch in range (max_epoch):\n",
    "  for i in tqdm(range(1000)):\n",
    "    batch =  get_batch(train_users, train_items, train_reviews, train_ratings)\n",
    "    user, item, rating, item_reviews, user_reviews = batch\n",
    "    user, item, rating, item_reviews, user_reviews = user.to(device), item.to(device), rating.to(device), item_reviews.to(device), user_reviews.to(device)\n",
    "\n",
    "    pred = mpcn(item_reviews, user_reviews)\n",
    "\n",
    "    # calculate the loss\n",
    "    loss = torch.mean((pred - rating)**2)\n",
    "\n",
    "    optimiser_mpcn.zero_grad()\n",
    "    loss.backward()\n",
    "    optimiser_mpcn.step()\n",
    "\n",
    "  '''evaluate the model on the test set'''\n",
    "  rmse_test = evaluation()\n",
    "  RMSE.append(rmse_test)\n",
    "  # print out the loss of the models and the test rmse\n",
    "  print('Epoch:', epoch, 'Loss:', loss.item(), 'Test RMSE:', rmse_test)\n",
    "  if rmse_test < best_rmse:\n",
    "    best_rmse = rmse_test\n",
    "    save_training()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open (f'{analytics_path}/rmse_GloVe_p1.txt', 'w') as f:\n",
    "  for r in RMSE:\n",
    "    f.write(str(np.round(r, 4)) + '\\n')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.11 ('y3project')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.11"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "9af667c24e890afcb270fe85ca560e1aa788703788da50f20e13467161b45801"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
