{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "from transformers import AlbertTokenizerFast, AlbertModel\n",
    "# from gensim.models import Word2Vec\n",
    "# from nets import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://zh-v2.d2l.ai/chapter_natural-language-processing-applications/sentiment-analysis-cnn.html\n",
    "class GloveEmbedding:\n",
    "    \"\"\"Token Embedding.\"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        self.idx_to_token, self.idx_to_vec = self._load_embedding()\n",
    "        self.unknown_idx = 0\n",
    "        # a dictionary like {'best':1} from idx_to_token\n",
    "        self.token_to_idx = {\n",
    "            token: idx for idx, token in enumerate(self.idx_to_token)}\n",
    "\n",
    "    def _load_embedding(self):\n",
    "        idx_to_token, idx_to_vec = ['<unk>'], []\n",
    "        data_dir = '../../Datasets/Model/nlp/'\n",
    "        # GloVe website: https://nlp.stanford.edu/projects/glove/\n",
    "        # fastText website: https://fasttext.cc/\n",
    "        with open(os.path.join(data_dir, 'vec.txt'), encoding='gb18030', errors='ignore') as f:\n",
    "            for line in f:\n",
    "                elems = line.rstrip().split(' ')\n",
    "                # Skip header information, such as the top row in fastText\n",
    "                # structure: hello  [0.1, 0.2, 0.3, ...]\n",
    "                token, elems = elems[0], [float(elem) for elem in elems[1:]]\n",
    "                if len(elems) > 1:\n",
    "                    idx_to_token.append(token)\n",
    "                    idx_to_vec.append(elems)\n",
    "        idx_to_vec = [[0] * len(idx_to_vec[0])] + idx_to_vec\n",
    "        return idx_to_token, torch.tensor(idx_to_vec)\n",
    "\n",
    "    def __getitem__(self, tokens):\n",
    "        indices = [\n",
    "            self.token_to_idx.get(token, self.unknown_idx)\n",
    "            for token in tokens]\n",
    "        vecs = self.idx_to_vec[torch.tensor(indices)]\n",
    "        return vecs\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.idx_to_token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''CONSTANTS'''\n",
    "batch_size = 32\n",
    "max_epoch = 40\n",
    "lr = 0.001\n",
    "\n",
    "LD = 30\n",
    "LW = 20\n",
    "\n",
    "device = torch.device(\n",
    "    'cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
    "train_data_path = '../../Datasets/Model/train'\n",
    "test_data_path = '../../Datasets/Model/test'\n",
    "image_path = '../../Datasets/Images'\n",
    "data_save_path = '../../Save/Experiment5'\n",
    "analytics_path = '../../Analysis/Experiment5'\n",
    "nlp_path = '../../Datasets/Model/nlp'\n",
    "\n",
    "'''NLP'''\n",
    "# load word2vec models from nlp_path\n",
    "# model_review = Word2Vec.load(f'{nlp_path}/model_review.model')\n",
    "# model_title = Word2Vec.load(f'{nlp_path}/model_title.model')\n",
    "\n",
    "# get the vocabulary of reviews titles\n",
    "# vocab_review = model_review.wv.index_to_key\n",
    "# vocab_title = model_title.wv.index_to_key\n",
    "\n",
    "# # get the vectors of reviews and titles\n",
    "# vecs_review = torch.tensor(model_review.wv[vocab_review])\n",
    "# vecs_title = torch.tensor(model_title.wv[vocab_title])\n",
    "\n",
    "# get the glove embeddings\n",
    "# print(\"Loading glove embeddings...\")\n",
    "tokens = []\n",
    "with open('../../Datasets/Model/nlp/tokens.txt', 'r') as f:\n",
    "  for line in f:\n",
    "    tokens.append(line.strip())\n",
    "glove = GloveEmbedding()\n",
    "\n",
    "\n",
    "'''ANALYTICS'''\n",
    "\n",
    "rmse_arr = []\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_users = np.load(f'{train_data_path}/train_users.npy')\n",
    "train_items = np.load(f'{train_data_path}/train_items.npy')\n",
    "train_ratings = np.load(f'{train_data_path}/train_ratings.npy')\n",
    "train_reviews = np.load(\n",
    "    f'{train_data_path}/train_reviews.npy', allow_pickle=True)\n",
    "train_descriptions = np.load(\n",
    "    f'{train_data_path}/train_descriptions.npy', allow_pickle=True)\n",
    "train_titles = np.load(\n",
    "    f'{train_data_path}/train_titles.npy', allow_pickle=True)\n",
    "train_prices = np.load(f'{train_data_path}/train_prices.npy')\n",
    "train_categories = np.load(f'{train_data_path}/train_categories.npy')\n",
    "\n",
    "test_users = np.load(f'{test_data_path}/test_users.npy')\n",
    "test_items = np.load(f'{test_data_path}/test_items.npy')\n",
    "test_ratings = np.load(f'{test_data_path}/test_ratings.npy')\n",
    "test_reviews = np.load(f'{test_data_path}/test_reviews.npy', allow_pickle=True)\n",
    "test_descriptions = np.load(\n",
    "    f'{test_data_path}/test_descriptions.npy', allow_pickle=True)\n",
    "test_titles = np.load(\n",
    "    f'{test_data_path}/test_titles.npy', allow_pickle=True)\n",
    "test_prices = np.load(f'{test_data_path}/test_prices.npy')\n",
    "test_categories = np.load(f'{test_data_path}/test_categories.npy')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this function accepts a sequence, and pads the setences in the sequence to LW length, then returns the padded sequence\n",
    "def pad_sequence(sequence, lw = LW, ld = LD, placeholder=0):\n",
    "    arr = []\n",
    "    for sentence in sequence:\n",
    "      if len(sentence) < lw:\n",
    "        arr.append(sentence + [placeholder] * (lw - len(sentence)))\n",
    "      else:\n",
    "        arr.append(sentence[:lw])\n",
    "\n",
    "    if len(arr) > ld:\n",
    "      arr = arr[:ld]\n",
    "    else:\n",
    "      for i in range (ld-len(arr)):\n",
    "        arr.append([placeholder] * lw)\n",
    "        \n",
    "    return arr\n",
    "\n",
    "'''sample user'''\n",
    "'''this function takes a user, who is from a set of users, a set of items, a set of reviews, a set of titles, \n",
    "a set of categories, a set of prices, and a set of ratings.'''\n",
    "def sample_user(user, users, items, reviews, ratings):\n",
    "  # find out all the indicies of the user in users\n",
    "  user_indices = np.where(users == user)[0]\n",
    "  # randomly select an index from these indicies.\n",
    "  user_index = np.random.choice(user_indices)\n",
    "\n",
    "  # get the item that the user has purchased.\n",
    "  item = items[user_index]\n",
    "\n",
    "  # get the rating that the user has given to that item.\n",
    "  rating = ratings[user_index]\n",
    "\n",
    "  # find out the indices of the item in items\n",
    "  item_indices = np.where(items == item)[0]\n",
    "\n",
    "  # get the reviews of that item\n",
    "  item_reviews = [reviews[i] for i in item_indices if i!= user_index]\n",
    "  user_reviews = [reviews[i] for i in user_indices if i!= user_index]\n",
    "\n",
    "  user_reviews = pad_sequence(user_reviews)\n",
    "  item_reviews = pad_sequence(item_reviews)\n",
    "\n",
    "  return user, item, rating, item_reviews, user_reviews\n",
    "\n",
    "\n",
    "'''function to get a batch of training samples.\n",
    "this function selects a random user from a set of users\n",
    "and gets his information by function sample_user, repeat it for batch_size times and \n",
    "gets the batch of samples. Each element in the batch tuple is then transormed to a pytorch tensor and returned.'''\n",
    "def get_batch(users, items, reviews, ratings, batch_size=32, fixed_users_set = None):\n",
    "  # get a batch of users\n",
    "  if fixed_users_set is None:\n",
    "    batch_users = np.random.choice(users, size=batch_size)\n",
    "  else:\n",
    "    batch_users = fixed_users_set\n",
    "  # get the batch of samples\n",
    "  batch = [list(sample_user(user, users, items, reviews, ratings)) for user in batch_users]\n",
    "  # transform each column of the batch to a pytorch tensor\n",
    "  batch = [torch.tensor(sample) for sample in zip(*batch)]\n",
    "  return batch\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get a batch of training samples\n",
    "batch = get_batch(train_users, train_items, train_reviews, train_ratings)\n",
    "user, item, rating, item_reviews, user_reviews = batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def weights_init(m):\n",
    "  if isinstance(m, nn.Linear):\n",
    "    nn.init.normal_(m.weight.data)\n",
    "    nn.init.constant_(m.bias.data, 0.0)\n",
    "\n",
    "class FM(nn.Module):\n",
    "    def __init__(self, latent_dim, fea_num):\n",
    "        super().__init__()\n",
    "\n",
    "        self.latent_dim = latent_dim\n",
    "        self.w0 = nn.Parameter(torch.zeros([1, ], requires_grad=True))\n",
    "        self.w1 = nn.Parameter(torch.rand([fea_num, 1], requires_grad=True))\n",
    "        self.w2 = nn.Parameter(torch.rand([fea_num, latent_dim], requires_grad=True))\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        # inputs = inputs.long()\n",
    "        first_order = self.w0 + torch.mm(inputs, self.w1)\n",
    "        second_order = 1/2 * torch.sum(\n",
    "            torch.pow(torch.mm(inputs, self.w2), 2) -\n",
    "            torch.mm(torch.pow(inputs, 2), torch.pow(self.w2, 2)),\n",
    "\n",
    "            dim=1,\n",
    "            keepdim=True\n",
    "        )\n",
    "\n",
    "        return first_order + second_order\n",
    "\n",
    "\n",
    "class MPCN(nn.Module):\n",
    "  def __init__(self, vocab_size = len(tokens), embed_size = 50, lw = LW, fm_dim = 8, np = 2):\n",
    "      super().__init__()\n",
    "      self.np = np\n",
    "      self.embedding = nn.Embedding(vocab_size, embed_size)\n",
    "      self.embedding.weight.data.copy_(glove[tokens])\n",
    "      # initialise a weighting matrix of size (50 x 50)\n",
    "      self.W_g = nn.Parameter(torch.rand(embed_size, embed_size, requires_grad=True) * 0.001)\n",
    "      # initialise a bias vector of size (50)\n",
    "      self.b_g = nn.Parameter(torch.zeros(embed_size, requires_grad=True))\n",
    "      # initialise a weighting matrix of size (50 x 50)\n",
    "      self.W_u = nn.Parameter(torch.rand(embed_size, embed_size, requires_grad=True) * 0.001)\n",
    "      # initialise a bias vector of size (50)\n",
    "      self.b_u = nn.Parameter(torch.zeros(embed_size, requires_grad=True))\n",
    "      self.sigmoid = nn.Sigmoid()\n",
    "      self.tanh = nn.Tanh()\n",
    "\n",
    "      self.F_review = nn.Sequential(\n",
    "        nn.Linear(embed_size, 2*embed_size, bias=True),\n",
    "        nn.ReLU(),\n",
    "        nn.Dropout(0.2),\n",
    "        nn.Linear(2*embed_size, embed_size, bias=True),\n",
    "        nn.ReLU(),\n",
    "        nn.Dropout(0.2),\n",
    "      )\n",
    "\n",
    "      self.F_word = nn.Sequential(\n",
    "        nn.Linear(embed_size, 2*embed_size, bias=True),\n",
    "        nn.ReLU(),\n",
    "        nn.Dropout(0.2),\n",
    "        nn.Linear(2*embed_size, embed_size, bias=True),\n",
    "        nn.ReLU(),\n",
    "        nn.Dropout(0.2),\n",
    "      )\n",
    "\n",
    "      self.pointer_nn = nn.Linear(np * embed_size, embed_size,bias=True)\n",
    "\n",
    "      # initialise a weighting matrix of size (50 x 50)\n",
    "      self.M = nn.Parameter(torch.rand(embed_size, embed_size, requires_grad=True) * 0.001)\n",
    "      self.M_w = nn.Parameter(torch.rand(embed_size, embed_size, requires_grad=True) * 0.001)\n",
    "\n",
    "      self.fm = FM(fm_dim, 2 * embed_size)\n",
    "\n",
    "  def forward(self, a, b):\n",
    "    a = self.embedding(a.long())\n",
    "    b = self.embedding(b.long())\n",
    "    # size of a: (32, 30, 20, 50)\n",
    "    # size of b: (32, 30, 20, 50)\n",
    "\n",
    "    a_sumed = torch.sum(a, dim=2)\n",
    "    b_sumed = torch.sum(b, dim=2)\n",
    "    # size of a_sumed: (32, 30, 50)\n",
    "    # size of b_sumed: (32, 30, 50)\n",
    "\n",
    "    a_bar = self.sigmoid(a_sumed @ self.W_u) + self.b_g * self.tanh(a_sumed @ self.W_g + self.b_u)\n",
    "    b_bar = self.sigmoid(b_sumed @ self.W_u) + self.b_g * self.tanh(b_sumed @ self.W_g + self.b_u)\n",
    "    # size of a_bar: (32, 30, 50)\n",
    "    # size of b_bar: (32, 30, 50)\n",
    "\n",
    "    a_prime_arr = []\n",
    "    b_prime_arr = []\n",
    "\n",
    "    for i in range (self.np):\n",
    "      S = self.F_review(a_bar) @ self.M @ self.F_review(b_bar).permute(0,2,1)\n",
    "      # get the maximum of S columnwise\n",
    "      max_col_S = torch.max(S, dim=1)[0]\n",
    "      # get the maximum of S rowwise\n",
    "      max_row_S = torch.max(S, dim=2)[0]\n",
    "\n",
    "      # size of S: (32, 30, 30)\n",
    "      # size of max_col_S: (32, 30)\n",
    "      # size of max_row_S: (32, 30)\n",
    "\n",
    "      p_a = nn.functional.gumbel_softmax(logits=max_col_S, hard=True, dim=-1, tau=0.1).unsqueeze(1).unsqueeze(2)\n",
    "      p_b = nn.functional.gumbel_softmax(logits=max_row_S, hard=True, dim=-1, tau=0.1).unsqueeze(1).unsqueeze(2)\n",
    "      # size of p_a: (32, 1, 1, 30)\n",
    "      # size of p_b: (32, 1, 1, 30)\n",
    "\n",
    "      ''' This step implements the pointer selection mechanism in the paper \n",
    "      \n",
    "      p_a is originally a two-dimensional tensor of size (32 x 30(LW)), while a, b \\\n",
    "      are originally a four-dimensional tensor of size (32 x 30(LW) x 20 x 50) \\\n",
    "      we need to reshape p_a and p_b to a four-dimensional tensor of size (32, 1, 1, 30) \\\n",
    "      and multiply with reshaped a and b to (32, 20, 30, 50) (a.permute(0,2,1,3)) \\\n",
    "      then we get the correct but not ideally-shaped selected review of size a' = (32, 1, 20, 50) \\\n",
    "      then we squeeze the second dimension (a'.squeeze(1)) of a' to get (32, 20, 50)\n",
    "      \n",
    "      ''' \n",
    "\n",
    "      a_prime = (p_a @ a.permute(0,2,1,3)).permute(0,2,1,3).squeeze(1)\n",
    "      b_prime = (p_b @ b.permute(0,2,1,3)).permute(0,2,1,3).squeeze(1)\n",
    "      # size of a_prime: (32, lw, 50)\n",
    "      # size of b_prime: (32, lw, 50)\n",
    "\n",
    "      a_prime_arr.append(a_prime)\n",
    "      b_prime_arr.append(b_prime)\n",
    "\n",
    "    # concatenate the elements of a_prime_arr and b_prime_arr\n",
    "    a_prime = torch.cat(a_prime_arr, dim=1)\n",
    "    b_prime = torch.cat(b_prime_arr, dim=1)\n",
    "\n",
    "    \n",
    "    W = self.F_word(a_prime) @ self.M_w @ self.F_word(b_prime).permute(0,2,1)\n",
    "    # size of W: (batch_size, lw * np, lw * np)\n",
    "\n",
    "    # get the column average of W\n",
    "    avg_col_W = torch.mean(W, dim=1).unsqueeze(2)\n",
    "    # get the row average of W\n",
    "    avg_row_W = torch.mean(W, dim=2).unsqueeze(2)\n",
    "    # size of avg_col_W: (batch_size, lw * np, 1)\n",
    "    # size of avg_row_W: (batch_size, lw * np, 1)\n",
    "    \n",
    "    a_bar_prime = self.sigmoid(avg_col_W).permute(0,2,1) @ a_prime\n",
    "    b_bar_prime = self.sigmoid(avg_row_W).permute(0,2,1) @ a_prime\n",
    "    # size of a_bar_prime: (batch_size, 1, emb_size)\n",
    "    # size of b_bar_prime: (batch_size, 1, emb_size)\n",
    "\n",
    "    a_bar_prime = a_bar_prime.squeeze(1)\n",
    "    b_bar_prime = b_bar_prime.squeeze(1)\n",
    "\n",
    "    # concatenate a_bar_prime and b_bar_prime\n",
    "    a_bar_prime_bar_prime = torch.cat((a_bar_prime, b_bar_prime), dim=1)\n",
    "    \n",
    "    pred = self.fm(a_bar_prime_bar_prime)\n",
    "\n",
    "    # flatten the predictions\n",
    "    pred = pred.squeeze(1)\n",
    "\n",
    "    return pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "65891"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "mpcn = MPCN().apply(weights_init).to(device)\n",
    "optimiser_mpcn = optim.Adam(mpcn.parameters(), lr=lr, weight_decay=1e-6)\n",
    "\n",
    "def save_training(path = f'{data_save_path}/'):\n",
    "  torch.save(mpcn.state_dict(), path + 'mpcn_GloVe.pt')\n",
    "  torch.save(optimiser_mpcn.state_dict(), path + 'mpcn_GloVe_optimiser.pt')\n",
    "  print('Saved model and optimiser')\n",
    "\n",
    "def load_training(path = f'{data_save_path}/'):\n",
    "  mpcn.load_state_dict(torch.load(path + 'mpcn_GloVe.pt'))\n",
    "  optimiser_mpcn.load_state_dict(torch.load(path + 'mpcn_GloVe_optimiser.pt'))\n",
    "  print('Loaded model and optimiser')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([123259.5312, 123259.5312,  30880.9844, 119552.0234, 121406.1953,\n",
       "        123259.5312, 119552.0391,  64228.6797, 123259.5312, 119552.0391,\n",
       "        115841.3906, 115841.3984, 112127.5781,  19410.5469, 115841.3906,\n",
       "        123259.5312, 106551.0859, 123259.5312, 102638.1094, 117697.1016,\n",
       "        119552.0391, 123259.5312, 123259.5312, 117697.1484, 121406.1953,\n",
       "        123259.5312,  24272.2324,  69752.5781, 112127.5938, 123259.5312,\n",
       "         17119.7051, 113984.9062], grad_fn=<SqueezeBackward1>)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mpcn(user_reviews, item_reviews)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Program: Evaluating the baseline RMSE of the model on the test set\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3609/3609 [01:44<00:00, 34.63it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Baseline RMSE: 94869.80998458715\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1000/1000 [01:11<00:00, 14.07it/s]\n",
      "100%|██████████| 3609/3609 [01:47<00:00, 33.70it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0 Loss: 19.90329933166504 Test RMSE: 4.386193464257507\n",
      "Saved model and optimiser\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1000/1000 [01:09<00:00, 14.43it/s]\n",
      "100%|██████████| 3609/3609 [01:47<00:00, 33.65it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1 Loss: 17.62903594970703 Test RMSE: 4.375659168509771\n",
      "Saved model and optimiser\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1000/1000 [01:09<00:00, 14.35it/s]\n",
      "100%|██████████| 3609/3609 [01:49<00:00, 33.04it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 2 Loss: 19.095277786254883 Test RMSE: 4.376140376939507\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1000/1000 [01:15<00:00, 13.31it/s]\n",
      "100%|██████████| 3609/3609 [01:46<00:00, 33.78it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 3 Loss: 19.08814811706543 Test RMSE: 4.374155001152781\n",
      "Saved model and optimiser\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1000/1000 [01:09<00:00, 14.39it/s]\n",
      "100%|██████████| 3609/3609 [01:52<00:00, 31.96it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 4 Loss: 19.690696716308594 Test RMSE: 4.35392957065336\n",
      "Saved model and optimiser\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1000/1000 [01:09<00:00, 14.30it/s]\n",
      "100%|██████████| 3609/3609 [01:47<00:00, 33.53it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 5 Loss: 17.77881622314453 Test RMSE: 4.33364485511822\n",
      "Saved model and optimiser\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1000/1000 [01:11<00:00, 14.01it/s]\n",
      "100%|██████████| 3609/3609 [01:50<00:00, 32.62it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 6 Loss: 19.107324600219727 Test RMSE: 4.29802995984592\n",
      "Saved model and optimiser\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1000/1000 [01:13<00:00, 13.66it/s]\n",
      "100%|██████████| 3609/3609 [01:52<00:00, 32.01it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 7 Loss: 17.499919891357422 Test RMSE: 4.240987215322218\n",
      "Saved model and optimiser\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1000/1000 [01:25<00:00, 11.68it/s]\n",
      "100%|██████████| 3609/3609 [02:01<00:00, 29.60it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 8 Loss: 16.804912567138672 Test RMSE: 4.149872698127216\n",
      "Saved model and optimiser\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1000/1000 [01:20<00:00, 12.37it/s]\n",
      "100%|██████████| 3609/3609 [02:01<00:00, 29.73it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 9 Loss: 14.212203025817871 Test RMSE: 4.010668817204628\n",
      "Saved model and optimiser\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1000/1000 [01:23<00:00, 11.92it/s]\n",
      "100%|██████████| 3609/3609 [02:01<00:00, 29.80it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 10 Loss: 14.593608856201172 Test RMSE: 3.79184814938286\n",
      "Saved model and optimiser\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1000/1000 [01:24<00:00, 11.79it/s]\n",
      "100%|██████████| 3609/3609 [02:02<00:00, 29.51it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 11 Loss: 12.329566955566406 Test RMSE: 3.4744059468081994\n",
      "Saved model and optimiser\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1000/1000 [01:24<00:00, 11.86it/s]\n",
      "100%|██████████| 3609/3609 [02:02<00:00, 29.57it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 12 Loss: 7.3106536865234375 Test RMSE: 3.0525480123331223\n",
      "Saved model and optimiser\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1000/1000 [01:23<00:00, 11.93it/s]\n",
      "100%|██████████| 3609/3609 [02:00<00:00, 30.00it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 13 Loss: 6.385446548461914 Test RMSE: 2.559801051873651\n",
      "Saved model and optimiser\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1000/1000 [01:25<00:00, 11.73it/s]\n",
      "100%|██████████| 3609/3609 [02:00<00:00, 30.04it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 14 Loss: 3.944401741027832 Test RMSE: 2.0451852930454244\n",
      "Saved model and optimiser\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1000/1000 [01:25<00:00, 11.75it/s]\n",
      "100%|██████████| 3609/3609 [02:00<00:00, 29.93it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 15 Loss: 2.485124111175537 Test RMSE: 1.6098727463817095\n",
      "Saved model and optimiser\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1000/1000 [01:23<00:00, 11.91it/s]\n",
      "100%|██████████| 3609/3609 [02:00<00:00, 29.98it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 16 Loss: 1.5314792394638062 Test RMSE: 1.3135954705788244\n",
      "Saved model and optimiser\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1000/1000 [01:24<00:00, 11.84it/s]\n",
      "100%|██████████| 3609/3609 [01:59<00:00, 30.18it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 17 Loss: 1.7231385707855225 Test RMSE: 1.1861695564496746\n",
      "Saved model and optimiser\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1000/1000 [01:25<00:00, 11.70it/s]\n",
      "100%|██████████| 3609/3609 [02:01<00:00, 29.79it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 18 Loss: 1.0994749069213867 Test RMSE: 1.1589102603187518\n",
      "Saved model and optimiser\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1000/1000 [01:24<00:00, 11.77it/s]\n",
      "100%|██████████| 3609/3609 [02:01<00:00, 29.66it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 19 Loss: 1.732727289199829 Test RMSE: 1.1626733199451464\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1000/1000 [01:23<00:00, 12.04it/s]\n",
      "100%|██████████| 3609/3609 [02:00<00:00, 29.98it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 20 Loss: 0.9672714471817017 Test RMSE: 1.159410311099597\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1000/1000 [01:22<00:00, 12.06it/s]\n",
      "100%|██████████| 3609/3609 [01:59<00:00, 30.13it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 21 Loss: 1.3751386404037476 Test RMSE: 1.1595085343187554\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1000/1000 [01:24<00:00, 11.88it/s]\n",
      "100%|██████████| 3609/3609 [02:02<00:00, 29.49it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 22 Loss: 0.6974859237670898 Test RMSE: 1.160744788359988\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1000/1000 [01:21<00:00, 12.31it/s]\n",
      "100%|██████████| 3609/3609 [02:01<00:00, 29.71it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 23 Loss: 1.157067060470581 Test RMSE: 1.1567766834540723\n",
      "Saved model and optimiser\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1000/1000 [01:20<00:00, 12.36it/s]\n",
      "100%|██████████| 3609/3609 [02:00<00:00, 30.05it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 24 Loss: 0.8061389923095703 Test RMSE: 1.1610487714868134\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1000/1000 [01:22<00:00, 12.08it/s]\n",
      "100%|██████████| 3609/3609 [01:59<00:00, 30.18it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 25 Loss: 1.451151728630066 Test RMSE: 1.1581332388595387\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1000/1000 [01:22<00:00, 12.17it/s]\n",
      "100%|██████████| 3609/3609 [02:02<00:00, 29.57it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 26 Loss: 0.9399219155311584 Test RMSE: 1.1612041125538013\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1000/1000 [01:19<00:00, 12.58it/s]\n",
      "100%|██████████| 3609/3609 [02:02<00:00, 29.53it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 27 Loss: 1.2496604919433594 Test RMSE: 1.159525826800461\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1000/1000 [01:21<00:00, 12.28it/s]\n",
      "100%|██████████| 3609/3609 [02:00<00:00, 29.87it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 28 Loss: 2.258721113204956 Test RMSE: 1.1534367293557093\n",
      "Saved model and optimiser\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1000/1000 [01:22<00:00, 12.07it/s]\n",
      "100%|██████████| 3609/3609 [02:01<00:00, 29.78it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 29 Loss: 0.9368066787719727 Test RMSE: 1.1595968036117827\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1000/1000 [01:20<00:00, 12.47it/s]\n",
      "100%|██████████| 3609/3609 [02:02<00:00, 29.56it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 30 Loss: 1.180671215057373 Test RMSE: 1.1594029986386023\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1000/1000 [01:18<00:00, 12.72it/s]\n",
      "100%|██████████| 3609/3609 [02:02<00:00, 29.56it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 31 Loss: 1.6268490552902222 Test RMSE: 1.159044297886344\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1000/1000 [01:21<00:00, 12.26it/s]\n",
      "100%|██████████| 3609/3609 [02:00<00:00, 29.93it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 32 Loss: 1.264922857284546 Test RMSE: 1.1572884153745022\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1000/1000 [01:21<00:00, 12.31it/s]\n",
      "100%|██████████| 3609/3609 [02:01<00:00, 29.70it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 33 Loss: 0.9670934677124023 Test RMSE: 1.1579571630063563\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1000/1000 [01:18<00:00, 12.66it/s]\n",
      "100%|██████████| 3609/3609 [02:02<00:00, 29.45it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 34 Loss: 0.8707071542739868 Test RMSE: 1.160428020259818\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1000/1000 [01:19<00:00, 12.62it/s]\n",
      "100%|██████████| 3609/3609 [02:01<00:00, 29.75it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 35 Loss: 2.443507671356201 Test RMSE: 1.1585698484317455\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1000/1000 [01:21<00:00, 12.25it/s]\n",
      "100%|██████████| 3609/3609 [02:00<00:00, 30.01it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 36 Loss: 1.3454020023345947 Test RMSE: 1.159058560324891\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1000/1000 [01:21<00:00, 12.32it/s]\n",
      "100%|██████████| 3609/3609 [02:01<00:00, 29.65it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 37 Loss: 1.1399738788604736 Test RMSE: 1.1581646162045236\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1000/1000 [01:18<00:00, 12.69it/s]\n",
      "100%|██████████| 3609/3609 [02:03<00:00, 29.10it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 38 Loss: 1.8699942827224731 Test RMSE: 1.1620969855246963\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1000/1000 [01:18<00:00, 12.79it/s]\n",
      "100%|██████████| 3609/3609 [01:51<00:00, 32.34it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 39 Loss: 2.107036828994751 Test RMSE: 1.1587986240726522\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "'''Training'''\n",
    "RMSE = []\n",
    "best_rmse = 1e6\n",
    "\n",
    "def evaluation(test_batch_size = 32):\n",
    "  test_users_unique = list(set(test_users))\n",
    "  rmse_arr = []\n",
    "  for i in tqdm(range(0, len(test_users_unique), test_batch_size)):\n",
    "    test_users_batch = test_users_unique[i:i+test_batch_size]\n",
    "    batch_test = get_batch(test_users, test_items, test_reviews, test_ratings, fixed_users_set=test_users_batch)\n",
    "    user, item, rating, item_reviews, user_reviews = batch_test\n",
    "    user, item, rating, item_reviews, user_reviews = user.to(device), item.to(device), rating.to(device), item_reviews.to(device), user_reviews.to(device)\n",
    "\n",
    "    pred = mpcn(item_reviews, user_reviews)\n",
    "\n",
    "    # calculate rooted mean square error\n",
    "    rmse_arr.append(torch.sqrt(torch.mean((pred - rating)**2)).item())\n",
    "  \n",
    "  # return the mean of the rmse_arr\n",
    "  return np.mean(rmse_arr)\n",
    "\n",
    "print(\"Program: Evaluating the baseline RMSE of the model on the test set\")\n",
    "\n",
    "# set best_rmse to be the largest possible value\n",
    "best_rmse = evaluation()\n",
    "RMSE.append(best_rmse)\n",
    "\n",
    "# print baseline best_rmse\n",
    "print('Baseline RMSE:', best_rmse)\n",
    "\n",
    "for epoch in range (max_epoch):\n",
    "  for i in tqdm(range(1000)):\n",
    "    batch =  get_batch(train_users, train_items, train_reviews, train_ratings)\n",
    "    user, item, rating, item_reviews, user_reviews = batch\n",
    "    user, item, rating, item_reviews, user_reviews = user.to(device), item.to(device), rating.to(device), item_reviews.to(device), user_reviews.to(device)\n",
    "\n",
    "    pred = mpcn(item_reviews, user_reviews)\n",
    "\n",
    "    # calculate the loss\n",
    "    loss = torch.mean((pred - rating)**2)\n",
    "\n",
    "    optimiser_mpcn.zero_grad()\n",
    "    loss.backward()\n",
    "    optimiser_mpcn.step()\n",
    "\n",
    "  '''evaluate the model on the test set'''\n",
    "  rmse_test = evaluation()\n",
    "  RMSE.append(rmse_test)\n",
    "  # print out the loss of the models and the test rmse\n",
    "  print('Epoch:', epoch, 'Loss:', loss.item(), 'Test RMSE:', rmse_test)\n",
    "  if rmse_test < best_rmse:\n",
    "    best_rmse = rmse_test\n",
    "    save_training()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open (f'{analytics_path}/rmse_GloVe.txt', 'w') as f:\n",
    "  for r in RMSE:\n",
    "    f.write(str(np.round(r, 4)) + '\\n')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.11 ('y3project')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.11"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "9af667c24e890afcb270fe85ca560e1aa788703788da50f20e13467161b45801"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
