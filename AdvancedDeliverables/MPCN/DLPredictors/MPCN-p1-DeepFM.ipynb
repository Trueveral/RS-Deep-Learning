{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "from transformers import AlbertTokenizerFast, AlbertModel\n",
    "# from gensim.models import Word2Vec\n",
    "# from nets import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://zh-v2.d2l.ai/chapter_natural-language-processing-applications/sentiment-analysis-cnn.html\n",
    "class GloveEmbedding:\n",
    "    \"\"\"Token Embedding.\"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        self.idx_to_token, self.idx_to_vec = self._load_embedding()\n",
    "        self.unknown_idx = 0\n",
    "        # a dictionary like {'best':1} from idx_to_token\n",
    "        self.token_to_idx = {\n",
    "            token: idx for idx, token in enumerate(self.idx_to_token)}\n",
    "\n",
    "    def _load_embedding(self):\n",
    "        idx_to_token, idx_to_vec = ['<unk>'], []\n",
    "        data_dir = '../../Datasets/Model/nlp/'\n",
    "        # GloVe website: https://nlp.stanford.edu/projects/glove/\n",
    "        # fastText website: https://fasttext.cc/\n",
    "        with open(os.path.join(data_dir, 'vec.txt'), encoding='gb18030', errors='ignore') as f:\n",
    "            for line in f:\n",
    "                elems = line.rstrip().split(' ')\n",
    "                # Skip header information, such as the top row in fastText\n",
    "                # structure: hello  [0.1, 0.2, 0.3, ...]\n",
    "                token, elems = elems[0], [float(elem) for elem in elems[1:]]\n",
    "                if len(elems) > 1:\n",
    "                    idx_to_token.append(token)\n",
    "                    idx_to_vec.append(elems)\n",
    "        idx_to_vec = [[0] * len(idx_to_vec[0])] + idx_to_vec\n",
    "        return idx_to_token, torch.tensor(idx_to_vec)\n",
    "\n",
    "    def __getitem__(self, tokens):\n",
    "        indices = [\n",
    "            self.token_to_idx.get(token, self.unknown_idx)\n",
    "            for token in tokens]\n",
    "        vecs = self.idx_to_vec[torch.tensor(indices)]\n",
    "        return vecs\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.idx_to_token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''CONSTANTS'''\n",
    "batch_size = 32\n",
    "max_epoch = 40\n",
    "lr = 0.001\n",
    "\n",
    "LD = 30\n",
    "LW = 20\n",
    "\n",
    "device = torch.device(\n",
    "    'cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
    "train_data_path = '../../Datasets/Model/train'\n",
    "test_data_path = '../../Datasets/Model/test'\n",
    "image_path = '../../Datasets/Images'\n",
    "data_save_path = '../../Save/Experiment5'\n",
    "analytics_path = '../../Analysis/Experiment5'\n",
    "nlp_path = '../../Datasets/Model/nlp'\n",
    "\n",
    "'''NLP'''\n",
    "# load word2vec models from nlp_path\n",
    "# model_review = Word2Vec.load(f'{nlp_path}/model_review.model')\n",
    "# model_title = Word2Vec.load(f'{nlp_path}/model_title.model')\n",
    "\n",
    "# get the vocabulary of reviews titles\n",
    "# vocab_review = model_review.wv.index_to_key\n",
    "# vocab_title = model_title.wv.index_to_key\n",
    "\n",
    "# # get the vectors of reviews and titles\n",
    "# vecs_review = torch.tensor(model_review.wv[vocab_review])\n",
    "# vecs_title = torch.tensor(model_title.wv[vocab_title])\n",
    "\n",
    "# get the glove embeddings\n",
    "# print(\"Loading glove embeddings...\")\n",
    "# glove = GloveEmbedding()\n",
    "\n",
    "\n",
    "'''ANALYTICS'''\n",
    "\n",
    "rmse_arr = []\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda')"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_users = np.load(f'{train_data_path}/train_users.npy')\n",
    "train_items = np.load(f'{train_data_path}/train_items.npy')\n",
    "train_ratings = np.load(f'{train_data_path}/train_ratings.npy')\n",
    "train_reviews = np.load(\n",
    "    f'{train_data_path}/train_reviews.npy', allow_pickle=True)\n",
    "train_descriptions = np.load(\n",
    "    f'{train_data_path}/train_descriptions.npy', allow_pickle=True)\n",
    "train_titles = np.load(\n",
    "    f'{train_data_path}/train_titles.npy', allow_pickle=True)\n",
    "train_prices = np.load(f'{train_data_path}/train_prices.npy')\n",
    "train_categories = np.load(f'{train_data_path}/train_categories.npy')\n",
    "\n",
    "test_users = np.load(f'{test_data_path}/test_users.npy')\n",
    "test_items = np.load(f'{test_data_path}/test_items.npy')\n",
    "test_ratings = np.load(f'{test_data_path}/test_ratings.npy')\n",
    "test_reviews = np.load(f'{test_data_path}/test_reviews.npy', allow_pickle=True)\n",
    "test_descriptions = np.load(\n",
    "    f'{test_data_path}/test_descriptions.npy', allow_pickle=True)\n",
    "test_titles = np.load(\n",
    "    f'{test_data_path}/test_titles.npy', allow_pickle=True)\n",
    "test_prices = np.load(f'{test_data_path}/test_prices.npy')\n",
    "test_categories = np.load(f'{test_data_path}/test_categories.npy')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "72693"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max(test_items)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this function accepts a sequence, and pads the setences in the sequence to LW length, then returns the padded sequence\n",
    "def pad_sequence(sequence, lw = LW, ld = LD, placeholder=0):\n",
    "    arr = []\n",
    "    for sentence in sequence:\n",
    "      if len(sentence) < lw:\n",
    "        arr.append(sentence + [placeholder] * (lw - len(sentence)))\n",
    "      else:\n",
    "        arr.append(sentence[:lw])\n",
    "\n",
    "    if len(arr) > ld:\n",
    "      arr = arr[:ld]\n",
    "    else:\n",
    "      for i in range (ld-len(arr)):\n",
    "        arr.append([placeholder] * lw)\n",
    "        \n",
    "    return arr\n",
    "\n",
    "'''sample user'''\n",
    "'''this function takes a user, who is from a set of users, a set of items, a set of reviews, a set of titles, \n",
    "a set of categories, a set of prices, and a set of ratings.'''\n",
    "def sample_user(user, users, items, reviews, ratings):\n",
    "  # find out all the indicies of the user in users\n",
    "  user_indices = np.where(users == user)[0]\n",
    "  # randomly select an index from these indicies.\n",
    "  user_index = np.random.choice(user_indices)\n",
    "\n",
    "  # get the item that the user has purchased.\n",
    "  item = items[user_index]\n",
    "\n",
    "  # get the rating that the user has given to that item.\n",
    "  rating = ratings[user_index]\n",
    "\n",
    "  # find out the indices of the item in items\n",
    "  item_indices = np.where(items == item)[0]\n",
    "\n",
    "  # get the reviews of that item\n",
    "  item_reviews = [reviews[i] for i in item_indices if i != user_index]\n",
    "  user_reviews = [reviews[i] for i in user_indices if i != user_index]\n",
    "\n",
    "  user_reviews = pad_sequence(user_reviews)\n",
    "  item_reviews = pad_sequence(item_reviews)\n",
    "\n",
    "  return user, item, rating, item_reviews, user_reviews\n",
    "\n",
    "\n",
    "'''function to get a batch of training samples.\n",
    "this function selects a random user from a set of users\n",
    "and gets his information by function sample_user, repeat it for batch_size times and \n",
    "gets the batch of samples. Each element in the batch tuple is then transormed to a pytorch tensor and returned.'''\n",
    "def get_batch(users, items, reviews, ratings, batch_size=32, fixed_users_set = None):\n",
    "  # get a batch of users\n",
    "  if fixed_users_set is None:\n",
    "    batch_users = np.random.choice(users, size=batch_size)\n",
    "  else:\n",
    "    batch_users = fixed_users_set\n",
    "  # get the batch of samples\n",
    "  batch = [list(sample_user(user, users, items, reviews, ratings)) for user in batch_users]\n",
    "  # transform each column of the batch to a pytorch tensor\n",
    "  batch = [torch.tensor(sample) for sample in zip(*batch)]\n",
    "  return batch\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get a batch of training samples\n",
    "batch = get_batch(train_users, train_items, train_reviews, train_ratings)\n",
    "user, item, rating, item_reviews, user_reviews = batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def weights_init(m):\n",
    "  if isinstance(m, nn.Linear):\n",
    "    nn.init.normal_(m.weight.data)\n",
    "    nn.init.constant_(m.bias.data, 0.0)\n",
    "\n",
    "class FM(nn.Module):\n",
    "    def __init__(self, latent_dim, fea_num):\n",
    "        super().__init__()\n",
    "\n",
    "        self.latent_dim = latent_dim\n",
    "        self.w0 = nn.Parameter(torch.zeros([1, ], requires_grad=True))\n",
    "        self.w1 = nn.Parameter(torch.rand([fea_num, 1], requires_grad=True))\n",
    "        self.w2 = nn.Parameter(torch.rand([fea_num, latent_dim], requires_grad=True))\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        # inputs = inputs.long()\n",
    "        first_order = self.w0 + torch.mm(inputs, self.w1)\n",
    "        second_order = 1/2 * torch.sum(\n",
    "            torch.pow(torch.mm(inputs, self.w2), 2) -\n",
    "            torch.mm(torch.pow(inputs, 2), torch.pow(self.w2, 2)),\n",
    "\n",
    "            dim=1,\n",
    "            keepdim=True\n",
    "        )\n",
    "\n",
    "        return first_order + second_order\n",
    "\n",
    "\n",
    "class DeepFM(nn.Module):\n",
    "  def __init__(self, fea_num, latent_dim):\n",
    "\n",
    "    super().__init__()\n",
    "    \n",
    "    self.w0 = nn.Parameter(torch.zeros([1, ]))\n",
    "    self.w1 = nn.Parameter(torch.rand([fea_num, 1]))\n",
    "    self.w2 = nn.Parameter(torch.rand([fea_num, latent_dim]))\n",
    "\n",
    "    self.mlp = nn.Sequential(\n",
    "        nn.Linear(latent_dim, 64),\n",
    "        nn.ReLU(inplace=True),\n",
    "        nn.Linear(64, 32),\n",
    "        nn.ReLU(inplace=True),\n",
    "        nn.Linear(32, 16),\n",
    "        nn.ReLU(inplace=True),\n",
    "        nn.Linear(16, latent_dim),\n",
    "        nn.ReLU(inplace=True),\n",
    "        nn.Linear(latent_dim, 1)\n",
    "    )\n",
    "\n",
    "  def forward(self, inputs):\n",
    "    '''FM'''\n",
    "    fm_first_order = self.w0 + torch.mm(inputs, self.w1)\n",
    "    fm_second_order = 1/2 * torch.sum(\n",
    "        torch.pow(torch.mm(inputs, self.w2), 2)\n",
    "        -\n",
    "        torch.mm(torch.pow(inputs, 2), torch.pow(self.w2, 2)),\n",
    "        dim=1,\n",
    "        keepdim=True\n",
    "    )\n",
    "    fm_out = fm_first_order + fm_second_order\n",
    "\n",
    "    '''MLP'''\n",
    "    mlp_x = torch.mm(inputs, self.w2)\n",
    "    mlp_out = self.mlp(mlp_x)\n",
    "\n",
    "    return torch.add(fm_out, mlp_out) / 2\n",
    "      \n",
    "\n",
    "\n",
    "class MPCN(nn.Module):\n",
    "  def __init__(self, vocab_size = 100000, embed_size = 50, lw = LW, fm_dim = 8, np=1):\n",
    "      super().__init__()\n",
    "      self.np = np\n",
    "      self.embedding = nn.Embedding(vocab_size, embed_size)\n",
    "      # initialise a weighting matrix of size (50 x 50)\n",
    "      self.W_g = nn.Parameter(torch.rand(embed_size, embed_size, requires_grad=True) * 0.001)\n",
    "      # initialise a bias vector of size (50)\n",
    "      self.b_g = nn.Parameter(torch.zeros(embed_size, requires_grad=True))\n",
    "      # initialise a weighting matrix of size (50 x 50)\n",
    "      self.W_u = nn.Parameter(torch.rand(embed_size, embed_size, requires_grad=True) * 0.001)\n",
    "      # initialise a bias vector of size (50)\n",
    "      self.b_u = nn.Parameter(torch.zeros(embed_size, requires_grad=True))\n",
    "      self.sigmoid = nn.Sigmoid()\n",
    "      self.tanh = nn.Tanh()\n",
    "\n",
    "      self.F_review = nn.Sequential(\n",
    "        nn.Linear(embed_size, 2*embed_size, bias=True),\n",
    "        nn.Sigmoid(),\n",
    "        nn.Dropout(0.2),\n",
    "        nn.Linear(2*embed_size, embed_size, bias=True),\n",
    "        nn.Sigmoid(),\n",
    "        nn.Dropout(0.2),\n",
    "      )\n",
    "\n",
    "      self.F_word = nn.Sequential(\n",
    "        nn.Linear(embed_size, 2*embed_size, bias=True),\n",
    "        nn.Sigmoid(),\n",
    "        nn.Dropout(0.2),\n",
    "        nn.Linear(2*embed_size, embed_size, bias=True),\n",
    "        nn.Sigmoid(),\n",
    "        nn.Dropout(0.2),\n",
    "      )\n",
    "\n",
    "      # initialise a weighting matrix of size (50 x 50)\n",
    "      self.M = nn.Parameter(torch.rand(embed_size, embed_size, requires_grad=True) * 0.001)\n",
    "      self.M_w = nn.Parameter(torch.rand(embed_size, embed_size, requires_grad=True) * 0.001)\n",
    "\n",
    "      self.deepfm = DeepFM(2*embed_size, fm_dim)\n",
    "\n",
    "  def forward(self, a, b):\n",
    "    a = self.embedding(a.long())\n",
    "    b = self.embedding(b.long())\n",
    "    # size of a: (32, 30, 20, 50)\n",
    "    # size of b: (32, 30, 20, 50)\n",
    "\n",
    "    a_sumed = torch.sum(a, dim=2)\n",
    "    b_sumed = torch.sum(b, dim=2)\n",
    "    # size of a_sumed: (32, 30, 50)\n",
    "    # size of b_sumed: (32, 30, 50)\n",
    "\n",
    "    a_bar = self.sigmoid(a_sumed @ self.W_u) + self.b_g * self.tanh(a_sumed @ self.W_g + self.b_u)\n",
    "    b_bar = self.sigmoid(b_sumed @ self.W_u) + self.b_g * self.tanh(b_sumed @ self.W_g + self.b_u)\n",
    "    # size of a_bar: (32, 30, 50)\n",
    "    # size of b_bar: (32, 30, 50)\n",
    "\n",
    "    a_prime_arr = []\n",
    "    b_prime_arr = []\n",
    "\n",
    "\n",
    "    for i in range (self.np):\n",
    "      S = self.F_review(a_bar) @ self.M @ self.F_review(b_bar).permute(0,2,1)\n",
    "      # get the maximum of S columnwise\n",
    "      max_col_S = torch.max(S, dim=1)[0]\n",
    "      # get the maximum of S rowwise\n",
    "      max_row_S = torch.max(S, dim=2)[0]\n",
    "\n",
    "      # size of S: (32, 30, 30)\n",
    "      # size of max_col_S: (32, 30)\n",
    "      # size of max_row_S: (32, 30)\n",
    "\n",
    "      p_a = nn.functional.gumbel_softmax(logits=max_col_S, hard=True, dim=-1, tau=0.1).unsqueeze(1).unsqueeze(2)\n",
    "      p_b = nn.functional.gumbel_softmax(logits=max_row_S, hard=True, dim=-1, tau=0.1).unsqueeze(1).unsqueeze(2)\n",
    "      # size of p_a: (32, 1, 1, 30)\n",
    "      # size of p_b: (32, 1, 1, 30)\n",
    "\n",
    "      ''' This step implements the pointer selection mechanism in the paper \n",
    "      \n",
    "      p_a is originally a two-dimensional tensor of size (32 x 30(LW)), while a, b \\\n",
    "      are originally a four-dimensional tensor of size (32 x 30(LW) x 20 x 50) \\\n",
    "      we need to reshape p_a and p_b to a four-dimensional tensor of size (32, 1, 1, 30) \\\n",
    "      and multiply with reshaped a and b to (32, 20, 30, 50) (a.permute(0,2,1,3)) \\\n",
    "      then we get the correct but not ideally-shaped selected review of size a' = (32, 1, 20, 50) \\\n",
    "      then we squeeze the second dimension (a'.squeeze(1)) of a' to get (32, 20, 50)\n",
    "      \n",
    "      ''' \n",
    "\n",
    "      a_prime = (p_a @ a.permute(0,2,1,3)).permute(0,2,1,3).squeeze(1)\n",
    "      b_prime = (p_b @ b.permute(0,2,1,3)).permute(0,2,1,3).squeeze(1)\n",
    "      # size of a_prime: (32, 20, 50)\n",
    "      # size of b_prime: (32, 20, 50)\n",
    "\n",
    "      a_prime_arr.append(a_prime)\n",
    "      b_prime_arr.append(b_prime)\n",
    "    \n",
    "    a_prime = torch.cat(a_prime_arr, dim=1)\n",
    "    b_prime = torch.cat(b_prime_arr, dim=1)\n",
    "    \n",
    "    W = self.F_word(a_prime) @ self.M_w @ self.F_word(b_prime).permute(0,2,1)\n",
    "    # size of W: (32, 20, 20)\n",
    "\n",
    "    # get the column average of W\n",
    "    avg_col_W = torch.mean(W, dim=1).unsqueeze(2)\n",
    "    # get the row average of W\n",
    "    avg_row_W = torch.mean(W, dim=2).unsqueeze(2)\n",
    "    # size of avg_col_W: (32, 20, 1)\n",
    "    # size of avg_row_W: (32, 20, 1)\n",
    "    \n",
    "    a_bar_prime = self.sigmoid(avg_col_W).permute(0,2,1) @ a_prime\n",
    "    b_bar_prime = self.sigmoid(avg_row_W).permute(0,2,1) @ a_prime\n",
    "    # size of a_bar_prime: (32, 1, 50)\n",
    "    # size of b_bar_prime: (32, 1, 50)\n",
    "\n",
    "    a_bar_prime = a_bar_prime.squeeze(1)\n",
    "    b_bar_prime = b_bar_prime.squeeze(1)\n",
    "\n",
    "    # concatenate a_bar_prime and b_bar_prime\n",
    "    a_bar_prime_bar_prime = torch.cat((a_bar_prime, b_bar_prime), dim=1)\n",
    "    \n",
    "    pred = self.deepfm(a_bar_prime_bar_prime)\n",
    "\n",
    "    # flatten the predictions\n",
    "    pred = pred.squeeze(1)\n",
    "\n",
    "    return pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "mpcn = MPCN().apply(weights_init).to(device)\n",
    "optimiser_mpcn = optim.Adam(mpcn.parameters(), lr=lr, weight_decay=1e-6)\n",
    "\n",
    "def save_training(path = f'{data_save_path}/'):\n",
    "  torch.save(mpcn.state_dict(), path + 'mpcn_p1.pt')\n",
    "  torch.save(optimiser_mpcn.state_dict(), path + 'mpcn_optimiser_p1.pt')\n",
    "  print('Saved model and optimiser')\n",
    "\n",
    "def load_training(path = f'{data_save_path}/'):\n",
    "  mpcn.load_state_dict(torch.load(path + 'mpcn_p1.pt'))\n",
    "  optimiser_mpcn.load_state_dict(torch.load(path + 'mpcn_optimiser_p1.pt'))\n",
    "  print('Loaded model and optimiser')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Program: Evaluating the baseline RMSE of the model on the test set\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3609/3609 [00:56<00:00, 64.44it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Baseline RMSE: 6383.381097900864\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1000/1000 [00:50<00:00, 19.74it/s]\n",
      "100%|██████████| 3609/3609 [00:57<00:00, 62.90it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0 Loss: 62.45726776123047 Test RMSE: 6.853791579608278\n",
      "Saved model and optimiser\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1000/1000 [00:54<00:00, 18.34it/s]\n",
      "100%|██████████| 3609/3609 [00:59<00:00, 60.96it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1 Loss: 13.040255546569824 Test RMSE: 4.150354142927601\n",
      "Saved model and optimiser\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1000/1000 [00:49<00:00, 20.17it/s]\n",
      "100%|██████████| 3609/3609 [00:58<00:00, 61.36it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 2 Loss: 6.503207206726074 Test RMSE: 2.7123325235066105\n",
      "Saved model and optimiser\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1000/1000 [00:54<00:00, 18.35it/s]\n",
      "100%|██████████| 3609/3609 [00:56<00:00, 63.54it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 3 Loss: 7.067770957946777 Test RMSE: 2.055351365963552\n",
      "Saved model and optimiser\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1000/1000 [00:52<00:00, 19.21it/s]\n",
      "100%|██████████| 3609/3609 [01:00<00:00, 59.99it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 4 Loss: 4.691794395446777 Test RMSE: 1.7347197838521922\n",
      "Saved model and optimiser\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1000/1000 [00:50<00:00, 19.90it/s]\n",
      "100%|██████████| 3609/3609 [00:58<00:00, 62.13it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 5 Loss: 3.1622402667999268 Test RMSE: 1.5272515007715604\n",
      "Saved model and optimiser\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1000/1000 [00:54<00:00, 18.30it/s]\n",
      "100%|██████████| 3609/3609 [00:57<00:00, 62.26it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 6 Loss: 2.01723051071167 Test RMSE: 1.387649569808033\n",
      "Saved model and optimiser\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1000/1000 [00:54<00:00, 18.37it/s]\n",
      "100%|██████████| 3609/3609 [01:02<00:00, 57.74it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 7 Loss: 1.4777305126190186 Test RMSE: 1.2975893093463489\n",
      "Saved model and optimiser\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1000/1000 [00:56<00:00, 17.70it/s]\n",
      "100%|██████████| 3609/3609 [01:01<00:00, 58.72it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 8 Loss: 1.3974556922912598 Test RMSE: 1.2362700798531985\n",
      "Saved model and optimiser\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1000/1000 [00:57<00:00, 17.42it/s]\n",
      "100%|██████████| 3609/3609 [01:02<00:00, 58.12it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 9 Loss: 2.7163071632385254 Test RMSE: 1.2040511144928472\n",
      "Saved model and optimiser\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1000/1000 [00:54<00:00, 18.50it/s]\n",
      "100%|██████████| 3609/3609 [01:02<00:00, 58.17it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 10 Loss: 1.3884291648864746 Test RMSE: 1.1926787327980195\n",
      "Saved model and optimiser\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1000/1000 [00:57<00:00, 17.38it/s]\n",
      "100%|██████████| 3609/3609 [01:01<00:00, 58.81it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 11 Loss: 0.5358558893203735 Test RMSE: 1.1786322850340718\n",
      "Saved model and optimiser\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1000/1000 [00:51<00:00, 19.51it/s]\n",
      "100%|██████████| 3609/3609 [00:59<00:00, 60.83it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 12 Loss: 1.6865324974060059 Test RMSE: 1.168100298062095\n",
      "Saved model and optimiser\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1000/1000 [00:51<00:00, 19.51it/s]\n",
      "100%|██████████| 3609/3609 [00:58<00:00, 62.14it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 13 Loss: 0.8364065289497375 Test RMSE: 1.167677474834477\n",
      "Saved model and optimiser\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1000/1000 [00:52<00:00, 18.93it/s]\n",
      "100%|██████████| 3609/3609 [00:58<00:00, 61.34it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 14 Loss: 0.9937040209770203 Test RMSE: 1.1614283172005462\n",
      "Saved model and optimiser\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1000/1000 [00:50<00:00, 19.88it/s]\n",
      "100%|██████████| 3609/3609 [00:58<00:00, 61.66it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 15 Loss: 1.643054485321045 Test RMSE: 1.1687796628022662\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1000/1000 [00:56<00:00, 17.72it/s]\n",
      "100%|██████████| 3609/3609 [01:01<00:00, 58.85it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 16 Loss: 1.7234382629394531 Test RMSE: 1.1626632654392743\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1000/1000 [00:56<00:00, 17.72it/s]\n",
      "100%|██████████| 3609/3609 [01:01<00:00, 58.80it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 17 Loss: 1.2654353380203247 Test RMSE: 1.1602833354925641\n",
      "Saved model and optimiser\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1000/1000 [00:56<00:00, 17.77it/s]\n",
      "100%|██████████| 3609/3609 [01:01<00:00, 58.57it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 18 Loss: 1.1225569248199463 Test RMSE: 1.1568407896285773\n",
      "Saved model and optimiser\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1000/1000 [00:57<00:00, 17.48it/s]\n",
      "100%|██████████| 3609/3609 [01:01<00:00, 59.02it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 19 Loss: 0.7401853799819946 Test RMSE: 1.1561213299883935\n",
      "Saved model and optimiser\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1000/1000 [00:55<00:00, 18.12it/s]\n",
      "100%|██████████| 3609/3609 [01:01<00:00, 58.75it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 20 Loss: 1.2943466901779175 Test RMSE: 1.1570266175382387\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1000/1000 [00:55<00:00, 17.92it/s]\n",
      "100%|██████████| 3609/3609 [00:57<00:00, 62.82it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 21 Loss: 1.4607505798339844 Test RMSE: 1.1602758322438562\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1000/1000 [00:54<00:00, 18.44it/s]\n",
      "100%|██████████| 3609/3609 [01:01<00:00, 58.55it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 22 Loss: 1.576810359954834 Test RMSE: 1.160066010401037\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1000/1000 [00:56<00:00, 17.71it/s]\n",
      "100%|██████████| 3609/3609 [01:01<00:00, 58.86it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 23 Loss: 1.4996401071548462 Test RMSE: 1.1600380639648067\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1000/1000 [00:57<00:00, 17.46it/s]\n",
      "100%|██████████| 3609/3609 [01:01<00:00, 58.75it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 24 Loss: 1.187844157218933 Test RMSE: 1.1566162467432273\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1000/1000 [00:56<00:00, 17.81it/s]\n",
      "100%|██████████| 3609/3609 [01:01<00:00, 58.70it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 25 Loss: 1.3065863847732544 Test RMSE: 1.1577861493860595\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1000/1000 [00:57<00:00, 17.42it/s]\n",
      "100%|██████████| 3609/3609 [01:00<00:00, 59.26it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 26 Loss: 1.4685020446777344 Test RMSE: 1.163452296136125\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1000/1000 [00:56<00:00, 17.60it/s]\n",
      "100%|██████████| 3609/3609 [01:01<00:00, 58.85it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 27 Loss: 1.1270415782928467 Test RMSE: 1.1578776938131095\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1000/1000 [00:56<00:00, 17.73it/s]\n",
      "100%|██████████| 3609/3609 [01:00<00:00, 59.99it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 28 Loss: 1.1200264692306519 Test RMSE: 1.1667065675254011\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1000/1000 [00:57<00:00, 17.26it/s]\n",
      "100%|██████████| 3609/3609 [01:01<00:00, 59.06it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 29 Loss: 1.2600312232971191 Test RMSE: 1.1692447253428326\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1000/1000 [00:56<00:00, 17.66it/s]\n",
      "100%|██████████| 3609/3609 [00:58<00:00, 61.83it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 30 Loss: 1.2501894235610962 Test RMSE: 1.1590744179515304\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1000/1000 [00:52<00:00, 18.96it/s]\n",
      "100%|██████████| 3609/3609 [00:58<00:00, 62.17it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 31 Loss: 1.8198254108428955 Test RMSE: 1.1705525134798231\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1000/1000 [00:53<00:00, 18.56it/s]\n",
      "100%|██████████| 3609/3609 [00:57<00:00, 62.83it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 32 Loss: 1.8371176719665527 Test RMSE: 1.1563703128590614\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1000/1000 [00:45<00:00, 22.16it/s]\n",
      "100%|██████████| 3609/3609 [00:52<00:00, 69.08it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 33 Loss: 0.7392568588256836 Test RMSE: 1.1773403519307784\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1000/1000 [00:44<00:00, 22.52it/s]\n",
      "100%|██████████| 3609/3609 [00:52<00:00, 68.69it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 34 Loss: 1.2563724517822266 Test RMSE: 1.1579477420867925\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1000/1000 [00:44<00:00, 22.28it/s]\n",
      "100%|██████████| 3609/3609 [00:52<00:00, 68.94it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 35 Loss: 1.601342797279358 Test RMSE: 1.1761498720766799\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1000/1000 [00:43<00:00, 22.74it/s]\n",
      "100%|██████████| 3609/3609 [00:52<00:00, 68.93it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 36 Loss: 1.7325950860977173 Test RMSE: 1.164426777543834\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1000/1000 [00:44<00:00, 22.41it/s]\n",
      "100%|██████████| 3609/3609 [00:49<00:00, 72.20it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 37 Loss: 1.4667139053344727 Test RMSE: 1.161170880313592\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1000/1000 [00:36<00:00, 27.45it/s]\n",
      "100%|██████████| 3609/3609 [00:47<00:00, 75.43it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 38 Loss: 1.0771491527557373 Test RMSE: 1.1592055894589286\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1000/1000 [00:41<00:00, 24.13it/s]\n",
      "100%|██████████| 3609/3609 [00:48<00:00, 75.13it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 39 Loss: 0.43612024188041687 Test RMSE: 1.1822737908389767\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "'''Training'''\n",
    "RMSE = []\n",
    "best_rmse = 1e6\n",
    "\n",
    "def evaluation(test_batch_size = 32):\n",
    "  test_users_unique = list(set(test_users))\n",
    "  rmse_arr = []\n",
    "  for i in tqdm(range(0, len(test_users_unique), test_batch_size)):\n",
    "    test_users_batch = test_users_unique[i:i+test_batch_size]\n",
    "    batch_test = get_batch(test_users, test_items, test_reviews, test_ratings, fixed_users_set=test_users_batch)\n",
    "    user, item, rating, item_reviews, user_reviews = batch_test\n",
    "    user, item, rating, item_reviews, user_reviews = user.to(device), item.to(device), rating.to(device), item_reviews.to(device), user_reviews.to(device)\n",
    "\n",
    "    pred = mpcn(item_reviews, user_reviews)\n",
    "\n",
    "    # calculate rooted mean square error\n",
    "    rmse_arr.append(torch.sqrt(torch.mean((pred - rating)**2)).item())\n",
    "  \n",
    "  # return the mean of the rmse_arr\n",
    "  return np.mean(rmse_arr)\n",
    "\n",
    "print(\"Program: Evaluating the baseline RMSE of the model on the test set\")\n",
    "\n",
    "# set best_rmse to be the largest possible value\n",
    "best_rmse = evaluation()\n",
    "RMSE.append(best_rmse)\n",
    "\n",
    "# print baseline best_rmse\n",
    "print('Baseline RMSE:', best_rmse)\n",
    "\n",
    "for epoch in range (max_epoch):\n",
    "  for i in tqdm(range(1000)):\n",
    "    batch =  get_batch(train_users, train_items, train_reviews, train_ratings)\n",
    "    user, item, rating, item_reviews, user_reviews = batch\n",
    "    user, item, rating, item_reviews, user_reviews = user.to(device), item.to(device), rating.to(device), item_reviews.to(device), user_reviews.to(device)\n",
    "\n",
    "    pred = mpcn(item_reviews, user_reviews)\n",
    "\n",
    "    # calculate the loss\n",
    "    loss = torch.mean((pred - rating)**2)\n",
    "\n",
    "    optimiser_mpcn.zero_grad()\n",
    "    loss.backward()\n",
    "    optimiser_mpcn.step()\n",
    "\n",
    "  '''evaluate the model on the test set'''\n",
    "  rmse_test = evaluation()\n",
    "  RMSE.append(rmse_test)\n",
    "  # print out the loss of the models and the test rmse\n",
    "  print('Epoch:', epoch, 'Loss:', loss.item(), 'Test RMSE:', rmse_test)\n",
    "  if rmse_test < best_rmse:\n",
    "    best_rmse = rmse_test\n",
    "    save_training()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open (f'{analytics_path}/rmse_p1_deepfm.txt', 'w') as f:\n",
    "  for r in RMSE:\n",
    "    f.write(str(np.round(r, 4)) + '\\n')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.13 ('project')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "55dd00c9eb77ed7777fb0dde425a704f9d443acf14be2e60396578eee1e9e5b0"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
