{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "from transformers import AlbertTokenizerFast, AlbertModel\n",
    "# from gensim.models import Word2Vec\n",
    "# from nets import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://zh-v2.d2l.ai/chapter_natural-language-processing-applications/sentiment-analysis-cnn.html\n",
    "class GloveEmbedding:\n",
    "    \"\"\"Token Embedding.\"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        self.idx_to_token, self.idx_to_vec = self._load_embedding()\n",
    "        self.unknown_idx = 0\n",
    "        # a dictionary like {'best':1} from idx_to_token\n",
    "        self.token_to_idx = {\n",
    "            token: idx for idx, token in enumerate(self.idx_to_token)}\n",
    "\n",
    "    def _load_embedding(self):\n",
    "        idx_to_token, idx_to_vec = ['<unk>'], []\n",
    "        data_dir = '../../Datasets/Model/nlp/'\n",
    "        # GloVe website: https://nlp.stanford.edu/projects/glove/\n",
    "        # fastText website: https://fasttext.cc/\n",
    "        with open(os.path.join(data_dir, 'vec.txt'), encoding='gb18030', errors='ignore') as f:\n",
    "            for line in f:\n",
    "                elems = line.rstrip().split(' ')\n",
    "                # Skip header information, such as the top row in fastText\n",
    "                # structure: hello  [0.1, 0.2, 0.3, ...]\n",
    "                token, elems = elems[0], [float(elem) for elem in elems[1:]]\n",
    "                if len(elems) > 1:\n",
    "                    idx_to_token.append(token)\n",
    "                    idx_to_vec.append(elems)\n",
    "        idx_to_vec = [[0] * len(idx_to_vec[0])] + idx_to_vec\n",
    "        return idx_to_token, torch.tensor(idx_to_vec)\n",
    "\n",
    "    def __getitem__(self, tokens):\n",
    "        indices = [\n",
    "            self.token_to_idx.get(token, self.unknown_idx)\n",
    "            for token in tokens]\n",
    "        vecs = self.idx_to_vec[torch.tensor(indices)]\n",
    "        return vecs\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.idx_to_token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''CONSTANTS'''\n",
    "batch_size = 32\n",
    "max_epoch = 40\n",
    "lr = 0.001\n",
    "\n",
    "LD = 30\n",
    "LW = 20\n",
    "\n",
    "device = torch.device(\n",
    "    'cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
    "train_data_path = '../../Datasets/Model/train'\n",
    "test_data_path = '../../Datasets/Model/test'\n",
    "image_path = '../../Datasets/Images'\n",
    "data_save_path = '../../Save/Experiment5'\n",
    "analytics_path = '../../Analysis/Experiment5'\n",
    "nlp_path = '../../Datasets/Model/nlp'\n",
    "\n",
    "'''NLP'''\n",
    "# load word2vec models from nlp_path\n",
    "# model_review = Word2Vec.load(f'{nlp_path}/model_review.model')\n",
    "# model_title = Word2Vec.load(f'{nlp_path}/model_title.model')\n",
    "\n",
    "# get the vocabulary of reviews titles\n",
    "# vocab_review = model_review.wv.index_to_key\n",
    "# vocab_title = model_title.wv.index_to_key\n",
    "\n",
    "# # get the vectors of reviews and titles\n",
    "# vecs_review = torch.tensor(model_review.wv[vocab_review])\n",
    "# vecs_title = torch.tensor(model_title.wv[vocab_title])\n",
    "\n",
    "# get the glove embeddings\n",
    "# print(\"Loading glove embeddings...\")\n",
    "# glove = GloveEmbedding()\n",
    "\n",
    "\n",
    "'''ANALYTICS'''\n",
    "\n",
    "rmse_arr = []\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda')"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_users = np.load(f'{train_data_path}/train_users.npy')\n",
    "train_items = np.load(f'{train_data_path}/train_items.npy')\n",
    "train_ratings = np.load(f'{train_data_path}/train_ratings.npy')\n",
    "train_reviews = np.load(\n",
    "    f'{train_data_path}/train_reviews.npy', allow_pickle=True)\n",
    "train_descriptions = np.load(\n",
    "    f'{train_data_path}/train_descriptions.npy', allow_pickle=True)\n",
    "train_titles = np.load(\n",
    "    f'{train_data_path}/train_titles.npy', allow_pickle=True)\n",
    "train_prices = np.load(f'{train_data_path}/train_prices.npy')\n",
    "train_categories = np.load(f'{train_data_path}/train_categories.npy')\n",
    "\n",
    "test_users = np.load(f'{test_data_path}/test_users.npy')\n",
    "test_items = np.load(f'{test_data_path}/test_items.npy')\n",
    "test_ratings = np.load(f'{test_data_path}/test_ratings.npy')\n",
    "test_reviews = np.load(f'{test_data_path}/test_reviews.npy', allow_pickle=True)\n",
    "test_descriptions = np.load(\n",
    "    f'{test_data_path}/test_descriptions.npy', allow_pickle=True)\n",
    "test_titles = np.load(\n",
    "    f'{test_data_path}/test_titles.npy', allow_pickle=True)\n",
    "test_prices = np.load(f'{test_data_path}/test_prices.npy')\n",
    "test_categories = np.load(f'{test_data_path}/test_categories.npy')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "72693"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max(test_items)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this function accepts a sequence, and pads the setences in the sequence to LW length, then returns the padded sequence\n",
    "def pad_sequence(sequence, lw = LW, ld = LD, placeholder=0):\n",
    "    arr = []\n",
    "    for sentence in sequence:\n",
    "      if len(sentence) < lw:\n",
    "        arr.append(sentence + [placeholder] * (lw - len(sentence)))\n",
    "      else:\n",
    "        arr.append(sentence[:lw])\n",
    "\n",
    "    if len(arr) > ld:\n",
    "      arr = arr[:ld]\n",
    "    else:\n",
    "      for i in range (ld-len(arr)):\n",
    "        arr.append([placeholder] * lw)\n",
    "        \n",
    "    return arr\n",
    "\n",
    "'''sample user'''\n",
    "'''this function takes a user, who is from a set of users, a set of items, a set of reviews, a set of titles, \n",
    "a set of categories, a set of prices, and a set of ratings.'''\n",
    "def sample_user(user, users, items, reviews, ratings):\n",
    "  # find out all the indicies of the user in users\n",
    "  user_indices = np.where(users == user)[0]\n",
    "  # randomly select an index from these indicies.\n",
    "  user_index = np.random.choice(user_indices)\n",
    "\n",
    "  # get the item that the user has purchased.\n",
    "  item = items[user_index]\n",
    "\n",
    "  # get the rating that the user has given to that item.\n",
    "  rating = ratings[user_index]\n",
    "\n",
    "  # find out the indices of the item in items\n",
    "  item_indices = np.where(items == item)[0]\n",
    "\n",
    "  # get the reviews of that item\n",
    "  item_reviews = [reviews[i] for i in item_indices if i != user_index]\n",
    "  user_reviews = [reviews[i] for i in user_indices if i != user_index]\n",
    "\n",
    "  user_reviews = pad_sequence(user_reviews)\n",
    "  item_reviews = pad_sequence(item_reviews)\n",
    "\n",
    "  return user, item, rating, item_reviews, user_reviews\n",
    "\n",
    "\n",
    "'''function to get a batch of training samples.\n",
    "this function selects a random user from a set of users\n",
    "and gets his information by function sample_user, repeat it for batch_size times and \n",
    "gets the batch of samples. Each element in the batch tuple is then transormed to a pytorch tensor and returned.'''\n",
    "def get_batch(users, items, reviews, ratings, batch_size=32, fixed_users_set = None):\n",
    "  # get a batch of users\n",
    "  if fixed_users_set is None:\n",
    "    batch_users = np.random.choice(users, size=batch_size)\n",
    "  else:\n",
    "    batch_users = fixed_users_set\n",
    "  # get the batch of samples\n",
    "  batch = [list(sample_user(user, users, items, reviews, ratings)) for user in batch_users]\n",
    "  # transform each column of the batch to a pytorch tensor\n",
    "  batch = [torch.tensor(sample) for sample in zip(*batch)]\n",
    "  return batch\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get a batch of training samples\n",
    "batch = get_batch(train_users, train_items, train_reviews, train_ratings)\n",
    "user, item, rating, item_reviews, user_reviews = batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def weights_init(m):\n",
    "  if isinstance(m, nn.Linear):\n",
    "    nn.init.normal_(m.weight.data)\n",
    "    nn.init.constant_(m.bias.data, 0.0)\n",
    "\n",
    "class FM(nn.Module):\n",
    "    def __init__(self, latent_dim, fea_num):\n",
    "        super().__init__()\n",
    "\n",
    "        self.latent_dim = latent_dim\n",
    "        self.w0 = nn.Parameter(torch.zeros([1, ], requires_grad=True))\n",
    "        self.w1 = nn.Parameter(torch.rand([fea_num, 1], requires_grad=True))\n",
    "        self.w2 = nn.Parameter(torch.rand([fea_num, latent_dim], requires_grad=True))\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        # inputs = inputs.long()\n",
    "        first_order = self.w0 + torch.mm(inputs, self.w1)\n",
    "        second_order = 1/2 * torch.sum(\n",
    "            torch.pow(torch.mm(inputs, self.w2), 2) -\n",
    "            torch.mm(torch.pow(inputs, 2), torch.pow(self.w2, 2)),\n",
    "\n",
    "            dim=1,\n",
    "            keepdim=True\n",
    "        )\n",
    "\n",
    "        return first_order + second_order\n",
    "\n",
    "\n",
    "class NCF(nn.Module):\n",
    "  def __init__(self, fea_num, latent_dim):\n",
    "    super().__init__()\n",
    "\n",
    "    self.mlp = nn.Sequential(\n",
    "        nn.Linear(fea_num, 8*latent_dim),\n",
    "        nn.ReLU(),\n",
    "        nn.Linear(8*latent_dim, 4*latent_dim),\n",
    "        nn.ReLU(),\n",
    "        nn.Linear(4*latent_dim, 2*latent_dim),\n",
    "        nn.ReLU(),\n",
    "        nn.Linear(2*latent_dim, latent_dim),\n",
    "        nn.ReLU(),\n",
    "        nn.Linear(latent_dim, 1)\n",
    "    )\n",
    "\n",
    "  \n",
    "  def forward(self, inputs):\n",
    "        out = self.mlp(inputs)\n",
    "\n",
    "        return out\n",
    "      \n",
    "\n",
    "\n",
    "class MPCN(nn.Module):\n",
    "  def __init__(self, vocab_size = 100000, embed_size = 50, lw = LW, fm_dim = 8, np=1):\n",
    "      super().__init__()\n",
    "      self.np = np\n",
    "      self.embedding = nn.Embedding(vocab_size, embed_size)\n",
    "      # initialise a weighting matrix of size (50 x 50)\n",
    "      self.W_g = nn.Parameter(torch.rand(embed_size, embed_size, requires_grad=True) * 0.001)\n",
    "      # initialise a bias vector of size (50)\n",
    "      self.b_g = nn.Parameter(torch.zeros(embed_size, requires_grad=True))\n",
    "      # initialise a weighting matrix of size (50 x 50)\n",
    "      self.W_u = nn.Parameter(torch.rand(embed_size, embed_size, requires_grad=True) * 0.001)\n",
    "      # initialise a bias vector of size (50)\n",
    "      self.b_u = nn.Parameter(torch.zeros(embed_size, requires_grad=True))\n",
    "      self.sigmoid = nn.Sigmoid()\n",
    "      self.tanh = nn.Tanh()\n",
    "\n",
    "      self.F_review = nn.Sequential(\n",
    "        nn.Linear(embed_size, 2*embed_size, bias=True),\n",
    "        nn.Sigmoid(),\n",
    "        nn.Dropout(0.2),\n",
    "        nn.Linear(2*embed_size, embed_size, bias=True),\n",
    "        nn.Sigmoid(),\n",
    "        nn.Dropout(0.2),\n",
    "      )\n",
    "\n",
    "      self.F_word = nn.Sequential(\n",
    "        nn.Linear(embed_size, 2*embed_size, bias=True),\n",
    "        nn.Sigmoid(),\n",
    "        nn.Dropout(0.2),\n",
    "        nn.Linear(2*embed_size, embed_size, bias=True),\n",
    "        nn.Sigmoid(),\n",
    "        nn.Dropout(0.2),\n",
    "      )\n",
    "\n",
    "      # initialise a weighting matrix of size (50 x 50)\n",
    "      self.M = nn.Parameter(torch.rand(embed_size, embed_size, requires_grad=True) * 0.001)\n",
    "      self.M_w = nn.Parameter(torch.rand(embed_size, embed_size, requires_grad=True) * 0.001)\n",
    "\n",
    "      self.ncf = NCF(2 * embed_size, fm_dim)\n",
    "\n",
    "  def forward(self, a, b):\n",
    "    a = self.embedding(a.long())\n",
    "    b = self.embedding(b.long())\n",
    "    # size of a: (32, 30, 20, 50)\n",
    "    # size of b: (32, 30, 20, 50)\n",
    "\n",
    "    a_sumed = torch.sum(a, dim=2)\n",
    "    b_sumed = torch.sum(b, dim=2)\n",
    "    # size of a_sumed: (32, 30, 50)\n",
    "    # size of b_sumed: (32, 30, 50)\n",
    "\n",
    "    a_bar = self.sigmoid(a_sumed @ self.W_u) + self.b_g * self.tanh(a_sumed @ self.W_g + self.b_u)\n",
    "    b_bar = self.sigmoid(b_sumed @ self.W_u) + self.b_g * self.tanh(b_sumed @ self.W_g + self.b_u)\n",
    "    # size of a_bar: (32, 30, 50)\n",
    "    # size of b_bar: (32, 30, 50)\n",
    "\n",
    "    a_prime_arr = []\n",
    "    b_prime_arr = []\n",
    "\n",
    "\n",
    "    for i in range (self.np):\n",
    "      S = self.F_review(a_bar) @ self.M @ self.F_review(b_bar).permute(0,2,1)\n",
    "      # get the maximum of S columnwise\n",
    "      max_col_S = torch.max(S, dim=1)[0]\n",
    "      # get the maximum of S rowwise\n",
    "      max_row_S = torch.max(S, dim=2)[0]\n",
    "\n",
    "      # size of S: (32, 30, 30)\n",
    "      # size of max_col_S: (32, 30)\n",
    "      # size of max_row_S: (32, 30)\n",
    "\n",
    "      p_a = nn.functional.gumbel_softmax(logits=max_col_S, hard=True, dim=-1, tau=0.1).unsqueeze(1).unsqueeze(2)\n",
    "      p_b = nn.functional.gumbel_softmax(logits=max_row_S, hard=True, dim=-1, tau=0.1).unsqueeze(1).unsqueeze(2)\n",
    "      # size of p_a: (32, 1, 1, 30)\n",
    "      # size of p_b: (32, 1, 1, 30)\n",
    "\n",
    "      ''' This step implements the pointer selection mechanism in the paper \n",
    "      \n",
    "      p_a is originally a two-dimensional tensor of size (32 x 30(LW)), while a, b \\\n",
    "      are originally a four-dimensional tensor of size (32 x 30(LW) x 20 x 50) \\\n",
    "      we need to reshape p_a and p_b to a four-dimensional tensor of size (32, 1, 1, 30) \\\n",
    "      and multiply with reshaped a and b to (32, 20, 30, 50) (a.permute(0,2,1,3)) \\\n",
    "      then we get the correct but not ideally-shaped selected review of size a' = (32, 1, 20, 50) \\\n",
    "      then we squeeze the second dimension (a'.squeeze(1)) of a' to get (32, 20, 50)\n",
    "      \n",
    "      ''' \n",
    "\n",
    "      a_prime = (p_a @ a.permute(0,2,1,3)).permute(0,2,1,3).squeeze(1)\n",
    "      b_prime = (p_b @ b.permute(0,2,1,3)).permute(0,2,1,3).squeeze(1)\n",
    "      # size of a_prime: (32, 20, 50)\n",
    "      # size of b_prime: (32, 20, 50)\n",
    "\n",
    "      a_prime_arr.append(a_prime)\n",
    "      b_prime_arr.append(b_prime)\n",
    "    \n",
    "    a_prime = torch.cat(a_prime_arr, dim=1)\n",
    "    b_prime = torch.cat(b_prime_arr, dim=1)\n",
    "    \n",
    "    W = self.F_word(a_prime) @ self.M_w @ self.F_word(b_prime).permute(0,2,1)\n",
    "    # size of W: (32, 20, 20)\n",
    "\n",
    "    # get the column average of W\n",
    "    avg_col_W = torch.mean(W, dim=1).unsqueeze(2)\n",
    "    # get the row average of W\n",
    "    avg_row_W = torch.mean(W, dim=2).unsqueeze(2)\n",
    "    # size of avg_col_W: (32, 20, 1)\n",
    "    # size of avg_row_W: (32, 20, 1)\n",
    "    \n",
    "    a_bar_prime = self.sigmoid(avg_col_W).permute(0,2,1) @ a_prime\n",
    "    b_bar_prime = self.sigmoid(avg_row_W).permute(0,2,1) @ a_prime\n",
    "    # size of a_bar_prime: (32, 1, 50)\n",
    "    # size of b_bar_prime: (32, 1, 50)\n",
    "\n",
    "    a_bar_prime = a_bar_prime.squeeze(1)\n",
    "    b_bar_prime = b_bar_prime.squeeze(1)\n",
    "\n",
    "    # concatenate a_bar_prime and b_bar_prime\n",
    "    a_bar_prime_bar_prime = torch.cat((a_bar_prime, b_bar_prime), dim=1)\n",
    "    \n",
    "    pred = self.ncf(a_bar_prime_bar_prime)\n",
    "\n",
    "    # flatten the predictions\n",
    "    pred = pred.squeeze(1)\n",
    "\n",
    "    return pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "mpcn = MPCN().apply(weights_init).to(device)\n",
    "optimiser_mpcn = optim.Adam(mpcn.parameters(), lr=lr, weight_decay=1e-6)\n",
    "\n",
    "def save_training(path = f'{data_save_path}/'):\n",
    "  torch.save(mpcn.state_dict(), path + 'mpcn_p1.pt')\n",
    "  torch.save(optimiser_mpcn.state_dict(), path + 'mpcn_optimiser_p1.pt')\n",
    "  print('Saved model and optimiser')\n",
    "\n",
    "def load_training(path = f'{data_save_path}/'):\n",
    "  mpcn.load_state_dict(torch.load(path + 'mpcn_p1.pt'))\n",
    "  optimiser_mpcn.load_state_dict(torch.load(path + 'mpcn_optimiser_p1.pt'))\n",
    "  print('Loaded model and optimiser')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Program: Evaluating the baseline RMSE of the model on the test set\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3609/3609 [00:47<00:00, 76.63it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Baseline RMSE: 3401.4275120845887\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1000/1000 [00:36<00:00, 27.35it/s]\n",
      "100%|██████████| 3609/3609 [00:45<00:00, 79.43it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0 Loss: 19.550861358642578 Test RMSE: 3.503962274402355\n",
      "Saved model and optimiser\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1000/1000 [00:41<00:00, 23.99it/s]\n",
      "100%|██████████| 3609/3609 [00:47<00:00, 76.07it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1 Loss: 4.081015110015869 Test RMSE: 2.135879512010238\n",
      "Saved model and optimiser\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1000/1000 [00:37<00:00, 26.68it/s]\n",
      "100%|██████████| 3609/3609 [00:48<00:00, 74.12it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 2 Loss: 3.668989658355713 Test RMSE: 1.6528276474101289\n",
      "Saved model and optimiser\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1000/1000 [00:37<00:00, 26.70it/s]\n",
      "100%|██████████| 3609/3609 [00:45<00:00, 79.95it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 3 Loss: 1.5314949750900269 Test RMSE: 1.4317819406927441\n",
      "Saved model and optimiser\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1000/1000 [00:38<00:00, 25.77it/s]\n",
      "100%|██████████| 3609/3609 [00:47<00:00, 75.95it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 4 Loss: 2.005390167236328 Test RMSE: 1.3287098977862384\n",
      "Saved model and optimiser\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1000/1000 [00:37<00:00, 26.52it/s]\n",
      "100%|██████████| 3609/3609 [00:52<00:00, 69.08it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 5 Loss: 2.1778438091278076 Test RMSE: 1.282813455415056\n",
      "Saved model and optimiser\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1000/1000 [00:47<00:00, 20.95it/s]\n",
      "100%|██████████| 3609/3609 [00:56<00:00, 64.12it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 6 Loss: 1.5074999332427979 Test RMSE: 1.2408966995170818\n",
      "Saved model and optimiser\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1000/1000 [00:49<00:00, 20.00it/s]\n",
      "100%|██████████| 3609/3609 [00:59<00:00, 60.26it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 7 Loss: 1.3124845027923584 Test RMSE: 1.2023690574120933\n",
      "Saved model and optimiser\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1000/1000 [00:49<00:00, 20.38it/s]\n",
      "100%|██████████| 3609/3609 [00:56<00:00, 64.22it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 8 Loss: 1.4972972869873047 Test RMSE: 1.1885572844642984\n",
      "Saved model and optimiser\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1000/1000 [00:53<00:00, 18.70it/s]\n",
      "100%|██████████| 3609/3609 [00:57<00:00, 62.30it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 9 Loss: 1.0240168571472168 Test RMSE: 1.1723066225016205\n",
      "Saved model and optimiser\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1000/1000 [00:48<00:00, 20.79it/s]\n",
      "100%|██████████| 3609/3609 [00:59<00:00, 60.99it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 10 Loss: 1.1144740581512451 Test RMSE: 1.1640995261609042\n",
      "Saved model and optimiser\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1000/1000 [00:51<00:00, 19.27it/s]\n",
      "100%|██████████| 3609/3609 [00:55<00:00, 65.23it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 11 Loss: 0.9677311778068542 Test RMSE: 1.1642094979856972\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1000/1000 [00:51<00:00, 19.42it/s]\n",
      "100%|██████████| 3609/3609 [00:59<00:00, 60.32it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 12 Loss: 0.8899453282356262 Test RMSE: 1.1651077059769306\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1000/1000 [00:49<00:00, 20.20it/s]\n",
      "100%|██████████| 3609/3609 [01:00<00:00, 60.03it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 13 Loss: 0.9913767576217651 Test RMSE: 1.1598792160175024\n",
      "Saved model and optimiser\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1000/1000 [00:57<00:00, 17.52it/s]\n",
      "100%|██████████| 3609/3609 [01:01<00:00, 58.79it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 14 Loss: 1.1981946229934692 Test RMSE: 1.1541492714053476\n",
      "Saved model and optimiser\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1000/1000 [00:52<00:00, 18.97it/s]\n",
      "100%|██████████| 3609/3609 [01:02<00:00, 58.17it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 15 Loss: 1.2084776163101196 Test RMSE: 1.1601476449654617\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1000/1000 [00:55<00:00, 17.98it/s]\n",
      "100%|██████████| 3609/3609 [00:59<00:00, 60.89it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 16 Loss: 1.6595919132232666 Test RMSE: 1.1615920372536455\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1000/1000 [00:54<00:00, 18.39it/s]\n",
      "100%|██████████| 3609/3609 [01:02<00:00, 57.97it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 17 Loss: 1.0950918197631836 Test RMSE: 1.1601845515372187\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1000/1000 [00:53<00:00, 18.65it/s]\n",
      "100%|██████████| 3609/3609 [00:58<00:00, 62.13it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 18 Loss: 1.785452127456665 Test RMSE: 1.1578429785089237\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1000/1000 [00:51<00:00, 19.39it/s]\n",
      "100%|██████████| 3609/3609 [00:57<00:00, 62.48it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 19 Loss: 1.2797178030014038 Test RMSE: 1.161452292504156\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1000/1000 [00:48<00:00, 20.58it/s]\n",
      "100%|██████████| 3609/3609 [00:58<00:00, 61.32it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 20 Loss: 1.7926697731018066 Test RMSE: 1.1580032561254092\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1000/1000 [00:52<00:00, 19.23it/s]\n",
      "100%|██████████| 3609/3609 [00:56<00:00, 63.70it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 21 Loss: 1.5071146488189697 Test RMSE: 1.1570751427051664\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1000/1000 [00:49<00:00, 20.03it/s]\n",
      "100%|██████████| 3609/3609 [00:59<00:00, 60.28it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 22 Loss: 1.7680213451385498 Test RMSE: 1.155981690228299\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1000/1000 [00:55<00:00, 18.18it/s]\n",
      "100%|██████████| 3609/3609 [00:59<00:00, 60.16it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 23 Loss: 0.7923392057418823 Test RMSE: 1.1590585085816514\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1000/1000 [00:55<00:00, 17.95it/s]\n",
      "100%|██████████| 3609/3609 [01:00<00:00, 59.30it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 24 Loss: 0.8759399056434631 Test RMSE: 1.1580414258069562\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1000/1000 [00:53<00:00, 18.59it/s]\n",
      "100%|██████████| 3609/3609 [01:01<00:00, 58.87it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 25 Loss: 1.7028827667236328 Test RMSE: 1.1575512494958977\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1000/1000 [00:56<00:00, 17.70it/s]\n",
      "100%|██████████| 3609/3609 [01:00<00:00, 59.95it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 26 Loss: 0.8535757064819336 Test RMSE: 1.159193335047849\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1000/1000 [00:54<00:00, 18.25it/s]\n",
      "100%|██████████| 3609/3609 [01:00<00:00, 59.17it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 27 Loss: 1.1584547758102417 Test RMSE: 1.1580451457379564\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1000/1000 [00:50<00:00, 19.70it/s]\n",
      "100%|██████████| 3609/3609 [00:56<00:00, 63.56it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 28 Loss: 0.8191600441932678 Test RMSE: 1.1578383684554698\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1000/1000 [00:55<00:00, 18.08it/s]\n",
      "100%|██████████| 3609/3609 [01:00<00:00, 59.63it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 29 Loss: 1.9517998695373535 Test RMSE: 1.1593601434440222\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1000/1000 [00:54<00:00, 18.35it/s]\n",
      "100%|██████████| 3609/3609 [01:00<00:00, 59.32it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 30 Loss: 1.496029257774353 Test RMSE: 1.157070606723985\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1000/1000 [00:56<00:00, 17.74it/s]\n",
      "100%|██████████| 3609/3609 [01:00<00:00, 59.94it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 31 Loss: 1.8208281993865967 Test RMSE: 1.154004019031435\n",
      "Saved model and optimiser\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1000/1000 [00:55<00:00, 18.17it/s]\n",
      "100%|██████████| 3609/3609 [01:00<00:00, 59.35it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 32 Loss: 1.6306430101394653 Test RMSE: 1.1592333792451017\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1000/1000 [00:55<00:00, 18.04it/s]\n",
      "100%|██████████| 3609/3609 [01:00<00:00, 60.14it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 33 Loss: 0.7999215126037598 Test RMSE: 1.1590965486656235\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1000/1000 [00:55<00:00, 18.02it/s]\n",
      "100%|██████████| 3609/3609 [01:00<00:00, 59.74it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 34 Loss: 1.41782808303833 Test RMSE: 1.1626644622721345\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1000/1000 [00:54<00:00, 18.33it/s]\n",
      "100%|██████████| 3609/3609 [01:00<00:00, 59.78it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 35 Loss: 0.8863755464553833 Test RMSE: 1.159613502970294\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1000/1000 [00:56<00:00, 17.55it/s]\n",
      "100%|██████████| 3609/3609 [01:00<00:00, 59.31it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 36 Loss: 1.3144127130508423 Test RMSE: 1.1617925677744758\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1000/1000 [00:54<00:00, 18.43it/s]\n",
      "100%|██████████| 3609/3609 [00:57<00:00, 63.30it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 37 Loss: 2.2919869422912598 Test RMSE: 1.1543264689621222\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1000/1000 [00:51<00:00, 19.44it/s]\n",
      "100%|██████████| 3609/3609 [00:57<00:00, 62.49it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 38 Loss: 1.7055588960647583 Test RMSE: 1.162048129369493\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1000/1000 [00:52<00:00, 19.00it/s]\n",
      "100%|██████████| 3609/3609 [00:56<00:00, 63.42it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 39 Loss: 1.403722882270813 Test RMSE: 1.157728889091552\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "'''Training'''\n",
    "RMSE = []\n",
    "best_rmse = 1e6\n",
    "\n",
    "def evaluation(test_batch_size = 32):\n",
    "  test_users_unique = list(set(test_users))\n",
    "  rmse_arr = []\n",
    "  for i in tqdm(range(0, len(test_users_unique), test_batch_size)):\n",
    "    test_users_batch = test_users_unique[i:i+test_batch_size]\n",
    "    batch_test = get_batch(test_users, test_items, test_reviews, test_ratings, fixed_users_set=test_users_batch)\n",
    "    user, item, rating, item_reviews, user_reviews = batch_test\n",
    "    user, item, rating, item_reviews, user_reviews = user.to(device), item.to(device), rating.to(device), item_reviews.to(device), user_reviews.to(device)\n",
    "\n",
    "    pred = mpcn(item_reviews, user_reviews)\n",
    "\n",
    "    # calculate rooted mean square error\n",
    "    rmse_arr.append(torch.sqrt(torch.mean((pred - rating)**2)).item())\n",
    "  \n",
    "  # return the mean of the rmse_arr\n",
    "  return np.mean(rmse_arr)\n",
    "\n",
    "print(\"Program: Evaluating the baseline RMSE of the model on the test set\")\n",
    "\n",
    "# set best_rmse to be the largest possible value\n",
    "best_rmse = evaluation()\n",
    "RMSE.append(best_rmse)\n",
    "\n",
    "# print baseline best_rmse\n",
    "print('Baseline RMSE:', best_rmse)\n",
    "\n",
    "for epoch in range (max_epoch):\n",
    "  for i in tqdm(range(1000)):\n",
    "    batch =  get_batch(train_users, train_items, train_reviews, train_ratings)\n",
    "    user, item, rating, item_reviews, user_reviews = batch\n",
    "    user, item, rating, item_reviews, user_reviews = user.to(device), item.to(device), rating.to(device), item_reviews.to(device), user_reviews.to(device)\n",
    "\n",
    "    pred = mpcn(item_reviews, user_reviews)\n",
    "\n",
    "    # calculate the loss\n",
    "    loss = torch.mean((pred - rating)**2)\n",
    "\n",
    "    optimiser_mpcn.zero_grad()\n",
    "    loss.backward()\n",
    "    optimiser_mpcn.step()\n",
    "\n",
    "  '''evaluate the model on the test set'''\n",
    "  rmse_test = evaluation()\n",
    "  RMSE.append(rmse_test)\n",
    "  # print out the loss of the models and the test rmse\n",
    "  print('Epoch:', epoch, 'Loss:', loss.item(), 'Test RMSE:', rmse_test)\n",
    "  if rmse_test < best_rmse:\n",
    "    best_rmse = rmse_test\n",
    "    save_training()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open (f'{analytics_path}/rmse_p1_ncf.txt', 'w') as f:\n",
    "  for r in RMSE:\n",
    "    f.write(str(np.round(r, 4)) + '\\n')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.13 ('project')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "55dd00c9eb77ed7777fb0dde425a704f9d443acf14be2e60396578eee1e9e5b0"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
