{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\zhouzihan\\.conda\\envs\\project\\lib\\site-packages\\tqdm\\auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchvision\n",
    "from torchvision import transforms\n",
    "import matplotlib.pyplot as plt\n",
    "from torch.optim import Adam\n",
    "from tqdm import tqdm\n",
    "from torch.utils.data import DataLoader\n",
    "import random\n",
    "from collections import Counter\n",
    "import math\n",
    "from PIL import Image\n",
    "from pathlib import Path\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 32\n",
    "max_epoch = 10\n",
    "device = torch.device(\n",
    "    'cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
    "train_data_path = './numpy_datasets/train'\n",
    "test_data_path = './numpy_datasets/test'\n",
    "image_path = '../Images'\n",
    "data_save_path = './TransNets'\n",
    "lr = 0.000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyDataset(torch.utils.data.Dataset):\n",
    "  def __init__(self, root):\n",
    "    self.transform = transforms.Compose([\n",
    "      transforms.Resize((224,224)),\n",
    "      transforms.CenterCrop(224),\n",
    "      transforms.ToTensor(),\n",
    "      transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "    ])\n",
    "    images_path = Path(root)\n",
    "\n",
    "    images_list = list(image_path.glob('*.jpg'))\n",
    "    images_list_str = [str(x) for x in images_list]\n",
    "    self.images = images_list_str\n",
    "\n",
    "\n",
    "  def __getitem__(self, item):\n",
    "    pass\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<module 'ntpath' from 'c:\\\\Users\\\\zhouzihan\\\\.conda\\\\envs\\\\project\\\\lib\\\\ntpath.py'>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "os.path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: './numpy_datasets/train/train_users.npy'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m文稿\\DurhamWorks\\Y3\\Codes\\y3project.nosync\\Models\\final.ipynb Cell 4'\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> <a href='vscode-notebook-cell://whiteghost/%E6%96%87%E7%A8%BF/DurhamWorks/Y3/Codes/y3project.nosync/Models/final.ipynb#ch0000003?line=0'>1</a>\u001b[0m train_users \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39;49mload(\u001b[39mf\u001b[39;49m\u001b[39m'\u001b[39;49m\u001b[39m{\u001b[39;49;00mtrain_data_path\u001b[39m}\u001b[39;49;00m\u001b[39m/train_users.npy\u001b[39;49m\u001b[39m'\u001b[39;49m)\n\u001b[0;32m      <a href='vscode-notebook-cell://whiteghost/%E6%96%87%E7%A8%BF/DurhamWorks/Y3/Codes/y3project.nosync/Models/final.ipynb#ch0000003?line=1'>2</a>\u001b[0m train_items \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39mload(\u001b[39mf\u001b[39m\u001b[39m'\u001b[39m\u001b[39m{\u001b[39;00mtrain_data_path\u001b[39m}\u001b[39;00m\u001b[39m/train_items.npy\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[0;32m      <a href='vscode-notebook-cell://whiteghost/%E6%96%87%E7%A8%BF/DurhamWorks/Y3/Codes/y3project.nosync/Models/final.ipynb#ch0000003?line=2'>3</a>\u001b[0m train_ratings \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39mload(\u001b[39mf\u001b[39m\u001b[39m'\u001b[39m\u001b[39m{\u001b[39;00mtrain_data_path\u001b[39m}\u001b[39;00m\u001b[39m/train_ratings.npy\u001b[39m\u001b[39m'\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\zhouzihan\\.conda\\envs\\project\\lib\\site-packages\\numpy\\lib\\npyio.py:407\u001b[0m, in \u001b[0;36mload\u001b[1;34m(file, mmap_mode, allow_pickle, fix_imports, encoding)\u001b[0m\n\u001b[0;32m    <a href='file:///c%3A/Users/zhouzihan/.conda/envs/project/lib/site-packages/numpy/lib/npyio.py?line=404'>405</a>\u001b[0m     own_fid \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m\n\u001b[0;32m    <a href='file:///c%3A/Users/zhouzihan/.conda/envs/project/lib/site-packages/numpy/lib/npyio.py?line=405'>406</a>\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m--> <a href='file:///c%3A/Users/zhouzihan/.conda/envs/project/lib/site-packages/numpy/lib/npyio.py?line=406'>407</a>\u001b[0m     fid \u001b[39m=\u001b[39m stack\u001b[39m.\u001b[39menter_context(\u001b[39mopen\u001b[39;49m(os_fspath(file), \u001b[39m\"\u001b[39;49m\u001b[39mrb\u001b[39;49m\u001b[39m\"\u001b[39;49m))\n\u001b[0;32m    <a href='file:///c%3A/Users/zhouzihan/.conda/envs/project/lib/site-packages/numpy/lib/npyio.py?line=407'>408</a>\u001b[0m     own_fid \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m\n\u001b[0;32m    <a href='file:///c%3A/Users/zhouzihan/.conda/envs/project/lib/site-packages/numpy/lib/npyio.py?line=409'>410</a>\u001b[0m \u001b[39m# Code to distinguish from NumPy binary files and pickles.\u001b[39;00m\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: './numpy_datasets/train/train_users.npy'"
     ]
    }
   ],
   "source": [
    "train_users = np.load(f'{train_data_path}/train_users.npy')\n",
    "train_items = np.load(f'{train_data_path}/train_items.npy')\n",
    "train_ratings = np.load(f'{train_data_path}/train_ratings.npy')\n",
    "train_reviews = np.load(\n",
    "    f'{train_data_path}/train_reviews.npy', allow_pickle=True)\n",
    "train_descriptions = np.load(\n",
    "    f'{train_data_path}/train_descriptions.npy', allow_pickle=True)\n",
    "train_prices = np.load(f'{train_data_path}/train_prices.npy')\n",
    "train_categories = np.load(f'{train_data_path}/train_categories.npy')\n",
    "\n",
    "test_users = np.load(f'{test_data_path}/test_users.npy')\n",
    "test_items = np.load(f'{test_data_path}/test_items.npy')\n",
    "test_ratings = np.load(f'{test_data_path}/test_ratings.npy')\n",
    "test_reviews = np.load(f'{test_data_path}/test_reviews.npy', allow_pickle=True)\n",
    "test_descriptions = np.load(\n",
    "    f'{test_data_path}/test_descriptions.npy', allow_pickle=True)\n",
    "test_prices = np.load(f'{test_data_path}/test_prices.npy')\n",
    "test_categories = np.load(f'{test_data_path}/test_categories.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cycle(iterable):\n",
    "  while True:\n",
    "    for x in iterable:\n",
    "      yield x\n",
    "\n",
    "\n",
    "train_iterator = cycle(DataLoader(\n",
    "    np.unique(train_users), batch_size=batch_size, shuffle=True))\n",
    "\n",
    "test_iterator = cycle(DataLoader(\n",
    "    np.unique(test_users), batch_size=batch_size, shuffle=True\n",
    "))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_loader = cycle(DataLoader(torchvision.datasets.ImageFolder(\n",
    "  '../Images',\n",
    "  transform=torchvision.transforms.Compose([\n",
    "    torchvision.transforms.Resize((224,224)),\n",
    "    torchvision.transforms.ToTensor(),\n",
    "    torchvision.transforms.Normalize(mean=[.5, .5, .5], std=[.5, .5, .5])\n",
    "  ])\n",
    "))\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "  input: user u, sets\n",
    "  item: i\n",
    "  output: user_reviews <- [reviews of u],  \n",
    "          user[descriptions that u has seen], \n",
    "          most liked category of u, \n",
    "          average price of u purchasing items, \n",
    "          item_reviews <- [reviews of i],\n",
    "          item_description <- description of i,\n",
    "          item_price <- price of i,\n",
    "          item_image <- image of i,\n",
    "          unseen_image <- the image of an unseen item to u,\n",
    "          rev_ui <- rating of u on i\n",
    "\n",
    "  status: this function works fine\n",
    "'''\n",
    "\n",
    "\n",
    "def sample_user(user, user_set, item_set, rev_set, desc_set, cat_set, price_set, img_set, rating_set):\n",
    "  # -> u\n",
    "  user = int(user)\n",
    "  user_meta = []\n",
    "  item_meta = []\n",
    "  user_reviews = []\n",
    "  user_descriptions =[]\n",
    "  user_category = []\n",
    "  user_price = []\n",
    "  item_reviews = []\n",
    "  item_description = None\n",
    "  item_category = None\n",
    "  item_price = None\n",
    "  item_image = None\n",
    "  unseen_image = None\n",
    "  rev_ui = None\n",
    "\n",
    "  user_indicies = np.where(user_set == user)[0]\n",
    "\n",
    "  # generates the index of a rated item for the user\n",
    "  chosen_user_item_idx = user_indicies[random.randint(0, len(user_indicies)-1)]\n",
    "\n",
    "  # randomly select a rated item -> i\n",
    "  item = item_set[chosen_user_item_idx]\n",
    "  item_description = desc_set[chosen_user_item_idx]\n",
    "  item_category = cat_set[chosen_user_item_idx]\n",
    "  item_price = price_set[chosen_user_item_idx]\n",
    "\n",
    "  # gets the rating of u, i\n",
    "  rating_ui = rating_set[chosen_user_item_idx]\n",
    "\n",
    "  # gets item indices to get all reviews of the item -> i\n",
    "  item_indicies = np.where(item_set == item)[0]\n",
    "\n",
    "  # all indicies in arrays are same (identical to user review number)\n",
    "  for u_idx in user_indicies:\n",
    "    # out of reviews, other metadata is always accessible either or not in test mode\n",
    "    user_descriptions.append(desc_set[u_idx])\n",
    "    user_category.append(cat_set[u_idx])\n",
    "    user_price.append(price_set[u_idx])\n",
    "    if u_idx != chosen_user_item_idx:\n",
    "      user_reviews.append(rev_set[u_idx])\n",
    "    else:\n",
    "      rev_ui = rev_set[u_idx]\n",
    "\n",
    "  for i_idx in item_indicies:\n",
    "    if i_idx != chosen_user_item_idx:\n",
    "      item_reviews.append(rev_set[i_idx])\n",
    "\n",
    "\n",
    "  user_reviews, user_descriptions, item_reviews = \\\n",
    "    np.array(\n",
    "      user_reviews).flatten(), np.array(user_descriptions).flatten(), np.array(item_reviews).flatten()\n",
    "\n",
    "  # according to the transnets paper - regularise the combined strings\n",
    "  if len(user_reviews) > 1000:\n",
    "    user_reviews = user_reviews[:1000]\n",
    "  if len(user_descriptions) > 1000:\n",
    "    user_descriptions = user_descriptions[:1000]\n",
    "  if len(item_reviews) > 1000:\n",
    "    item_reviews = item_reviews[:1000]\n",
    "\n",
    "  if len(user_reviews) == 0:\n",
    "    user_reviews = np.zeros((64,))\n",
    "  if len(item_reviews) == 0:\n",
    "    item_reviews = np.zeros((64,))\n",
    "  if len(user_descriptions) == 0:\n",
    "    user_descriptions = np.zeros((64,))\n",
    "\n",
    "  # for computational correctness\n",
    "  user_reviews, user_descriptions, item_reviews, item_description = torch.from_numpy(np.array(user_reviews)), torch.from_numpy(np.array(user_descriptions)),\\\n",
    "      torch.from_numpy(np.array(item_reviews)), torch.from_numpy(np.array(item_description))\n",
    "  \n",
    "  user_category = Counter(user_category).most_common(1)[0][0]\n",
    "  user_price = sum(user_price)/len(user_price)\n",
    "  \n",
    "  rev_ui = torch.from_numpy(np.array(rev_ui))\n",
    "\n",
    "  user_meta = [torch.tensor(user_category), torch.tensor([user_price],dtype=torch.float32), user_descriptions, user_reviews]\n",
    "  item_meta = [torch.tensor(item), torch.tensor(item_category), torch.tensor([item_price], dtype=torch.float32), item_description, item_reviews]\n",
    "\n",
    "  return user_meta, item_meta, item_image, unseen_image, rev_ui, rating_ui\n",
    "\n",
    "def l1_loss(pred, y):\n",
    "  return torch.mean(y - pred)\n",
    "\n",
    "\n",
    "def l2_loss(pred, y):\n",
    "  return torch.mean(torch.pow(pred,2) - torch.pow(y, 2))\n",
    "\n",
    "\n",
    "def weights_init(m):\n",
    "  if type(m) in (nn.Linear, nn.Conv1d, nn.Conv2d, nn.Embedding):\n",
    "    nn.init.xavier_normal_(m.weight)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'train_users' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m文稿\\DurhamWorks\\Y3\\Codes\\y3project.nosync\\Models\\final.ipynb Cell 9'\u001b[0m in \u001b[0;36m<cell line: 173>\u001b[1;34m()\u001b[0m\n\u001b[0;32m    <a href='vscode-notebook-cell://whiteghost/%E6%96%87%E7%A8%BF/DurhamWorks/Y3/Codes/y3project.nosync/Models/final.ipynb#ch0000007?line=168'>169</a>\u001b[0m \u001b[39m'''\u001b[39;00m\n\u001b[0;32m    <a href='vscode-notebook-cell://whiteghost/%E6%96%87%E7%A8%BF/DurhamWorks/Y3/Codes/y3project.nosync/Models/final.ipynb#ch0000007?line=169'>170</a>\u001b[0m \u001b[39mMeta data\u001b[39;00m\n\u001b[0;32m    <a href='vscode-notebook-cell://whiteghost/%E6%96%87%E7%A8%BF/DurhamWorks/Y3/Codes/y3project.nosync/Models/final.ipynb#ch0000007?line=170'>171</a>\u001b[0m \u001b[39m'''\u001b[39;00m\n\u001b[0;32m    <a href='vscode-notebook-cell://whiteghost/%E6%96%87%E7%A8%BF/DurhamWorks/Y3/Codes/y3project.nosync/Models/final.ipynb#ch0000007?line=171'>172</a>\u001b[0m \u001b[39m# user latent vectors -> this embedding should be updated\u001b[39;00m\n\u001b[1;32m--> <a href='vscode-notebook-cell://whiteghost/%E6%96%87%E7%A8%BF/DurhamWorks/Y3/Codes/y3project.nosync/Models/final.ipynb#ch0000007?line=172'>173</a>\u001b[0m mf_u \u001b[39m=\u001b[39m GMF(\u001b[39mmax\u001b[39m(train_users)\u001b[39m+\u001b[39m\u001b[39m1\u001b[39m, \u001b[39m10\u001b[39m, \u001b[39mTrue\u001b[39;00m)\u001b[39m.\u001b[39mapply(weights_init)\u001b[39m.\u001b[39mto(device)\n\u001b[0;32m    <a href='vscode-notebook-cell://whiteghost/%E6%96%87%E7%A8%BF/DurhamWorks/Y3/Codes/y3project.nosync/Models/final.ipynb#ch0000007?line=173'>174</a>\u001b[0m \u001b[39m# item latent vectors -> this embedding should be updated\u001b[39;00m\n\u001b[0;32m    <a href='vscode-notebook-cell://whiteghost/%E6%96%87%E7%A8%BF/DurhamWorks/Y3/Codes/y3project.nosync/Models/final.ipynb#ch0000007?line=174'>175</a>\u001b[0m mf_i \u001b[39m=\u001b[39m GMF(\u001b[39mmax\u001b[39m(\u001b[39mmax\u001b[39m(train_items), \u001b[39mmax\u001b[39m(test_items))\u001b[39m+\u001b[39m\u001b[39m1\u001b[39m, \u001b[39m10\u001b[39m, \u001b[39mTrue\u001b[39;00m)\u001b[39m.\u001b[39mapply(weights_init)\u001b[39m.\u001b[39mto(device)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'train_users' is not defined"
     ]
    }
   ],
   "source": [
    "class FM(nn.Module):\n",
    "    def __init__(self, latent_dim, fea_num):\n",
    "        super().__init__()\n",
    "\n",
    "        self.latent_dim = latent_dim\n",
    "        self.w0 = nn.Parameter(torch.zeros([1, ]))\n",
    "        self.w1 = nn.Parameter(torch.rand([fea_num, 1]))\n",
    "        self.w2 = nn.Parameter(torch.rand([fea_num, latent_dim]))\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        # inputs = inputs.long()\n",
    "        first_order = self.w0 + torch.mm(inputs, self.w1)\n",
    "        second_order = 1/2 * torch.sum(\n",
    "            torch.pow(torch.mm(inputs, self.w2), 2) -\n",
    "            torch.mm(torch.pow(inputs, 2), torch.pow(self.w2, 2)),\n",
    "\n",
    "            dim=1,\n",
    "            keepdim=True\n",
    "        )\n",
    "\n",
    "        return first_order + second_order\n",
    "\n",
    "\n",
    "class GMF(nn.Module):\n",
    "    def __init__(self, inp_range, latent_dim=20, dropout = True):\n",
    "        super().__init__()\n",
    "        self.emb = nn.Embedding(\n",
    "            num_embeddings=inp_range, embedding_dim=latent_dim)\n",
    "        self.dropout = nn.Dropout(0.5)\n",
    "        self.use_dropout = dropout\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        embedding = self.emb(inputs)\n",
    "        if self.use_dropout:\n",
    "          embedding = self.dropout(embedding)\n",
    "\n",
    "        return embedding\n",
    "\n",
    "class TextCNN(nn.Module):\n",
    "  def __init__(self, kernel_size, neurons, latent_vector_size, vocab_size, embed_size):\n",
    "      super().__init__()\n",
    "      self.embedding = nn.Embedding(vocab_size, embed_size)\n",
    "      self.dropout = nn.Dropout(0.5)\n",
    "      self.decoder = nn.Linear(300, latent_vector_size)\n",
    "      self.pooling = nn.AdaptiveMaxPool1d(1)\n",
    "      self.convs = nn.ModuleList()\n",
    "      self.tanh = nn.Tanh()\n",
    "      self.embed_size = embed_size\n",
    "      for _ in range(3):\n",
    "        self.convs.append(nn.Conv1d(embed_size, neurons, kernel_size))\n",
    "\n",
    "  def forward(self, inputs):\n",
    "    embeddings = self.embedding(inputs.long())\n",
    "\n",
    "    print(embeddings.shape)\n",
    "    embeddings = embeddings.view(1, len(embeddings), self.embed_size).permute(0, 2, 1)\n",
    "\n",
    "    encoding = torch.cat([\n",
    "        torch.squeeze(self.pooling(conv(embeddings)), dim=-1)\n",
    "        for conv in self.convs\n",
    "    ], dim=1)\n",
    "\n",
    "    out = self.decoder(self.dropout(encoding))\n",
    "    return out\n",
    "\n",
    "\n",
    "class TransformMLP(nn.Module):\n",
    "  def __init__(self, concated_size, latent_vector_size):\n",
    "      super().__init__()\n",
    "      self.net = nn.Sequential(\n",
    "          nn.Linear(concated_size, 2*concated_size),\n",
    "          nn.ReLU(inplace=True),\n",
    "          nn.Linear(2*concated_size, latent_vector_size),\n",
    "          nn.Dropout(0.5, inplace=True)\n",
    "      )\n",
    "\n",
    "  def forward(self, x):\n",
    "    out = self.net(x)\n",
    "    out = out.view(1, out.shape[0])\n",
    "    return out\n",
    "\n",
    "\n",
    "# https://d2l.ai/chapter_convolutional-modern/resnet.html\n",
    "class Residual(nn.Module):\n",
    "  def __init__(self, input_channels, num_channels, use_1x1_conv=False, downsample=1):\n",
    "    super().__init__()\n",
    "    self.net = nn.Sequential(\n",
    "        nn.Conv2d(input_channels, num_channels, kernel_size=3, padding=1,stride=downsample),\n",
    "        nn.BatchNorm2d(num_channels),\n",
    "        nn.ReLU(inplace=True),\n",
    "        nn.Conv2d(num_channels, num_channels, kernel_size=3, padding=1),\n",
    "        nn.BatchNorm2d(num_channels)\n",
    "    )\n",
    "    # reduce image size by 2\n",
    "    if use_1x1_conv:\n",
    "      self.net2 = nn.Conv2d(input_channels, num_channels,\n",
    "                            kernel_size=1, stride=downsample)\n",
    "    else:\n",
    "      self.net2 = None\n",
    "\n",
    "  def forward(self, x):\n",
    "    res = self.net(x)\n",
    "    if self.net2 != None:\n",
    "      x = self.net2(x)\n",
    "    res += x\n",
    "    out = F.relu(res)\n",
    "    return out\n",
    "\n",
    "def resnet_block(input_channels, num_channels, num_residuals, first_block=False):\n",
    "  blk = []\n",
    "  for i in range(num_residuals):\n",
    "    if i == 0 and not first_block:\n",
    "      blk.append(Residual(input_channels, num_channels,\n",
    "                 use_1x1_conv=True, downsample=2))\n",
    "    else:\n",
    "      blk.append(Residual(num_channels, num_channels))\n",
    "  return blk\n",
    "\n",
    "\n",
    "# resnet siamesecnn\n",
    "\n",
    "'''\n",
    "Here, instead of seeking a final layer that can be adapted to general-purpose prediction tasks, \n",
    "we hope to learn a representation whose dimensions explain the variance in users\\' fashion preferences\n",
    "'''\n",
    "class SiameseCNN(nn.Module):\n",
    "  def __init__(self):\n",
    "    super().__init__()\n",
    "    self.net = nn.Sequential(\n",
    "        # b1\n",
    "        nn.Sequential(\n",
    "            nn.Conv2d(3, 64, kernel_size=7, stride=2, padding=3),  # 112\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=3, stride=2, padding=1),  # 56\n",
    "        ),\n",
    "\n",
    "        # b2 - first block\n",
    "        nn.Sequential(*resnet_block(64, 64, 2, first_block=True)), # x. + y.  56 / 4 + 56 / 4  = 28\n",
    "\n",
    "        # b3\n",
    "        nn.Sequential(*resnet_block(64, 128, 2)), # out: 7 + 7 = 14\n",
    "\n",
    "        # b4\n",
    "        nn.Sequential(*resnet_block(128, 256, 2)), # out: 3.5 + 3.5 = 7\n",
    "\n",
    "        # b5\n",
    "        nn.Sequential(*resnet_block(256, 512, 2)), # out: 3.5 ?\n",
    "\n",
    "        nn.AdaptiveAvgPool2d((1, 1)),\n",
    "        nn.Flatten(),\n",
    "        nn.Linear(512, 512),\n",
    "        nn.Dropout(inplace=True),\n",
    "        nn.Linear(512, 100)\n",
    "    )\n",
    "\n",
    "  def forward(self, x1, x2):\n",
    "    return self.net(x1), self.net(x2)\n",
    "\n",
    "textCNN_I = TextCNN(3, 100, 50, 50000, 64).apply(weights_init).to(device)\n",
    "textCNN_U = TextCNN(3, 100, 50, 50000, 64).apply(weights_init).to(device)\n",
    "textCNN_T = TextCNN(3, 100, 50, 50000, 64).apply(weights_init).to(device)\n",
    "\n",
    "\n",
    "transform = TransformMLP(100, 50).apply(weights_init).to(device)\n",
    "fm_T = FM(8, 50).apply(weights_init).to(device)\n",
    "fm_S = FM(8, 220).apply(weights_init).to(device)\n",
    "\n",
    "'''\n",
    "Meta data\n",
    "'''\n",
    "# user latent vectors -> this embedding should be updated\n",
    "mf_u = GMF(max(train_users)+1, 10, True).apply(weights_init).to(device)\n",
    "# item latent vectors -> this embedding should be updated\n",
    "mf_i = GMF(max(max(train_items), max(test_items))+1, 10, True).apply(weights_init).to(device)\n",
    "\n",
    "# user description latent vectors -> this nn should be updated\n",
    "textCNN_DU = TextCNN(3, 100, 64, 50000, 36).apply(weights_init).to(device)\n",
    "# item description latent vectors -> this nn should be updated\n",
    "textCNN_DI = TextCNN(3, 100, 36, 50000, 36).apply(weights_init).to(device)\n",
    "# image latent vectors -> this nn should be updated\n",
    "imageCNN = SiameseCNN().apply(weights_init).to(device)\n",
    "\n",
    "optimiser_I = torch.optim.Adam(params=textCNN_I.parameters(), lr=lr)\n",
    "optimiser_U = torch.optim.Adam(params=textCNN_U.parameters(), lr=lr)\n",
    "optimiser_T = torch.optim.Adam(params=textCNN_T.parameters(), lr=lr)\n",
    "optimiser_DU = torch.optim.Adam(params=textCNN_DU.parameters(), lr=lr)\n",
    "optimiser_DI = torch.optim.Adam(params=textCNN_DI.parameters(), lr=lr)\n",
    "optimiser_trans = torch.optim.Adam(params=transform.parameters(), lr=lr)\n",
    "optimiser_FMT = torch.optim.Adam(params=fm_T.parameters(), lr=lr)\n",
    "optimiser_FMS = torch.optim.Adam(params=fm_S.parameters(), lr=lr)\n",
    "optimiser_image = torch.optim.Adam(params=imageCNN.parameters(), lr = lr)\n",
    "optimiser_MFU = torch.optim.Adam(params=mf_u.parameters(), lr = lr)\n",
    "optimiser_MFI = torch.optim.Adam(params=mf_i.parameters(), lr = lr)\n",
    "\n",
    "\n",
    "def save_training():\n",
    "  torch.save(\n",
    "      {'textCNNI': textCNN_I.state_dict(),\n",
    "       'textCNNU': textCNN_U.state_dict(),\n",
    "       'textCNNT': textCNN_T.state_dict(),\n",
    "       'transform': transform.state_dict(),\n",
    "       'fmT': fm_T.state_dict(),\n",
    "       'fmS': fm_S.state_dict(),\n",
    "       'mf_u': mf_u.state_dict(),\n",
    "       'mf_i':mf_i.state_dict(),\n",
    "       'textCNN_DU':textCNN_DU.state_dict(),\n",
    "       'textCNN_DI':textCNN_DI.state_dict(),\n",
    "       'imageCNN':imageCNN.state_dict(),\n",
    "       'epoch': epoch},\n",
    "      f'{data_save_path}/save.chkpt')\n",
    "\n",
    "\n",
    "def load_training():\n",
    "  global textCNN_I\n",
    "  global textCNN_U\n",
    "  global textCNN_T\n",
    "  global transform\n",
    "  global fm_T\n",
    "  global fm_S\n",
    "  global mf_u\n",
    "  global mf_i\n",
    "  global textCNN_DU\n",
    "  global textCNN_DI\n",
    "  global imageCNN\n",
    "\n",
    "  params = torch.load(f'{data_save_path}/save.chkpt')\n",
    "\n",
    "  textCNN_I.load_state_dict(params['textCNNI'])\n",
    "  textCNN_U.load_state_dict(params['textCNNU'])\n",
    "  textCNN_T.load_state_dict(params['textCNNT'])\n",
    "  transform.load_state_dict(params['transform'])\n",
    "  fm_T.load_state_dict(params['fmT'])\n",
    "  fm_S.load_state_dict(params['fmS'])\n",
    "  mf_u.load_state_dict(params['fm_u'])\n",
    "  mf_i.load_state_dict(params['fm_i'])\n",
    "  textCNN_DU.load_state_dict(params['textCNN_DU'])\n",
    "  textCNN_DI.load_state_dict(params['textCNN_DI'])\n",
    "  imageCNN.load_state_dict(params['imageCNN'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_rmse(preds, real):\n",
    "    rmse = 0\n",
    "    # produce known rui set\n",
    "    for i in range(len(preds)):\n",
    "        rmse += (preds[i] - real[i]) ** 2\n",
    "\n",
    "    rmse = math.sqrt(rmse/len(preds))\n",
    "\n",
    "    return rmse\n",
    "\n",
    "\n",
    "def eval_model():\n",
    "  preds = []\n",
    "  real = []\n",
    "  for u in tqdm(np.unique(test_users)):\n",
    "    user_meta, item_meta, item_image, unseen_image, _, rating_ui = sample_user(\n",
    "        u, test_users, test_items, test_reviews, test_descriptions, test_categories, test_prices, None, test_ratings)\n",
    "    user_category, user_price, user_descriptions, user_reviews = user_meta\n",
    "    i, item_category, item_price, item_description, item_reviews = item_meta\n",
    "\n",
    "    # transform the input\n",
    "    latent_rev_user = textCNN_U(user_reviews)\n",
    "    latent_rev_item = textCNN_I(item_reviews)\n",
    "    z0 = torch.flatten(torch.cat((latent_rev_user, latent_rev_item), dim=0))\n",
    "    z_L = transform(z0)\n",
    "    \n",
    "    # predict using the transformed input\n",
    "    latent_desc_i = textCNN_DI(item_description)  # -> 36\n",
    "    latent_desc_u = textCNN_DU(user_descriptions)  # -> 64\n",
    "\n",
    "    '''User and Item'''\n",
    "    latent_uid = mf_u(torch.tensor(u))\n",
    "    latent_iid = mf_i(i)  # -> 20\n",
    "\n",
    "    '''Meta data'''\n",
    "    cat_embed_u, cat_embed_i = F.one_hot(\n",
    "        user_category, 24), F.one_hot(item_category, 24)  # -> 24\n",
    "\n",
    "    latent_final = torch.cat(\n",
    "        (z_L.flatten(),  # 50\n",
    "         latent_uid,  # 10\n",
    "         latent_iid,  # 10\n",
    "         latent_desc_i.flatten(),  # 36\n",
    "         latent_desc_u.flatten(),  # 64\n",
    "         cat_embed_u,  # 24\n",
    "         cat_embed_i,  # 24\n",
    "         user_price,  # 1\n",
    "         item_price),  # 1\n",
    "        dim=0).view(1, 220)\n",
    "\n",
    "    '''Image - trained seperatedly'''\n",
    "    # img_result_1, img_result_2 = imageCNN(item_image, unseen_image)\n",
    "    # s = imageCNN.state_dict()\n",
    "    # # regularisation term\n",
    "    # nn_regularisers = 0\n",
    "    # for k in s:\n",
    "    #   params = s[k].flatten()\n",
    "    #   nn_regularisers += torch.sum(params ** 2)/2\n",
    "    # cost_train = torch.sum(torch.log(F.sigmoid(torch.sum(torch.dot(torch.subtract(img_result_1, img_result_2),latent_final)).flatten())))\n",
    "    # cost_train -= 0.001 * nn_regularisers\n",
    "    # loss_image += cost_train\n",
    "\n",
    "    pred = fm_S(latent_final)\n",
    "    preds.append(pred)\n",
    "    real.append(rating_ui)\n",
    "\n",
    "  return eval_rmse(preds, real)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/1000 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([64, 64])\n",
      "torch.Size([320, 64])\n",
      "torch.Size([384, 64])\n",
      "torch.Size([36, 36])\n",
      "torch.Size([216, 36])\n",
      "torch.Size([64, 64])\n",
      "torch.Size([128, 64])\n",
      "torch.Size([64, 64])\n",
      "torch.Size([36, 36])\n",
      "torch.Size([108, 36])\n",
      "torch.Size([64, 64])\n",
      "torch.Size([128, 64])\n",
      "torch.Size([128, 64])\n",
      "torch.Size([36, 36])\n",
      "torch.Size([108, 36])\n",
      "torch.Size([64, 64])\n",
      "torch.Size([128, 64])\n",
      "torch.Size([832, 64])\n",
      "torch.Size([36, 36])\n",
      "torch.Size([108, 36])\n",
      "torch.Size([64, 64])\n",
      "torch.Size([192, 64])\n",
      "torch.Size([64, 64])\n",
      "torch.Size([36, 36])\n",
      "torch.Size([144, 36])\n",
      "torch.Size([64, 64])\n",
      "torch.Size([128, 64])\n",
      "torch.Size([192, 64])\n",
      "torch.Size([36, 36])\n",
      "torch.Size([108, 36])\n",
      "torch.Size([64, 64])\n",
      "torch.Size([128, 64])\n",
      "torch.Size([64, 64])\n",
      "torch.Size([36, 36])\n",
      "torch.Size([108, 36])\n",
      "torch.Size([64, 64])\n",
      "torch.Size([384, 64])\n",
      "torch.Size([832, 64])\n",
      "torch.Size([36, 36])\n",
      "torch.Size([252, 36])\n",
      "torch.Size([64, 64])\n",
      "torch.Size([192, 64])\n",
      "torch.Size([320, 64])\n",
      "torch.Size([36, 36])\n",
      "torch.Size([144, 36])\n",
      "torch.Size([64, 64])\n",
      "torch.Size([128, 64])\n",
      "torch.Size([320, 64])\n",
      "torch.Size([36, 36])\n",
      "torch.Size([108, 36])\n",
      "torch.Size([64, 64])\n",
      "torch.Size([192, 64])\n",
      "torch.Size([64, 64])\n",
      "torch.Size([36, 36])\n",
      "torch.Size([144, 36])\n",
      "torch.Size([64, 64])\n",
      "torch.Size([128, 64])\n",
      "torch.Size([384, 64])\n",
      "torch.Size([36, 36])\n",
      "torch.Size([108, 36])\n",
      "torch.Size([64, 64])\n",
      "torch.Size([128, 64])\n",
      "torch.Size([1000, 64])\n",
      "torch.Size([36, 36])\n",
      "torch.Size([108, 36])\n",
      "torch.Size([64, 64])\n",
      "torch.Size([448, 64])\n",
      "torch.Size([1000, 64])\n",
      "torch.Size([36, 36])\n",
      "torch.Size([288, 36])\n",
      "torch.Size([64, 64])\n",
      "torch.Size([640, 64])\n",
      "torch.Size([64, 64])\n",
      "torch.Size([36, 36])\n",
      "torch.Size([396, 36])\n",
      "torch.Size([64, 64])\n",
      "torch.Size([128, 64])\n",
      "torch.Size([320, 64])\n",
      "torch.Size([36, 36])\n",
      "torch.Size([108, 36])\n",
      "torch.Size([64, 64])\n",
      "torch.Size([128, 64])\n",
      "torch.Size([1000, 64])\n",
      "torch.Size([36, 36])\n",
      "torch.Size([108, 36])\n",
      "torch.Size([64, 64])\n",
      "torch.Size([128, 64])\n",
      "torch.Size([256, 64])\n",
      "torch.Size([36, 36])\n",
      "torch.Size([108, 36])\n",
      "torch.Size([64, 64])\n",
      "torch.Size([128, 64])\n",
      "torch.Size([64, 64])\n",
      "torch.Size([36, 36])\n",
      "torch.Size([108, 36])\n",
      "torch.Size([64, 64])\n",
      "torch.Size([128, 64])\n",
      "torch.Size([1000, 64])\n",
      "torch.Size([36, 36])\n",
      "torch.Size([108, 36])\n",
      "torch.Size([64, 64])\n",
      "torch.Size([256, 64])\n",
      "torch.Size([128, 64])\n",
      "torch.Size([36, 36])\n",
      "torch.Size([180, 36])\n",
      "torch.Size([64, 64])\n",
      "torch.Size([256, 64])\n",
      "torch.Size([640, 64])\n",
      "torch.Size([36, 36])\n",
      "torch.Size([180, 36])\n",
      "torch.Size([64, 64])\n",
      "torch.Size([128, 64])\n",
      "torch.Size([896, 64])\n",
      "torch.Size([36, 36])\n",
      "torch.Size([108, 36])\n",
      "torch.Size([64, 64])\n",
      "torch.Size([128, 64])\n",
      "torch.Size([128, 64])\n",
      "torch.Size([36, 36])\n",
      "torch.Size([108, 36])\n",
      "torch.Size([64, 64])\n",
      "torch.Size([128, 64])\n",
      "torch.Size([448, 64])\n",
      "torch.Size([36, 36])\n",
      "torch.Size([108, 36])\n",
      "torch.Size([64, 64])\n",
      "torch.Size([192, 64])\n",
      "torch.Size([64, 64])\n",
      "torch.Size([36, 36])\n",
      "torch.Size([144, 36])\n",
      "torch.Size([64, 64])\n",
      "torch.Size([448, 64])\n",
      "torch.Size([1000, 64])\n",
      "torch.Size([36, 36])\n",
      "torch.Size([288, 36])\n",
      "torch.Size([64, 64])\n",
      "torch.Size([448, 64])\n",
      "torch.Size([64, 64])\n",
      "torch.Size([36, 36])\n",
      "torch.Size([288, 36])\n",
      "torch.Size([64, 64])\n",
      "torch.Size([128, 64])\n",
      "torch.Size([384, 64])\n",
      "torch.Size([36, 36])\n",
      "torch.Size([108, 36])\n",
      "torch.Size([64, 64])\n",
      "torch.Size([192, 64])\n",
      "torch.Size([192, 64])\n",
      "torch.Size([36, 36])\n",
      "torch.Size([144, 36])\n",
      "torch.Size([64, 64])\n",
      "torch.Size([128, 64])\n",
      "torch.Size([256, 64])\n",
      "torch.Size([36, 36])\n",
      "torch.Size([108, 36])\n",
      "torch.Size([64, 64])\n",
      "torch.Size([192, 64])\n",
      "torch.Size([192, 64])\n",
      "torch.Size([36, 36])\n",
      "torch.Size([144, 36])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 1/1000 [00:00<14:52,  1.12it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([64, 64])\n",
      "torch.Size([256, 64])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/Users/zhouzihan/Documents/DurhamWorks/Y3/Codes/CS_Y3_Project.nosync/Models/siamese.ipynb Cell 10'\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/zhouzihan/Documents/DurhamWorks/Y3/Codes/CS_Y3_Project.nosync/Models/siamese.ipynb#ch0000012?line=69'>70</a>\u001b[0m \u001b[39m'''THE SECTIONS BELOW ARE USED FOR PREDICTION'''\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/zhouzihan/Documents/DurhamWorks/Y3/Codes/CS_Y3_Project.nosync/Models/siamese.ipynb#ch0000012?line=71'>72</a>\u001b[0m \u001b[39m'''Learn to transform'''\u001b[39;00m\n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/zhouzihan/Documents/DurhamWorks/Y3/Codes/CS_Y3_Project.nosync/Models/siamese.ipynb#ch0000012?line=72'>73</a>\u001b[0m latent_rev_user \u001b[39m=\u001b[39m textCNN_U(user_reviews)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/zhouzihan/Documents/DurhamWorks/Y3/Codes/CS_Y3_Project.nosync/Models/siamese.ipynb#ch0000012?line=73'>74</a>\u001b[0m latent_rev_item \u001b[39m=\u001b[39m textCNN_I(item_reviews)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/zhouzihan/Documents/DurhamWorks/Y3/Codes/CS_Y3_Project.nosync/Models/siamese.ipynb#ch0000012?line=74'>75</a>\u001b[0m z0 \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mflatten(torch\u001b[39m.\u001b[39mcat((latent_rev_user, latent_rev_item), dim\u001b[39m=\u001b[39m\u001b[39m0\u001b[39m))\n",
      "File \u001b[0;32m~/miniconda3/envs/y3project/lib/python3.8/site-packages/torch/nn/modules/module.py:1102\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   <a href='file:///~/miniconda3/envs/y3project/lib/python3.8/site-packages/torch/nn/modules/module.py?line=1097'>1098</a>\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   <a href='file:///~/miniconda3/envs/y3project/lib/python3.8/site-packages/torch/nn/modules/module.py?line=1098'>1099</a>\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   <a href='file:///~/miniconda3/envs/y3project/lib/python3.8/site-packages/torch/nn/modules/module.py?line=1099'>1100</a>\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   <a href='file:///~/miniconda3/envs/y3project/lib/python3.8/site-packages/torch/nn/modules/module.py?line=1100'>1101</a>\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> <a href='file:///~/miniconda3/envs/y3project/lib/python3.8/site-packages/torch/nn/modules/module.py?line=1101'>1102</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   <a href='file:///~/miniconda3/envs/y3project/lib/python3.8/site-packages/torch/nn/modules/module.py?line=1102'>1103</a>\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   <a href='file:///~/miniconda3/envs/y3project/lib/python3.8/site-packages/torch/nn/modules/module.py?line=1103'>1104</a>\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "\u001b[1;32m/Users/zhouzihan/Documents/DurhamWorks/Y3/Codes/CS_Y3_Project.nosync/Models/siamese.ipynb Cell 8'\u001b[0m in \u001b[0;36mTextCNN.forward\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/zhouzihan/Documents/DurhamWorks/Y3/Codes/CS_Y3_Project.nosync/Models/siamese.ipynb#ch0000007?line=54'>55</a>\u001b[0m \u001b[39mprint\u001b[39m(embeddings\u001b[39m.\u001b[39mshape)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/zhouzihan/Documents/DurhamWorks/Y3/Codes/CS_Y3_Project.nosync/Models/siamese.ipynb#ch0000007?line=55'>56</a>\u001b[0m embeddings \u001b[39m=\u001b[39m embeddings\u001b[39m.\u001b[39mview(\u001b[39m1\u001b[39m, \u001b[39mlen\u001b[39m(embeddings), \u001b[39mself\u001b[39m\u001b[39m.\u001b[39membed_size)\u001b[39m.\u001b[39mpermute(\u001b[39m0\u001b[39m, \u001b[39m2\u001b[39m, \u001b[39m1\u001b[39m)\n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/zhouzihan/Documents/DurhamWorks/Y3/Codes/CS_Y3_Project.nosync/Models/siamese.ipynb#ch0000007?line=57'>58</a>\u001b[0m encoding \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mcat([\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/zhouzihan/Documents/DurhamWorks/Y3/Codes/CS_Y3_Project.nosync/Models/siamese.ipynb#ch0000007?line=58'>59</a>\u001b[0m     torch\u001b[39m.\u001b[39msqueeze(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpooling(conv(embeddings)), dim\u001b[39m=\u001b[39m\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/zhouzihan/Documents/DurhamWorks/Y3/Codes/CS_Y3_Project.nosync/Models/siamese.ipynb#ch0000007?line=59'>60</a>\u001b[0m     \u001b[39mfor\u001b[39;00m conv \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mconvs\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/zhouzihan/Documents/DurhamWorks/Y3/Codes/CS_Y3_Project.nosync/Models/siamese.ipynb#ch0000007?line=60'>61</a>\u001b[0m ], dim\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/zhouzihan/Documents/DurhamWorks/Y3/Codes/CS_Y3_Project.nosync/Models/siamese.ipynb#ch0000007?line=62'>63</a>\u001b[0m out \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdecoder(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdropout(encoding))\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/zhouzihan/Documents/DurhamWorks/Y3/Codes/CS_Y3_Project.nosync/Models/siamese.ipynb#ch0000007?line=63'>64</a>\u001b[0m \u001b[39mreturn\u001b[39;00m out\n",
      "\u001b[1;32m/Users/zhouzihan/Documents/DurhamWorks/Y3/Codes/CS_Y3_Project.nosync/Models/siamese.ipynb Cell 8'\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/zhouzihan/Documents/DurhamWorks/Y3/Codes/CS_Y3_Project.nosync/Models/siamese.ipynb#ch0000007?line=54'>55</a>\u001b[0m \u001b[39mprint\u001b[39m(embeddings\u001b[39m.\u001b[39mshape)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/zhouzihan/Documents/DurhamWorks/Y3/Codes/CS_Y3_Project.nosync/Models/siamese.ipynb#ch0000007?line=55'>56</a>\u001b[0m embeddings \u001b[39m=\u001b[39m embeddings\u001b[39m.\u001b[39mview(\u001b[39m1\u001b[39m, \u001b[39mlen\u001b[39m(embeddings), \u001b[39mself\u001b[39m\u001b[39m.\u001b[39membed_size)\u001b[39m.\u001b[39mpermute(\u001b[39m0\u001b[39m, \u001b[39m2\u001b[39m, \u001b[39m1\u001b[39m)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/zhouzihan/Documents/DurhamWorks/Y3/Codes/CS_Y3_Project.nosync/Models/siamese.ipynb#ch0000007?line=57'>58</a>\u001b[0m encoding \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mcat([\n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/zhouzihan/Documents/DurhamWorks/Y3/Codes/CS_Y3_Project.nosync/Models/siamese.ipynb#ch0000007?line=58'>59</a>\u001b[0m     torch\u001b[39m.\u001b[39msqueeze(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mpooling(conv(embeddings)), dim\u001b[39m=\u001b[39m\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/zhouzihan/Documents/DurhamWorks/Y3/Codes/CS_Y3_Project.nosync/Models/siamese.ipynb#ch0000007?line=59'>60</a>\u001b[0m     \u001b[39mfor\u001b[39;00m conv \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mconvs\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/zhouzihan/Documents/DurhamWorks/Y3/Codes/CS_Y3_Project.nosync/Models/siamese.ipynb#ch0000007?line=60'>61</a>\u001b[0m ], dim\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/zhouzihan/Documents/DurhamWorks/Y3/Codes/CS_Y3_Project.nosync/Models/siamese.ipynb#ch0000007?line=62'>63</a>\u001b[0m out \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdecoder(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdropout(encoding))\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/zhouzihan/Documents/DurhamWorks/Y3/Codes/CS_Y3_Project.nosync/Models/siamese.ipynb#ch0000007?line=63'>64</a>\u001b[0m \u001b[39mreturn\u001b[39;00m out\n",
      "File \u001b[0;32m~/miniconda3/envs/y3project/lib/python3.8/site-packages/torch/nn/modules/module.py:1102\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   <a href='file:///~/miniconda3/envs/y3project/lib/python3.8/site-packages/torch/nn/modules/module.py?line=1097'>1098</a>\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   <a href='file:///~/miniconda3/envs/y3project/lib/python3.8/site-packages/torch/nn/modules/module.py?line=1098'>1099</a>\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   <a href='file:///~/miniconda3/envs/y3project/lib/python3.8/site-packages/torch/nn/modules/module.py?line=1099'>1100</a>\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   <a href='file:///~/miniconda3/envs/y3project/lib/python3.8/site-packages/torch/nn/modules/module.py?line=1100'>1101</a>\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> <a href='file:///~/miniconda3/envs/y3project/lib/python3.8/site-packages/torch/nn/modules/module.py?line=1101'>1102</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   <a href='file:///~/miniconda3/envs/y3project/lib/python3.8/site-packages/torch/nn/modules/module.py?line=1102'>1103</a>\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   <a href='file:///~/miniconda3/envs/y3project/lib/python3.8/site-packages/torch/nn/modules/module.py?line=1103'>1104</a>\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/miniconda3/envs/y3project/lib/python3.8/site-packages/torch/nn/modules/pooling.py:1004\u001b[0m, in \u001b[0;36mAdaptiveMaxPool1d.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m   <a href='file:///~/miniconda3/envs/y3project/lib/python3.8/site-packages/torch/nn/modules/pooling.py?line=1002'>1003</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m: Tensor) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tensor:\n\u001b[0;32m-> <a href='file:///~/miniconda3/envs/y3project/lib/python3.8/site-packages/torch/nn/modules/pooling.py?line=1003'>1004</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m F\u001b[39m.\u001b[39;49madaptive_max_pool1d(\u001b[39minput\u001b[39;49m, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49moutput_size, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mreturn_indices)\n",
      "File \u001b[0;32m~/miniconda3/envs/y3project/lib/python3.8/site-packages/torch/_jit_internal.py:422\u001b[0m, in \u001b[0;36mboolean_dispatch.<locals>.fn\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    <a href='file:///~/miniconda3/envs/y3project/lib/python3.8/site-packages/torch/_jit_internal.py?line=419'>420</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m if_true(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[1;32m    <a href='file:///~/miniconda3/envs/y3project/lib/python3.8/site-packages/torch/_jit_internal.py?line=420'>421</a>\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m--> <a href='file:///~/miniconda3/envs/y3project/lib/python3.8/site-packages/torch/_jit_internal.py?line=421'>422</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m if_false(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/miniconda3/envs/y3project/lib/python3.8/site-packages/torch/nn/functional.py:1005\u001b[0m, in \u001b[0;36m_adaptive_max_pool1d\u001b[0;34m(input, output_size, return_indices)\u001b[0m\n\u001b[1;32m   <a href='file:///~/miniconda3/envs/y3project/lib/python3.8/site-packages/torch/nn/functional.py?line=1000'>1001</a>\u001b[0m \u001b[39mif\u001b[39;00m has_torch_function_unary(\u001b[39minput\u001b[39m):\n\u001b[1;32m   <a href='file:///~/miniconda3/envs/y3project/lib/python3.8/site-packages/torch/nn/functional.py?line=1001'>1002</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m   <a href='file:///~/miniconda3/envs/y3project/lib/python3.8/site-packages/torch/nn/functional.py?line=1002'>1003</a>\u001b[0m         adaptive_max_pool1d, (\u001b[39minput\u001b[39m,), \u001b[39minput\u001b[39m, output_size, return_indices\u001b[39m=\u001b[39mreturn_indices\n\u001b[1;32m   <a href='file:///~/miniconda3/envs/y3project/lib/python3.8/site-packages/torch/nn/functional.py?line=1003'>1004</a>\u001b[0m     )\n\u001b[0;32m-> <a href='file:///~/miniconda3/envs/y3project/lib/python3.8/site-packages/torch/nn/functional.py?line=1004'>1005</a>\u001b[0m \u001b[39mreturn\u001b[39;00m adaptive_max_pool1d_with_indices(\u001b[39minput\u001b[39;49m, output_size)[\u001b[39m0\u001b[39m]\n",
      "File \u001b[0;32m~/miniconda3/envs/y3project/lib/python3.8/site-packages/torch/nn/functional.py:997\u001b[0m, in \u001b[0;36madaptive_max_pool1d_with_indices\u001b[0;34m(input, output_size, return_indices)\u001b[0m\n\u001b[1;32m    <a href='file:///~/miniconda3/envs/y3project/lib/python3.8/site-packages/torch/nn/functional.py?line=992'>993</a>\u001b[0m \u001b[39mif\u001b[39;00m has_torch_function_unary(\u001b[39minput\u001b[39m):\n\u001b[1;32m    <a href='file:///~/miniconda3/envs/y3project/lib/python3.8/site-packages/torch/nn/functional.py?line=993'>994</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    <a href='file:///~/miniconda3/envs/y3project/lib/python3.8/site-packages/torch/nn/functional.py?line=994'>995</a>\u001b[0m         adaptive_max_pool1d_with_indices, (\u001b[39minput\u001b[39m,), \u001b[39minput\u001b[39m, output_size, return_indices\u001b[39m=\u001b[39mreturn_indices\n\u001b[1;32m    <a href='file:///~/miniconda3/envs/y3project/lib/python3.8/site-packages/torch/nn/functional.py?line=995'>996</a>\u001b[0m     )\n\u001b[0;32m--> <a href='file:///~/miniconda3/envs/y3project/lib/python3.8/site-packages/torch/nn/functional.py?line=996'>997</a>\u001b[0m \u001b[39mreturn\u001b[39;00m torch\u001b[39m.\u001b[39;49madaptive_max_pool1d(\u001b[39minput\u001b[39;49m, output_size)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# needs to be changed\n",
    "\n",
    "best_trans_params = None\n",
    "best_source_params = None\n",
    "best_rmse = np.inf\n",
    "\n",
    "for epoch in range(0, max_epoch):\n",
    "  for i in tqdm(range(1000)):\n",
    "    # batch_size = 50\n",
    "    users = next(train_iterator)\n",
    "    users = users.to(device)\n",
    "    loss_T = 0\n",
    "    loss_trans = 0\n",
    "    loss_S = 0\n",
    "    loss_image = 0\n",
    "\n",
    "    optimiser_T.zero_grad()\n",
    "    optimiser_FMT.zero_grad()\n",
    "    optimiser_I.zero_grad()\n",
    "    optimiser_U.zero_grad()\n",
    "    optimiser_trans.zero_grad()\n",
    "    optimiser_FMS.zero_grad()\n",
    "    optimiser_DI.zero_grad()\n",
    "    optimiser_DU.zero_grad()\n",
    "    optimiser_image.zero_grad()\n",
    "    optimiser_MFI.zero_grad()\n",
    "    optimiser_MFU.zero_grad()\n",
    "\n",
    "    for u in users:\n",
    "      '''Get data needed'''\n",
    "      user_meta, item_meta, item_image, unseen_image, rev_ui, rating_ui = sample_user(u, train_users, train_items, train_reviews, train_descriptions, train_categories, train_prices, None, train_ratings)\n",
    "      user_category, user_price, user_descriptions, user_reviews = user_meta\n",
    "      i, item_category, item_price, item_description, item_reviews = item_meta\n",
    "\n",
    "      '''\n",
    "      Training is as follows:\n",
    "      - Components to train:\n",
    "      1. TransNets\n",
    "      2. SiameseNetwork\n",
    "      3. Description Extractor\n",
    "      4. User / Item embedding MFs\n",
    "\n",
    "      - How they are trained:\n",
    "      1. TransNets: (Completed)\n",
    "        0. Principle: User review must be isolated from prediction networks in training time\n",
    "        1. Train target network predictor fm_T with real rev_ui, backpropagate loss_T\n",
    "        2. Train transform network (textCNN_U, textCNN_I, transform) by constuction z_L and compare with latent_rev_ui from target network\n",
    "        3. Train source network predictor fm_S with z_L\n",
    "      2. SiameseNetwork\n",
    "        1. Trained Individually by minimising the distance of user's preferred and not preferred images\n",
    "        2. The result is used to weight the final prediction\n",
    "      3. Description Extractors\n",
    "        1. Train with source network by generating latent_desc_u, latent_desc i\n",
    "        2. Feed latent_desc_u, latent_desc_i into fm_S and backpropagate errors\n",
    "      4. User / Item meta MFs\n",
    "        1. Train with source network by generating latent_uid, latent_iid\n",
    "        2. Feed latent_uid, latent_iid into fm_S and backpropagate errors\n",
    "      '''\n",
    "\n",
    "\n",
    "      '''TARGET AND TRANSFORM NETWORKS ARE NOT TO BE AFFECTED BY METADATA'''\n",
    "\n",
    "\n",
    "      '''Train target network on the actual review'''\n",
    "      latent_rev_ui = textCNN_T(rev_ui)\n",
    "      pred_T = fm_T(latent_rev_ui)\n",
    "      loss_T += l1_loss(rating_ui, pred_T)\n",
    "\n",
    "\n",
    "      '''THE SECTIONS BELOW ARE USED FOR PREDICTION'''\n",
    "\n",
    "      '''Learn to transform'''\n",
    "      latent_rev_user = textCNN_U(user_reviews)\n",
    "      latent_rev_item = textCNN_I(item_reviews)\n",
    "      z0 = torch.flatten(torch.cat((latent_rev_user, latent_rev_item), dim=0))\n",
    "      z_L = transform(z0) # 50\n",
    "      loss_trans += l2_loss(latent_rev_ui, z_L)\n",
    "\n",
    "      '''Train a predictor on the transformed input\n",
    "      This is where the altering begins'''\n",
    "\n",
    "      '''Description Extractor'''\n",
    "      latent_desc_i = textCNN_DI(item_description) # -> 36\n",
    "      latent_desc_u = textCNN_DU(user_descriptions) # -> 64\n",
    "\n",
    "      '''User and Item'''\n",
    "      latent_uid = mf_u(u)\n",
    "      latent_iid = mf_i(i) # -> 20\n",
    "\n",
    "      '''Meta data'''\n",
    "      cat_embed_u, cat_embed_i = F.one_hot(user_category, 24), F.one_hot(item_category, 24)  # -> 24\n",
    "\n",
    "      latent_final = torch.cat(\n",
    "        (z_L.flatten(), # 50\n",
    "        latent_uid, # 10\n",
    "        latent_iid, # 10\n",
    "        latent_desc_i.flatten(), # 36\n",
    "        latent_desc_u.flatten(), # 64\n",
    "        cat_embed_u, # 24\n",
    "        cat_embed_i, # 24\n",
    "        user_price, # 1\n",
    "        item_price), # 1\n",
    "        dim=0).view(1,220)\n",
    "\n",
    "      '''Image - trained seperatedly'''\n",
    "      # img_result_1, img_result_2 = imageCNN(item_image, unseen_image)\n",
    "      # s = imageCNN.state_dict()\n",
    "      # # regularisation term\n",
    "      # nn_regularisers = 0\n",
    "      # for k in s:\n",
    "      #   params = s[k].flatten()\n",
    "      #   nn_regularisers += torch.sum(params ** 2)/2\n",
    "      # cost_train = torch.sum(torch.log(F.sigmoid(torch.sum(torch.dot(torch.subtract(img_result_1, img_result_2),latent_final)).flatten())))\n",
    "      # cost_train -= 0.001 * nn_regularisers\n",
    "      # loss_image += cost_train\n",
    "      \n",
    "      pred_S = fm_S(latent_final)\n",
    "      loss_S += l1_loss(rating_ui, pred_S)\n",
    "\n",
    "    loss_S /= batch_size\n",
    "    loss_S.backward(retain_graph=True)\n",
    "    optimiser_FMS.step()\n",
    "    optimiser_MFU.step()\n",
    "    optimiser_MFI.step()\n",
    "    optimiser_DI.step()\n",
    "    optimiser_DU.step()\n",
    "    \n",
    "\n",
    "    loss_trans /= batch_size\n",
    "    loss_trans.backward(retain_graph=True)\n",
    "    optimiser_U.step()\n",
    "    optimiser_I.step()\n",
    "    optimiser_trans.step()\n",
    "\n",
    "    loss_T /= batch_size\n",
    "    loss_T.backward()\n",
    "    optimiser_T.step()\n",
    "    optimiser_FMT.step()\n",
    "\n",
    "    # independent backward\n",
    "    # loss_image /= batch_size\n",
    "    # loss_image.backward()\n",
    "    # optimiser_image.step()\n",
    "\n",
    "  with torch.no_grad():\n",
    "    save_training()\n",
    "    rmse = eval_model()\n",
    "    print(f\"epoch: [{epoch}/{max_epoch}]: rmse - {rmse}\")\n",
    "    if rmse < best_rmse:\n",
    "      best_trans_params = transform.parameters()\n",
    "      best_source_params = fm_S.parameters()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.11 ('y3project')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.11"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "9af667c24e890afcb270fe85ca560e1aa788703788da50f20e13467161b45801"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
