{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchvision\n",
    "from torchvision import transforms\n",
    "import matplotlib.pyplot as plt\n",
    "from torch.optim import Adam\n",
    "from tqdm import tqdm\n",
    "from torch.utils.data import DataLoader\n",
    "import random\n",
    "from collections import Counter\n",
    "import math\n",
    "from PIL import Image\n",
    "from pathlib import Path\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 16\n",
    "max_epoch = 40\n",
    "device = torch.device(\n",
    "    'cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
    "train_data_path = '../../Datasets/Model/train'\n",
    "test_data_path = '../../Datasets/Model/test'\n",
    "image_path = '../../Datasets/Images'\n",
    "data_save_path = '../../Save/Experiment2'\n",
    "analytics_path = '../../Analysis/Experiment2'\n",
    "lr = 0.0005\n",
    "rmse_arr = []\n",
    "tokens = []\n",
    "with open('../../Datasets/Model/nlp/tokens.txt', 'r') as f:\n",
    "  for line in f:\n",
    "    tokens.append(line.strip())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_users = np.load(f'{train_data_path}/train_users.npy')\n",
    "train_items = np.load(f'{train_data_path}/train_items.npy')\n",
    "train_ratings = np.load(f'{train_data_path}/train_ratings.npy')\n",
    "train_reviews = np.load(\n",
    "    f'{train_data_path}/train_reviews.npy', allow_pickle=True)\n",
    "train_descriptions = np.load(\n",
    "    f'{train_data_path}/train_descriptions.npy', allow_pickle=True)\n",
    "train_prices = np.load(f'{train_data_path}/train_prices.npy')\n",
    "train_categories = np.load(f'{train_data_path}/train_categories.npy')\n",
    "\n",
    "test_users = np.load(f'{test_data_path}/test_users.npy')\n",
    "test_items = np.load(f'{test_data_path}/test_items.npy')\n",
    "test_ratings = np.load(f'{test_data_path}/test_ratings.npy')\n",
    "test_reviews = np.load(f'{test_data_path}/test_reviews.npy', allow_pickle=True)\n",
    "test_descriptions = np.load(\n",
    "    f'{test_data_path}/test_descriptions.npy', allow_pickle=True)\n",
    "test_prices = np.load(f'{test_data_path}/test_prices.npy')\n",
    "test_categories = np.load(f'{test_data_path}/test_categories.npy')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cycle(iterable):\n",
    "  while True:\n",
    "    for x in iterable:\n",
    "      yield x\n",
    "\n",
    "\n",
    "train_iterator = cycle(DataLoader(\n",
    "    np.unique(train_users), batch_size=batch_size, shuffle=True))\n",
    "\n",
    "test_iterator = cycle(DataLoader(\n",
    "    np.unique(test_users), batch_size=batch_size, shuffle=True\n",
    "))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "  input: user u, sets\n",
    "  item: i\n",
    "  output: user_reviews <- [reviews of u],  \n",
    "          user[descriptions that u has seen], \n",
    "          most liked category of u, \n",
    "          average price of u purchasing items, \n",
    "          item_reviews <- [reviews of i],\n",
    "          item_description <- description of i,\n",
    "          item_price <- price of i,\n",
    "          item_image <- image of i,\n",
    "          unseen_image <- the image of an unseen item to u,\n",
    "          rev_ui <- rating of u on i\n",
    "\n",
    "  status: this function works fine\n",
    "'''\n",
    "\n",
    "\n",
    "'''vectorise code'''\n",
    "\n",
    "\n",
    "def sample_user(user, user_set, item_set, rev_set, desc_set, cat_set, price_set, img_set, rating_set):\n",
    "  # -> u\n",
    "  user = int(user)\n",
    "  user_meta = []\n",
    "  item_meta = []\n",
    "  user_reviews = []\n",
    "  user_descriptions = []\n",
    "  user_category = []\n",
    "  user_price = []\n",
    "  item_reviews = []\n",
    "  item_description = None\n",
    "  item_category = None\n",
    "  item_price = None\n",
    "  item_image = None\n",
    "  unseen_image = None\n",
    "  rev_ui = None\n",
    "\n",
    "  user_indicies = np.where(user_set == user)[0]\n",
    "\n",
    "  # generates the index of a rated item for the user\n",
    "  chosen_user_item_idx = user_indicies[random.randint(0, len(user_indicies)-1)]\n",
    "\n",
    "  # randomly select a rated item -> i\n",
    "  item = item_set[chosen_user_item_idx]\n",
    "  item_description = list(desc_set[chosen_user_item_idx])\n",
    "  item_category = cat_set[chosen_user_item_idx]\n",
    "  item_price = price_set[chosen_user_item_idx]\n",
    "\n",
    "  # gets the rating of u, i\n",
    "  rating_ui = rating_set[chosen_user_item_idx]\n",
    "\n",
    "  # gets item indices to get all reviews of the item -> i\n",
    "  item_indicies = np.where(item_set == item)[0]\n",
    "\n",
    "  # all indicies in arrays are same (identical to user review number)\n",
    "  for u_idx in user_indicies:\n",
    "      # out of reviews, other metadata is always accessible either or not in test mode\n",
    "      user_descriptions += list(desc_set[u_idx])\n",
    "      user_category.append(cat_set[u_idx])\n",
    "      user_price.append(price_set[u_idx].astype(np.float32))\n",
    "      if u_idx != chosen_user_item_idx:\n",
    "        user_reviews += list(rev_set[u_idx])\n",
    "      else:\n",
    "        rev_ui = list(rev_set[u_idx])\n",
    "\n",
    "  for i_idx in item_indicies:\n",
    "    if i_idx != chosen_user_item_idx:\n",
    "      item_reviews += list(rev_set[i_idx])\n",
    "\n",
    "  # according to the transnets paper - regularise the combined strings\n",
    "  if len(rev_ui) > 1000:\n",
    "    rev_ui = rev_ui[:1000]\n",
    "  if len(user_reviews) > 1000:\n",
    "    user_reviews = user_reviews[:1000]\n",
    "  if len(user_descriptions) > 1000:\n",
    "    user_descriptions = user_descriptions[:1000]\n",
    "  if len(item_reviews) > 1000:\n",
    "    item_reviews = item_reviews[:1000]\n",
    "\n",
    "  if len(rev_ui) <= 3:\n",
    "    rev_ui += [0] * 3\n",
    "  if len(user_reviews) <= 3:\n",
    "    user_reviews += [0] * 3\n",
    "  if len(item_reviews) <= 3:\n",
    "    item_reviews += [0] * 3\n",
    "  if len(user_descriptions) <= 3:\n",
    "    user_descriptions += [0] * 3\n",
    "\n",
    "  # for computational correctness\n",
    "  user_reviews, user_descriptions, item_reviews, item_description = torch.tensor(user_reviews), torch.tensor(user_descriptions),\\\n",
    "      torch.tensor(item_reviews), torch.tensor(item_description)\n",
    "\n",
    "  user_category = Counter(user_category).most_common(1)[0][0]\n",
    "  user_price = sum(user_price)/len(user_price)\n",
    "\n",
    "  rev_ui = torch.from_numpy(np.array(rev_ui))\n",
    "\n",
    "  user_meta = [torch.tensor(user_category), torch.tensor(\n",
    "      [user_price], dtype=torch.float32), user_descriptions, user_reviews]\n",
    "  item_meta = [torch.tensor(item), torch.tensor(item_category), torch.tensor(\n",
    "      [item_price], dtype=torch.float32), item_description, item_reviews]\n",
    "\n",
    "  return user_meta, item_meta, item_image, unseen_image, rev_ui, torch.tensor(rating_ui, dtype=torch.float32)\n",
    "\n",
    "\n",
    "def weights_init(m):\n",
    "  if type(m) in (nn.Linear, nn.Conv1d, nn.Conv2d, nn.Embedding):\n",
    "    nn.init.xavier_normal_(m.weight)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FM(nn.Module):\n",
    "    def __init__(self, latent_dim, fea_num):\n",
    "        super().__init__()\n",
    "\n",
    "        self.latent_dim = latent_dim\n",
    "        self.w0 = nn.Parameter(torch.zeros([1, ]))\n",
    "        self.w1 = nn.Parameter(torch.rand([fea_num, 1]))\n",
    "        self.w2 = nn.Parameter(torch.rand([fea_num, latent_dim]))\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        # inputs = inputs.long()\n",
    "        first_order = self.w0 + torch.mm(inputs, self.w1)\n",
    "        second_order = 1/2 * torch.sum(\n",
    "            torch.pow(torch.mm(inputs, self.w2), 2) -\n",
    "            torch.mm(torch.pow(inputs, 2), torch.pow(self.w2, 2)),\n",
    "\n",
    "            dim=1,\n",
    "            keepdim=True\n",
    "        )\n",
    "\n",
    "        return first_order + second_order\n",
    "\n",
    "\n",
    "class GMF(nn.Module):\n",
    "    def __init__(self, inp_range, latent_dim=20, dropout=True):\n",
    "        super().__init__()\n",
    "        self.emb = nn.Embedding(\n",
    "            num_embeddings=inp_range, embedding_dim=latent_dim)\n",
    "        self.dropout = nn.Dropout(0.5)\n",
    "        self.use_dropout = dropout\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        embedding = self.emb(inputs)\n",
    "        if self.use_dropout:\n",
    "          embedding = self.dropout(embedding)\n",
    "\n",
    "        return embedding\n",
    "\n",
    "\n",
    "fm_S = FM(8, 50).apply(weights_init).to(device)\n",
    "\n",
    "mf_u = GMF(max(train_users)+1, 25, True).apply(weights_init).to(device)\n",
    "# item latent vectors -> this embedding should be updated\n",
    "mf_i = GMF(max(max(train_items), max(test_items))+1,\n",
    "           25, True).apply(weights_init).to(device)\n",
    "\n",
    "optimiser_MFU = torch.optim.Adam(\n",
    "    params=mf_u.parameters(), lr=lr)\n",
    "optimiser_MFI = torch.optim.Adam(\n",
    "    params=mf_i.parameters(), lr=lr)\n",
    "\n",
    "optimiser_FMS = torch.optim.Adam(\n",
    "    params=fm_S.parameters(), lr=lr, weight_decay=0.01)\n",
    "\n",
    "\n",
    "def save_training():\n",
    "  torch.save(\n",
    "      {\n",
    "       'fmS': fm_S.state_dict(),\n",
    "       'mf_u': mf_u.state_dict(),\n",
    "       'mf_i': mf_i.state_dict()},\n",
    "      f'{data_save_path}/gmf.chkpt')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_rmse(preds, real):\n",
    "    rmse = 0\n",
    "    # produce known rui set\n",
    "    for i in range(len(preds)):\n",
    "        rmse += (preds[i] - real[i]) ** 2\n",
    "\n",
    "    rmse = math.sqrt(rmse/len(preds))\n",
    "\n",
    "    return rmse\n",
    "\n",
    "\n",
    "def l1regularise(mode):\n",
    "  global textCNN_I\n",
    "  global textCNN_U\n",
    "  global textCNN_T\n",
    "  global transform\n",
    "  global fm_T\n",
    "  global fm_S\n",
    "\n",
    "  model = None\n",
    "\n",
    "  if mode == 'I':\n",
    "    model = textCNN_I\n",
    "  elif mode == 'U':\n",
    "    model = textCNN_U\n",
    "  elif mode == 'T':\n",
    "    model = textCNN_T\n",
    "  elif mode == 'transform':\n",
    "    model = transform\n",
    "  elif mode == 'fmt':\n",
    "    model = fm_T\n",
    "  else:\n",
    "    model = fm_S\n",
    "\n",
    "  reg_loss = 0\n",
    "  for param in model.parameters():\n",
    "    reg_loss += torch.sum(torch.abs(param))\n",
    "\n",
    "  return reg_loss\n",
    "\n",
    "\n",
    "def eval_model():\n",
    "  preds = []\n",
    "  real = []\n",
    "  for u in tqdm(np.unique(test_users)):\n",
    "    user_meta, item_meta, _, _, _, rating_ui = sample_user(\n",
    "        u, test_users, test_items, test_reviews, test_descriptions, test_categories, test_prices, None, test_ratings)\n",
    "    _, _, _, user_reviews = user_meta\n",
    "    i, _, _, _, item_reviews = item_meta\n",
    "    user_reviews = user_reviews.to(device)\n",
    "    item_reviews = item_reviews.to(device)\n",
    "    i = i.to(device)\n",
    "\n",
    "    '''User and Item'''\n",
    "    latent_uid = mf_u(torch.tensor(u).to(device))\n",
    "    latent_iid = mf_i(i)  # -> 20\n",
    "\n",
    "    latent_final = torch.cat(\n",
    "        (\n",
    "         latent_uid,  # 10\n",
    "         latent_iid  # 10\n",
    "         ),  # 1\n",
    "        dim=0).view(1, 50)\n",
    "\n",
    "    pred = fm_S(latent_final)\n",
    "    preds.append(pred)\n",
    "    real.append(rating_ui)\n",
    "\n",
    "  return eval_rmse(preds, real)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"evaluating baseline rmse, please wait...\")\n",
    "\n",
    "base_rmse = eval_model()\n",
    "\n",
    "# start training\n",
    "print(f\"training started, baseline rmse:{base_rmse}\")\n",
    "\n",
    "best_trans_params = None\n",
    "best_source_params = None\n",
    "best_rmse = np.inf\n",
    "\n",
    "l1_loss = nn.L1Loss(reduction='mean')\n",
    "l2_loss = nn.MSELoss(reduction='mean')\n",
    "\n",
    "for epoch in range(0, max_epoch):\n",
    "  for i in tqdm(range(1000)):\n",
    "    # batch_size = 50\n",
    "    users = next(train_iterator)\n",
    "    users = users.to(device)\n",
    "    loss_S = 0\n",
    "\n",
    "    for u in users:\n",
    "      '''Get data needed'''\n",
    "      user_meta, item_meta, _, _, rev_ui, rating_ui = sample_user(\n",
    "          u, train_users, train_items, train_reviews, train_descriptions, train_categories, train_prices, None, train_ratings)\n",
    "      _, _, _, user_reviews = user_meta\n",
    "      i, _, _, _, item_reviews = item_meta\n",
    "      user_reviews = user_reviews.to(device)\n",
    "      item_reviews = item_reviews.to(device)\n",
    "      i = i.to(device)\n",
    "\n",
    "      '''Train a predictor on the transformed input'''\n",
    "\n",
    "      '''User and Item'''\n",
    "      latent_uid = mf_u(u)\n",
    "      latent_iid = mf_i(i)  # -> 20\n",
    "\n",
    "      latent_final = torch.cat(\n",
    "          (\n",
    "           latent_uid,  # 10\n",
    "           latent_iid  # 10\n",
    "           ),  # 1\n",
    "          dim=0).view(1, 50)\n",
    "\n",
    "      pred_S = fm_S(latent_final)\n",
    "      loss_S += l1_loss(rating_ui, torch.flatten(pred_S)[0])\n",
    "\n",
    "    optimiser_MFI.zero_grad()\n",
    "    optimiser_MFU.zero_grad()\n",
    "    optimiser_FMS.zero_grad()\n",
    "    loss_S /= batch_size\n",
    "    loss_S.backward()\n",
    "    optimiser_FMS.step()\n",
    "    optimiser_MFU.step()\n",
    "    optimiser_MFI.step()\n",
    "\n",
    "  with torch.no_grad():\n",
    "    rmse = eval_model()\n",
    "    rmse_arr.append(rmse)\n",
    "\n",
    "    if rmse < best_rmse:\n",
    "      best_rmse = rmse\n",
    "      save_training()\n",
    "    print(f\"epoch: [{epoch}/{max_epoch}]: rmse - {rmse}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_training()"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "9af667c24e890afcb270fe85ca560e1aa788703788da50f20e13467161b45801"
  },
  "kernelspec": {
   "display_name": "Python 3.8.11 ('y3project')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
