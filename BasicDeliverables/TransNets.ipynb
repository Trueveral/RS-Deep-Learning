{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from tqdm import tqdm\n",
    "from torch.utils.data import DataLoader\n",
    "import random\n",
    "from collections import Counter\n",
    "import math\n",
    "import os\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 32\n",
    "max_epoch = 40\n",
    "lr = 0.0005\n",
    "\n",
    "SINGLE_REVIEW_LIMIT = 100\n",
    "USER_REVIEW_LIMIT = 1000\n",
    "ITEM_REVIEW_LIMIT = 1000\n",
    "\n",
    "\n",
    "device = torch.device(\n",
    "    'cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
    "train_data_path = '../../Datasets/Model/train'\n",
    "test_data_path = '../../Datasets/Model/test'\n",
    "image_path = '../../Datasets/Images'\n",
    "data_save_path = '../../Save/BasicDeliverables/TransNets'\n",
    "analytics_path = '../../Analysis/BasicDeliverables/TransNets'\n",
    "rmse_arr = []\n",
    "tokens = []\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_users = np.load(f'{train_data_path}/train_users.npy')\n",
    "train_items = np.load(f'{train_data_path}/train_items.npy')\n",
    "train_ratings = np.load(f'{train_data_path}/train_ratings.npy')\n",
    "train_reviews = np.load(\n",
    "    f'{train_data_path}/train_reviews.npy', allow_pickle=True)\n",
    "\n",
    "test_users = np.load(f'{test_data_path}/test_users.npy')\n",
    "test_items = np.load(f'{test_data_path}/test_items.npy')\n",
    "test_ratings = np.load(f'{test_data_path}/test_ratings.npy')\n",
    "test_reviews = np.load(f'{test_data_path}/test_reviews.npy', allow_pickle=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this function cuts a sequence to a certain length, if the sequence is shorter than\n",
    "# that length, it fills the rest with a parameterised placeholder, if the sequence is longer than\n",
    "# that length, it cuts the sequence to that length.\n",
    "def pad_sequence(sequence, length, placeholder=0):\n",
    "  if len(sequence) > length:\n",
    "    sequence = sequence[:length]\n",
    "  else:\n",
    "    sequence += [placeholder] * (length - len(sequence))\n",
    "  return sequence\n",
    "\n",
    "'''sample user'''\n",
    "'''this function takes a user, who is from a set of users, a set of items, a set of reviews, a set of titles, \n",
    "a set of categories, a set of prices, a set of ratings.'''\n",
    "\n",
    "def sample_user(user, users, items, reviews, ratings):\n",
    "  # find out all the indicies of the user in users\n",
    "  user_indices = np.where(users == user)[0]\n",
    "  # randomly select an index from these indicies.\n",
    "  user_index = np.random.choice(user_indices)\n",
    "\n",
    "  # get the item that the user has purchased.\n",
    "  item = items[user_index]\n",
    "  # get the review that the user has written about that item.\n",
    "  review = reviews[user_index]\n",
    "  # get the rating that the user has given to that item.\n",
    "  rating = ratings[user_index]\n",
    "  # find out the indices of the item in items\n",
    "  item_indices = np.where(items == item)[0]\n",
    "  # get the reviews of that item, exept the reviews of the user\n",
    "  item_reviews = [reviews[i] for i in item_indices if i != user_index]\n",
    "  # flatten the list_reviews \n",
    "  item_reviews = [item for sublist in item_reviews for item in sublist]\n",
    "\n",
    "  '''user reviews'''\n",
    "  # get all the reviews of the items that the user has purchased, except the review of the user on that item\n",
    "  user_reviews = [reviews[i] for i in user_indices if i != user_index]\n",
    "  # flatten user_reviews\n",
    "  user_reviews = [item for sublist in user_reviews for item in sublist]\n",
    "  \n",
    "\n",
    "  # apply function pad_sequence to the user_titles, user_reviews, item_reviews, title, with padding 0\n",
    "  review = pad_sequence(review, SINGLE_REVIEW_LIMIT)\n",
    "  user_reviews = pad_sequence(user_reviews, USER_REVIEW_LIMIT)\n",
    "  item_reviews = pad_sequence(item_reviews, ITEM_REVIEW_LIMIT)\n",
    "\n",
    "  return user, item, review, rating, item_reviews, user_reviews\n",
    "\n",
    "\n",
    "'''function to get a batch of training samples.\n",
    "this function selects a random user from a set of users\n",
    "and gets his information by function sample_user, repeat it for batch_size times and \n",
    "gets the batch of samples. Each element in the batch tuple is then transormed to a pytorch tensor and returned.'''\n",
    "def get_batch(users, items, reviews, ratings, batch_size=32, fixed_users_set = None):\n",
    "  # get a batch of users\n",
    "  if fixed_users_set is None:\n",
    "    batch_users = np.random.choice(users, size=batch_size)\n",
    "  else:\n",
    "    batch_users = fixed_users_set\n",
    "  # get the batch of samples\n",
    "  batch = [list(sample_user(user, users, items, reviews, ratings)) for user in batch_users]\n",
    "  # transform each column of the batch to a pytorch tensor\n",
    "  batch = [torch.tensor(sample) for sample in zip(*batch)]\n",
    "  return batch\n",
    "\n",
    "\n",
    "def weights_init(m):\n",
    "  if isinstance(m, nn.Linear):\n",
    "    nn.init.xavier_uniform_(m.weight.data)\n",
    "    nn.init.constant_(m.bias.data, 0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ref: https://github.com/zhongqiangwu960812/AI-RecommenderSystem/blob/master/Rank/DeepFM/DeepFM_Model.ipynb\n",
    "'''builds a factorisation machine that is used to predict the rating of an item by a user.\n",
    "parameters: latent_dim: the dimension of the latent factors.\n",
    "            fea_num: the number of features.'''\n",
    "            \n",
    "class FM(nn.Module):\n",
    "    def __init__(self, latent_dim, fea_num):\n",
    "        super().__init__()\n",
    "\n",
    "        self.latent_dim = latent_dim\n",
    "        self.w0 = nn.Parameter(torch.zeros([1, ]))\n",
    "        self.w1 = nn.Parameter(torch.rand([fea_num, 1]))\n",
    "        self.w2 = nn.Parameter(torch.rand([fea_num, latent_dim]))\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        # inputs = inputs.long()\n",
    "        first_order = self.w0 + torch.mm(inputs, self.w1)\n",
    "        second_order = 1/2 * torch.sum(\n",
    "            torch.pow(torch.mm(inputs, self.w2), 2) -\n",
    "            torch.mm(torch.pow(inputs, 2), torch.pow(self.w2, 2)),\n",
    "\n",
    "            dim=1,\n",
    "            keepdim=True\n",
    "        )\n",
    "\n",
    "        return first_order + second_order\n",
    "\n",
    "\n",
    "class TextCNN(nn.Module):\n",
    "    def __init__(self, vocab_size, embed_size, kernel_sizes, num_neurons, latent_dim):\n",
    "        super(TextCNN, self).__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, embed_size)\n",
    "        self.dropout = nn.Dropout(0.5)\n",
    "        self.decoder = nn.Linear(sum(num_neurons), latent_dim)\n",
    "        self.pool = nn.AdaptiveMaxPool1d(1)\n",
    "        self.tanh = nn.Tanh()\n",
    "        self.convs = nn.ModuleList()\n",
    "        for c, k in zip(num_neurons, kernel_sizes):\n",
    "            self.convs.append(nn.Conv1d(embed_size, c, k))\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        embeddings = self.embedding(inputs)\n",
    "        embeddings = embeddings.permute(0, 2, 1)\n",
    "        encoding = torch.cat([\n",
    "            torch.squeeze(self.tanh(self.pool(conv(embeddings))), dim=-1)\n",
    "            for conv in self.convs], dim=1)\n",
    "        outputs = self.decoder(self.dropout(encoding))\n",
    "        return outputs\n",
    "\n",
    "\n",
    "class TransformMLP(nn.Module):\n",
    "  def __init__(self, concated_size, latent_vector_size):\n",
    "      super().__init__()\n",
    "      self.net = nn.Sequential(\n",
    "          nn.Linear(concated_size, 2*concated_size),\n",
    "          nn.Tanh(),\n",
    "          nn.Linear(2*concated_size, latent_vector_size),\n",
    "          nn.Tanh()\n",
    "      )\n",
    "      self.dropout = nn.Dropout(0.5)\n",
    "\n",
    "  def forward(self, x, batch_size=32):\n",
    "    out = self.dropout(self.net(x))\n",
    "    out = out.view(batch_size, out.shape[1])\n",
    "    return out\n",
    "\n",
    "\n",
    "textCNN_I = TextCNN(80000, 50, [3], [100], 50).apply(weights_init).to(device)\n",
    "textCNN_U = TextCNN(80000, 50, [3], [100], 50).apply(\n",
    "    weights_init).to(device)\n",
    "textCNN_T = TextCNN(80000, 50, [3], [100], 50).apply(\n",
    "    weights_init).to(device)\n",
    "\n",
    "\n",
    "transform = TransformMLP(100, 50).apply(weights_init).to(device)\n",
    "fm_T = FM(8, 50).apply(weights_init).to(device)\n",
    "fm_S = FM(8, 50).apply(weights_init).to(device)\n",
    "\n",
    "'''optimisers for the models, with weight decay of 0.01, and learning rate of lr'''\n",
    "optimiser_textCNN_I = torch.optim.Adam(textCNN_I.parameters(), lr=lr)\n",
    "optimiser_textCNN_U = torch.optim.Adam(textCNN_U.parameters(), lr=lr)\n",
    "optimiser_textCNN_T = torch.optim.Adam(textCNN_T.parameters(), lr=lr)\n",
    "optimiser_transform = torch.optim.Adam(transform.parameters(), lr=lr)\n",
    "optimiser_fm_T = torch.optim.Adam(fm_T.parameters(), lr=lr)\n",
    "optimiser_fm_S = torch.optim.Adam(fm_S.parameters(), lr=lr)\n",
    "\n",
    "\n",
    "'''function that saves the training, path is the path to the folder where the model will be saved'''\n",
    "def save_training(path):\n",
    "    torch.save(textCNN_I.state_dict(), path + 'textCNN_I.pth')\n",
    "    torch.save(textCNN_U.state_dict(), path + 'textCNN_U.pth')\n",
    "    torch.save(textCNN_T.state_dict(), path + 'textCNN_T.pth')\n",
    "    torch.save(transform.state_dict(), path + 'transform.pth')\n",
    "\n",
    "'''function that loads the model, path is the path to the folder where the model is saved'''\n",
    "def load_training(path):\n",
    "    textCNN_I.load_state_dict(torch.load(path + 'textCNN_I.pth'))\n",
    "    textCNN_U.load_state_dict(torch.load(path + 'textCNN_U.pth'))\n",
    "    textCNN_T.load_state_dict(torch.load(path + 'textCNN_T.pth'))\n",
    "    transform.load_state_dict(torch.load(path + 'transform.pth'))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''Training'''\n",
    "\n",
    "def evaluation(test_batch_size = 32):\n",
    "  test_users_unique = list(set(test_users))\n",
    "  rmse_arr = []\n",
    "\n",
    "  for i in tqdm(range(0, len(test_users_unique), test_batch_size)):\n",
    "    test_users_batch = test_users_unique[i:i+test_batch_size]\n",
    "    batch_test = get_batch(test_users, test_items, test_reviews, test_ratings, fixed_users_set=test_users_batch)\n",
    "    user_test, item_test, review_test, rating_test, item_reviews_test, user_reviews_test = batch_test\n",
    "    user_test, item_test, review_test, rating_test, item_reviews_test, user_reviews_test = \\\n",
    "      user_test.to(device), item_test.to(device), review_test.to(device), rating_test.to(device), item_reviews_test.to(device), user_reviews_test.to(device)\n",
    "\n",
    "    latent_review_user = textCNN_U(user_reviews_test)\n",
    "    latent_review_item = textCNN_I(item_reviews_test)\n",
    "    latent_review_user_item = torch.cat((latent_review_user, latent_review_item), 1)\n",
    "    transform_construction = transform(latent_review_user_item, batch_size=len(test_users_batch))\n",
    "    # flatten the transform_construction\n",
    "    transform_construction = transform_construction.view(transform_construction.size(0), -1)\n",
    "\n",
    "    prediction_source = fm_S(transform_construction)\n",
    "    # calculate rooted mean square error\n",
    "    rmse_arr.append(torch.sqrt(torch.mean((prediction_source - rating_test)**2)).item())\n",
    "  \n",
    "  # return the mean of the rmse_arr\n",
    "  return np.mean(rmse_arr)\n",
    "\n",
    "print(\"Program: Evaluating the baseline RMSE of the model on the test set\")\n",
    "\n",
    "# set best_rmse to be the largest possible value\n",
    "best_rmse = evaluation()\n",
    "\n",
    "# print baseline best_rmse\n",
    "print('Baseline RMSE:', best_rmse)\n",
    "\n",
    "for epoch in range (max_epoch):\n",
    "  for i in tqdm(range(1000)):\n",
    "    batch =  get_batch(train_users, train_items, train_reviews, train_ratings)\n",
    "    user, item, review, rating, item_reviews, user_reviews = batch\n",
    "    user, item, review, rating, item_reviews, user_reviews = user.to(device), item.to(device), review.to(device), rating.to(device), item_reviews.to(device), user_reviews.to(device)\n",
    "\n",
    "    '''Train target network'''\n",
    "    latent_review = textCNN_T(review)\n",
    "    latent_review_data = latent_review.data\n",
    "    prediction_target = fm_T(latent_review)\n",
    "    # calculate the L1 loss\n",
    "    loss_target = torch.mean(torch.abs(prediction_target - rating))\n",
    "\n",
    "    # backpropagation\n",
    "    optimiser_textCNN_T.zero_grad()\n",
    "    optimiser_fm_T.zero_grad()\n",
    "    loss_target.backward()\n",
    "    optimiser_fm_T.step()\n",
    "    optimiser_textCNN_T.step()\n",
    "\n",
    "    '''Train transform network'''\n",
    "    latent_review_user = textCNN_U(user_reviews)\n",
    "    latent_review_item = textCNN_I(item_reviews)\n",
    "\n",
    "    # concatenate the latent vectors and flatten them\n",
    "    latent_review_user_item = torch.cat((latent_review_user, latent_review_item), 1)\n",
    "    transform_construction = transform(latent_review_user_item)\n",
    "    transform_construction = transform_construction.view(transform_construction.size(0), -1)\n",
    "    # calculate the L2 loss between transform_construction and latent_review_data\n",
    "    loss_transform = torch.mean((transform_construction - latent_review_data) ** 2)\n",
    "\n",
    "    # backpropagation\n",
    "    optimiser_transform.zero_grad()\n",
    "    optimiser_textCNN_I.zero_grad()\n",
    "    optimiser_textCNN_U.zero_grad()\n",
    "    loss_transform.backward()\n",
    "    optimiser_textCNN_I.step()\n",
    "    optimiser_textCNN_U.step()\n",
    "    optimiser_transform.step()\n",
    "\n",
    "    '''Train source network'''\n",
    "    transform_construction_data = transform_construction.data\n",
    "    prediction_source = fm_S(transform_construction_data)\n",
    "    \n",
    "    # calculate the L1 loss\n",
    "    loss_source = torch.mean(torch.abs(prediction_source - rating))\n",
    "\n",
    "    # backpropagation\n",
    "    optimiser_fm_S.zero_grad()\n",
    "    loss_source.backward()\n",
    "    optimiser_fm_S.step()\n",
    "\n",
    "  '''evaluate the model on the test set'''\n",
    "  rmse_test = evaluation()\n",
    "  # print out the loss of the models and the test rmse\n",
    "  print('Epoch:', epoch, 'Loss_target:', loss_target.item(), 'Loss_transform:', loss_transform.item(), 'Loss_source:', loss_source.item(), 'RMSE_test:', rmse_test)\n",
    "  if rmse_test < best_rmse:\n",
    "    best_rmse = rmse_test\n",
    "    save_training(data_save_path+'/')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open (f'{analytics_path}/TransNets.txt', 'w') as f:\n",
    "  for r in rmse_arr:\n",
    "    f.write(str(np.round(r, 4)) + '\\n')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.11 ('y3project')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.11"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "9af667c24e890afcb270fe85ca560e1aa788703788da50f20e13467161b45801"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
