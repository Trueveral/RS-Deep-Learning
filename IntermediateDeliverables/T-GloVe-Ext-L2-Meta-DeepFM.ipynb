{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from tqdm import tqdm\n",
    "from torch.utils.data import DataLoader\n",
    "import random\n",
    "from collections import Counter\n",
    "import math\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://zh-v2.d2l.ai/chapter_natural-language-processing-applications/sentiment-analysis-cnn.html\n",
    "\n",
    "class TokenEmbedding:\n",
    "    \"\"\"Token Embedding.\"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        self.idx_to_token, self.idx_to_vec = self._load_embedding()\n",
    "        self.unknown_idx = 0\n",
    "        # a dictionary like {'best':1} from idx_to_token\n",
    "        self.token_to_idx = {\n",
    "            token: idx for idx, token in enumerate(self.idx_to_token)}\n",
    "\n",
    "    def _load_embedding(self):\n",
    "        idx_to_token, idx_to_vec = ['<unk>'], []\n",
    "        data_dir = '../../Datasets/Model/nlp/'\n",
    "        # GloVe website: https://nlp.stanford.edu/projects/glove/\n",
    "        # fastText website: https://fasttext.cc/\n",
    "        with open(os.path.join(data_dir, 'vec.txt'), encoding='gb18030', errors='ignore') as f:\n",
    "            for line in f:\n",
    "                elems = line.rstrip().split(' ')\n",
    "                # Skip header information, such as the top row in fastText\n",
    "                # structure: hello  [0.1, 0.2, 0.3, ...]\n",
    "                token, elems = elems[0], [float(elem) for elem in elems[1:]]\n",
    "                if len(elems) > 1:\n",
    "                    idx_to_token.append(token)\n",
    "                    idx_to_vec.append(elems)\n",
    "        idx_to_vec = [[0] * len(idx_to_vec[0])] + idx_to_vec\n",
    "        return idx_to_token, torch.tensor(idx_to_vec)\n",
    "\n",
    "    def __getitem__(self, tokens):\n",
    "        indices = [\n",
    "            self.token_to_idx.get(token, self.unknown_idx)\n",
    "            for token in tokens]\n",
    "        vecs = self.idx_to_vec[torch.tensor(indices)]\n",
    "        return vecs\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.idx_to_token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 32\n",
    "max_epoch = 40\n",
    "lr = 0.0005\n",
    "\n",
    "SINGLE_REVIEW_LIMIT = 100\n",
    "USER_REVIEW_LIMIT = 1000\n",
    "ITEM_REVIEW_LIMIT = 1000\n",
    "\n",
    "USER_TITLE_LIMIT = 100\n",
    "ITEM_TITLE_LIMIT = 15\n",
    "\n",
    "device = torch.device(\n",
    "    'cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
    "train_data_path = '../../Datasets/Model/train'\n",
    "test_data_path = '../../Datasets/Model/test'\n",
    "image_path = '../../Datasets/Images'\n",
    "data_save_path = '../../Save/IntermediateDeliverables/T-GloVe-Ext-L2-Meta-DeepFM'\n",
    "analytics_path = '../../Analysis/IntermediateDeliverables'\n",
    "\n",
    "rmse_arr = []\n",
    "tokens = []\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_users = np.load(f'{train_data_path}/train_users.npy')\n",
    "train_items = np.load(f'{train_data_path}/train_items.npy')\n",
    "train_ratings = np.load(f'{train_data_path}/train_ratings.npy')\n",
    "train_reviews = np.load(\n",
    "    f'{train_data_path}/train_reviews.npy', allow_pickle=True)\n",
    "train_descriptions = np.load(\n",
    "    f'{train_data_path}/train_descriptions.npy', allow_pickle=True)\n",
    "train_prices = np.load(f'{train_data_path}/train_prices.npy')\n",
    "train_categories = np.load(f'{train_data_path}/train_categories.npy')\n",
    "\n",
    "test_users = np.load(f'{test_data_path}/test_users.npy')\n",
    "test_items = np.load(f'{test_data_path}/test_items.npy')\n",
    "test_ratings = np.load(f'{test_data_path}/test_ratings.npy')\n",
    "test_reviews = np.load(f'{test_data_path}/test_reviews.npy', allow_pickle=True)\n",
    "test_descriptions = np.load(\n",
    "    f'{test_data_path}/test_descriptions.npy', allow_pickle=True)\n",
    "test_prices = np.load(f'{test_data_path}/test_prices.npy')\n",
    "test_categories = np.load(f'{test_data_path}/test_categories.npy')\n",
    "\n",
    "with open('../../Datasets/Model/nlp/keys_review.txt', 'rb') as f:\n",
    "  for line in f:\n",
    "    tokens.append(line.strip())\n",
    "w2v = TokenEmbedding()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this function cuts a sequence to a certain length, if the sequence is shorter than\n",
    "# that length, it fills the rest with a parameterised placeholder, if the sequence is longer than\n",
    "# that length, it cuts the sequence to that length.\n",
    "def pad_sequence(sequence, length, placeholder=0):\n",
    "  if len(sequence) > length:\n",
    "    sequence = sequence[:length]\n",
    "  else:\n",
    "    sequence += [placeholder] * (length - len(sequence))\n",
    "  return sequence\n",
    "\n",
    "'''sample user'''\n",
    "'''this function takes a user, who is from a set of users, a set of items, a set of reviews, a set of titles, \n",
    "a set of categories, a set of prices, a set of ratings.'''\n",
    "def sample_user(user, users, items, reviews, categories, prices, ratings):\n",
    "  # find out all the indicies of the user in users\n",
    "  user_indices = np.where(users == user)[0]\n",
    "  # randomly select an index from these indicies.\n",
    "  user_index = np.random.choice(user_indices)\n",
    "\n",
    "  # get the item that the user has purchased.\n",
    "  item = items[user_index]\n",
    "  # get the review that the user has written about that item.\n",
    "  review = reviews[user_index]\n",
    "  # get the rating that the user has given to that item.\n",
    "  rating = ratings[user_index]\n",
    "  # get the category of that item\n",
    "  category = categories[user_index]\n",
    "  # get the price of that item\n",
    "  price = prices[user_index]\n",
    "  # find out the indices of the item in items\n",
    "  item_indices = np.where(items == item)[0]\n",
    "  # get the reviews of that item, exept the reviews of the user\n",
    "  item_reviews = [reviews[i] for i in item_indices if i != user_index]\n",
    "  # flatten the list_reviews \n",
    "  item_reviews = [item for sublist in item_reviews for item in sublist]\n",
    "\n",
    "  '''user reviews'''\n",
    "  # get all the reviews of the items that the user has purchased, except the review of the user on that item\n",
    "  user_reviews = [reviews[i] for i in user_indices if i != user_index]\n",
    "  # flatten user_reviews\n",
    "  user_reviews = [item for sublist in user_reviews for item in sublist]\n",
    "  \n",
    "\n",
    "  '''user category and price'''\n",
    "  # get all the categories the user has seen, and the most liked category of the user\n",
    "  user_categories = categories[users == user]\n",
    "  user_category = np.argmax(np.bincount(user_categories))\n",
    "\n",
    "  # get the average price of the user\n",
    "  user_prices = prices[users == user]\n",
    "  user_price = np.argmax(np.bincount(user_prices))\n",
    "\n",
    "  # apply function pad_sequence to the user_titles, user_reviews, item_reviews, title, with padding 0\n",
    "  review = pad_sequence(review, SINGLE_REVIEW_LIMIT)\n",
    "  user_reviews = pad_sequence(user_reviews, USER_REVIEW_LIMIT)\n",
    "  item_reviews = pad_sequence(item_reviews, ITEM_REVIEW_LIMIT)\n",
    "\n",
    "  return user, item, review, rating, category, price, item_reviews, user_reviews, user_category, user_price\n",
    "\n",
    "\n",
    "'''function to get a batch of training samples.\n",
    "this function selects a random user from a set of users\n",
    "and gets his information by function sample_user, repeat it for batch_size times and \n",
    "gets the batch of samples. Each element in the batch tuple is then transormed to a pytorch tensor and returned.'''\n",
    "def get_batch(users, items, reviews, categories, prices, ratings, batch_size=32, fixed_users_set = None):\n",
    "  # get a batch of users\n",
    "  if fixed_users_set is None:\n",
    "    batch_users = np.random.choice(users, size=batch_size)\n",
    "  else:\n",
    "    batch_users = fixed_users_set\n",
    "  # get the batch of samples\n",
    "  batch = [list(sample_user(user, users, items, reviews, categories, prices, ratings)) for user in batch_users]\n",
    "  # transform each column of the batch to a pytorch tensor\n",
    "  batch = [torch.tensor(sample) for sample in zip(*batch)]\n",
    "  return batch\n",
    "\n",
    "\n",
    "def weights_init(m):\n",
    "  if isinstance(m, nn.Linear):\n",
    "    nn.init.xavier_uniform_(m.weight.data)\n",
    "    nn.init.constant_(m.bias.data, 0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get a batch of training samples\n",
    "batch = get_batch(train_users, train_items, train_reviews, train_categories, train_prices, train_ratings, batch_size=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ref: https://github.com/zhongqiangwu960812/AI-RecommenderSystem/blob/master/Rank/DeepFM/DeepFM_Model.ipynb\n",
    "'''builds a factorisation machine that is used to predict the rating of an item by a user.\n",
    "parameters: latent_dim: the dimension of the latent factors.\n",
    "            fea_num: the number of features.'''\n",
    "            \n",
    "class FM(nn.Module):\n",
    "    def __init__(self, latent_dim, fea_num):\n",
    "        super().__init__()\n",
    "\n",
    "        self.latent_dim = latent_dim\n",
    "        self.w0 = nn.Parameter(torch.zeros([1, ]))\n",
    "        self.w1 = nn.Parameter(torch.rand([fea_num, 1]))\n",
    "        self.w2 = nn.Parameter(torch.rand([fea_num, latent_dim]))\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        # inputs = inputs.long()\n",
    "        first_order = self.w0 + torch.mm(inputs, self.w1)\n",
    "        second_order = 1/2 * torch.sum(\n",
    "            torch.pow(torch.mm(inputs, self.w2), 2) -\n",
    "            torch.mm(torch.pow(inputs, 2), torch.pow(self.w2, 2)),\n",
    "\n",
    "            dim=1,\n",
    "            keepdim=True\n",
    "        )\n",
    "\n",
    "        return first_order + second_order\n",
    "\n",
    "\n",
    "class DeepFM(nn.Module):\n",
    "  def __init__(self, fea_num, latent_dim):\n",
    "    super().__init__()\n",
    "    self.w0 = nn.Parameter(torch.zeros([1, ]))\n",
    "    self.w1 = nn.Parameter(torch.rand([fea_num, 1]))\n",
    "    self.w2 = nn.Parameter(torch.rand([fea_num, latent_dim]))\n",
    "\n",
    "    self.mlp = nn.Sequential(\n",
    "            nn.Linear(latent_dim, 8*latent_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.5),\n",
    "            nn.Linear(8*latent_dim, 4*latent_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.5),\n",
    "            nn.Linear(4*latent_dim, 2*latent_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.5),\n",
    "            nn.Linear(2*latent_dim, latent_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.5),\n",
    "            nn.Linear(latent_dim, 1),\n",
    "            nn.ReLU(),\n",
    "        )\n",
    "\n",
    "  def forward(self, inputs):\n",
    "    '''FM'''\n",
    "    fm_first_order = self.w0 + torch.mm(inputs, self.w1)\n",
    "    fm_second_order = 1/2 * torch.sum(\n",
    "        torch.pow(torch.mm(inputs, self.w2), 2)\n",
    "        -\n",
    "        torch.mm(torch.pow(inputs, 2), torch.pow(self.w2, 2)),\n",
    "        dim=1,\n",
    "        keepdim=True\n",
    "    )\n",
    "    fm_out = fm_first_order + fm_second_order\n",
    "\n",
    "    '''MLP'''\n",
    "    mlp_x = torch.mm(inputs, self.w2)\n",
    "    mlp_out = self.mlp(mlp_x)\n",
    "\n",
    "    return torch.add(fm_out, mlp_out ) /2\n",
    "\n",
    "\n",
    "class GMF(nn.Module):\n",
    "    def __init__(self, inp_range, latent_dim=20, dropout=True):\n",
    "        super().__init__()\n",
    "        self.emb = nn.Embedding(\n",
    "            num_embeddings=inp_range, embedding_dim=latent_dim)\n",
    "        self.dropout = nn.Dropout(0.5)\n",
    "        self.use_dropout = dropout\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        embedding = self.emb(inputs)\n",
    "        if self.use_dropout:\n",
    "          embedding = self.dropout(embedding)\n",
    "\n",
    "        return embedding\n",
    "\n",
    "# ref: https://zh.d2l.ai/chapter_natural-language-processing-applications/sentiment-analysis-cnn.html\n",
    "\n",
    "# builds a cnn for text classification  \n",
    "\n",
    "\n",
    "class TextCNN(nn.Module):\n",
    "    def __init__(self, vocab_size, embed_size, kernel_sizes, num_neurons, latent_dim):\n",
    "        super(TextCNN, self).__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, embed_size)\n",
    "        self.dropout = nn.Dropout(0.5)\n",
    "        self.decoder = nn.Linear(sum(num_neurons), latent_dim)\n",
    "        self.pool = nn.AdaptiveMaxPool1d(1)\n",
    "        self.tanh = nn.Tanh()\n",
    "        self.convs = nn.ModuleList()\n",
    "        for c, k in zip(num_neurons, kernel_sizes):\n",
    "            self.convs.append(nn.Conv1d(embed_size, c, k))\n",
    "\n",
    "        embeds = w2v[tokens]\n",
    "        self.embedding.weight.data.copy_(embeds)\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        embeddings = self.embedding(inputs)\n",
    "        embeddings = embeddings.permute(0, 2, 1)\n",
    "        encoding = torch.cat([\n",
    "            torch.squeeze(self.tanh(self.pool(conv(embeddings))), dim=-1)\n",
    "            for conv in self.convs], dim=1)\n",
    "        outputs = self.decoder(self.dropout(encoding))\n",
    "        return outputs\n",
    "\n",
    "\n",
    "class TransformMLP(nn.Module):\n",
    "  def __init__(self, concated_size, latent_vector_size):\n",
    "      super().__init__()\n",
    "      self.net = nn.Sequential(\n",
    "          nn.Linear(concated_size, 2*concated_size),\n",
    "          nn.Tanh(),\n",
    "          nn.Linear(2*concated_size, latent_vector_size),\n",
    "          nn.Tanh()\n",
    "      )\n",
    "      self.dropout = nn.Dropout(0.5)\n",
    "\n",
    "  def forward(self, x, batch_size=32):\n",
    "    out = self.dropout(self.net(x))\n",
    "    out = out.view(batch_size, out.shape[1])\n",
    "    return out\n",
    "\n",
    "\n",
    "textCNN_I = TextCNN(len(tokens), 50, [3], [100], 50).apply(weights_init).to(device)\n",
    "textCNN_U = TextCNN(len(tokens), 50, [3], [100], 50).apply(\n",
    "    weights_init).to(device)\n",
    "textCNN_T = TextCNN(len(tokens), 50, [3], [100], 50).apply(\n",
    "    weights_init).to(device)\n",
    "\n",
    "\n",
    "transform = TransformMLP(100, 50).apply(weights_init).to(device)\n",
    "fm_T = FM(8, 50).apply(weights_init).to(device)\n",
    "fm_S = DeepFM(156, 8).apply(weights_init).to(device)\n",
    "\n",
    "# user latent vectors -> this embedding should be updated\n",
    "mf_u = GMF(max(train_users)+1, 25, True).apply(weights_init).to(device)\n",
    "# item latent vectors -> this embedding should be updated\n",
    "mf_i = GMF(max(max(train_items), max(test_items))+1,\n",
    "           25, True).apply(weights_init).to(device)\n",
    "\n",
    "'''optimisers for the models, with weight decay of 0.01, and learning rate of lr'''\n",
    "optimiser_textCNN_I = torch.optim.Adam(textCNN_I.parameters(), lr=lr, weight_decay=0.01)\n",
    "optimiser_textCNN_U = torch.optim.Adam(textCNN_U.parameters(), lr=lr, weight_decay=0.01)\n",
    "optimiser_textCNN_T = torch.optim.Adam(textCNN_T.parameters(), lr=lr, weight_decay=0.01)\n",
    "optimiser_transform = torch.optim.Adam(transform.parameters(), lr=lr, weight_decay=0.01)\n",
    "optimiser_fm_T = torch.optim.Adam(fm_T.parameters(), lr=lr, weight_decay=0.01)\n",
    "optimiser_fm_S = torch.optim.Adam(fm_S.parameters(), lr=lr, weight_decay=0.01)\n",
    "optimiser_mf_u = torch.optim.Adam(mf_u.parameters(), lr=lr)\n",
    "optimiser_mf_i = torch.optim.Adam(mf_i.parameters(), lr=lr)\n",
    "\n",
    "'''function that saves the training, path is the path to the folder where the model will be saved'''\n",
    "def save_training(path):\n",
    "    torch.save(textCNN_I.state_dict(), path + 'textCNN_I.pth')\n",
    "    torch.save(textCNN_U.state_dict(), path + 'textCNN_U.pth')\n",
    "    torch.save(textCNN_T.state_dict(), path + 'textCNN_T.pth')\n",
    "    torch.save(transform.state_dict(), path + 'transform.pth')\n",
    "    torch.save(fm_T.state_dict(), path + 'fm_T.pth')\n",
    "    torch.save(fm_S.state_dict(), path + 'fm_S.pth')\n",
    "    torch.save(mf_u.state_dict(), path + 'mf_u.pth')\n",
    "    torch.save(mf_i.state_dict(), path + 'mf_i.pth')\n",
    "\n",
    "'''function that loads the model, path is the path to the folder where the model is saved'''\n",
    "def load_training(path):\n",
    "    textCNN_I.load_state_dict(torch.load(path + 'textCNN_I.pth'))\n",
    "    textCNN_U.load_state_dict(torch.load(path + 'textCNN_U.pth'))\n",
    "    textCNN_T.load_state_dict(torch.load(path + 'textCNN_T.pth'))\n",
    "    transform.load_state_dict(torch.load(path + 'transform.pth'))\n",
    "    fm_T.load_state_dict(torch.load(path + 'fm_T.pth'))\n",
    "    fm_S.load_state_dict(torch.load(path + 'fm_S.pth'))\n",
    "    mf_u.load_state_dict(torch.load(path + 'mf_u.pth'))\n",
    "    mf_i.load_state_dict(torch.load(path + 'mf_i.pth'))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def evaluation(test_batch_size = 32):\n",
    "  test_users_unique = list(set(test_users))\n",
    "  rmse_arr = []\n",
    "\n",
    "  for i in tqdm(range(0, len(test_users_unique), test_batch_size)):\n",
    "    test_users_batch = test_users_unique[i:i+test_batch_size]\n",
    "    batch_test = get_batch(test_users, test_items, test_reviews, test_categories, test_prices, test_ratings, fixed_users_set=test_users_batch)\n",
    "    user_test, item_test, review_test, rating_test, category_test, price_test, item_reviews_test, user_reviews_test, user_category_test, user_price_test = batch_test\n",
    "    user_test, item_test, review_test, rating_test, category_test, price_test, item_reviews_test, user_reviews_test, user_category_test, user_price_test = \\\n",
    "      user_test.to(device), item_test.to(device), review_test.to(device), rating_test.to(device), category_test.to(device), price_test.to(device), item_reviews_test.to(device), user_reviews_test.to(device), user_category_test.to(device), user_price_test.to(device)\n",
    "\n",
    "    latent_review_user = textCNN_U(user_reviews_test)\n",
    "    latent_review_item = textCNN_I(item_reviews_test)\n",
    "    latent_review_user_item = torch.cat((latent_review_user, latent_review_item), 1)\n",
    "    transform_construction = transform(latent_review_user_item, batch_size=len(test_users_batch))\n",
    "    # flatten the transform_construction\n",
    "    transform_construction = transform_construction.view(transform_construction.size(0), -1)\n",
    "\n",
    "    # get the latent representations of user id and item id\n",
    "    latent_user = mf_u(user_test)\n",
    "    latent_item = mf_i(item_test)\n",
    "\n",
    "    # one-hot user and item categories and prices\n",
    "    user_category_onehot = F.one_hot(user_category_test, num_classes=24)\n",
    "    item_category_onehot = F.one_hot(category_test, num_classes=24)\n",
    "\n",
    "    user_price_onehot = F.one_hot(user_price_test, num_classes=4)\n",
    "    item_price_onehot = F.one_hot(price_test, num_classes=4)\n",
    "\n",
    "    # get the latent vectors of user and item category and price\n",
    "    latent_user_item = torch.cat((transform_construction, latent_user, latent_item, user_category_onehot, item_category_onehot, user_price_onehot, item_price_onehot), 1)\n",
    "\n",
    "    prediction_source = fm_S(latent_user_item)\n",
    "\n",
    "    # calculate rooted mean square error\n",
    "    rmse_arr.append(torch.sqrt(torch.mean((prediction_source - rating_test)**2)).item())\n",
    "  \n",
    "  # return the mean of the rmse_arr\n",
    "  return np.mean(rmse_arr)\n",
    "\n",
    "print(\"Program: Evaluating the baseline RMSE of the model on the test set\")\n",
    "\n",
    "# set best_rmse to be the largest possible value\n",
    "best_rmse = evaluation()\n",
    "\n",
    "# print baseline best_rmse\n",
    "print('Baseline RMSE:', best_rmse)\n",
    "\n",
    "for epoch in range (max_epoch):\n",
    "  for i in tqdm(range(1000)):\n",
    "    batch =  get_batch(train_users, train_items, train_reviews, train_titles, train_categories, train_prices, train_ratings)\n",
    "    user, item, review, rating, category, price, item_reviews, user_reviews, user_category, user_price = batch\n",
    "    user, item, review, rating, category, price, item_reviews, user_reviews, user_category, user_price = user.to(device), item.to(device), review.to(device), rating.to(device), category.to(device), price.to(device), item_reviews.to(device), user_reviews.to(device), user_category.to(device), user_price.to(device)\n",
    "\n",
    "    '''Train target network'''\n",
    "    latent_review = textCNN_T(review)\n",
    "    latent_review_data = latent_review.data\n",
    "    prediction_target = fm_T(latent_review)\n",
    "    # calculate the L1 loss\n",
    "    loss_target = torch.mean(torch.abs(prediction_target - rating))\n",
    "\n",
    "    # backpropagation\n",
    "    optimiser_textCNN_T.zero_grad()\n",
    "    optimiser_fm_T.zero_grad()\n",
    "    loss_target.backward()\n",
    "    optimiser_fm_T.step()\n",
    "    optimiser_textCNN_T.step()\n",
    "\n",
    "    '''Train transform network'''\n",
    "    latent_review_user = textCNN_U(user_reviews)\n",
    "    latent_review_item = textCNN_I(item_reviews)\n",
    "\n",
    "    # concatenate the latent vectors and flatten them\n",
    "    latent_review_user_item = torch.cat((latent_review_user, latent_review_item), 1)\n",
    "    transform_construction = transform(latent_review_user_item)\n",
    "    transform_construction = transform_construction.view(transform_construction.size(0), -1)\n",
    "    # calculate the L2 loss between transform_construction and latent_review_data\n",
    "    loss_transform = torch.mean((transform_construction - latent_review_data) ** 2)\n",
    "\n",
    "    # backpropagation\n",
    "    optimiser_transform.zero_grad()\n",
    "    optimiser_textCNN_I.zero_grad()\n",
    "    optimiser_textCNN_U.zero_grad()\n",
    "    loss_transform.backward()\n",
    "    optimiser_textCNN_I.step()\n",
    "    optimiser_textCNN_U.step()\n",
    "    optimiser_transform.step()\n",
    "\n",
    "    '''Train source network'''\n",
    "    # get the latent representations of user id and item id\n",
    "    latent_user = mf_u(user)\n",
    "    latent_item = mf_i(item)\n",
    "\n",
    "    # one-hot user and item categories and prices\n",
    "    user_category_onehot = torch.zeros(user_category.shape[0], 24).to(device)\n",
    "    user_category_onehot.scatter_(1, user_category.unsqueeze(1), 1)\n",
    "    item_category_onehot = torch.zeros(category.shape[0], 24).to(device)\n",
    "    item_category_onehot.scatter_(1, category.unsqueeze(1), 1)\n",
    "\n",
    "    user_price_onehot = torch.zeros(user_price.shape[0], 4).to(device)\n",
    "    user_price_onehot.scatter_(1, user_price.unsqueeze(1), 1)\n",
    "    item_price_onehot = torch.zeros(price.shape[0], 4).to(device)\n",
    "    item_price_onehot.scatter_(1, price.unsqueeze(1), 1)\n",
    "\n",
    "    # concatenate the latent vectors and use them as input for the source network\n",
    "    latent_user_item = torch.cat((transform_construction, latent_user, latent_item, user_category_onehot, item_category_onehot, user_price_onehot, item_price_onehot), 1)\n",
    "    latent_user_item_data = latent_user_item.data\n",
    "    prediction_source = fm_S(latent_user_item_data)\n",
    "    \n",
    "    # calculate the L1 loss\n",
    "    loss_source = torch.mean(torch.abs(prediction_source - rating))\n",
    "\n",
    "    # backpropagation\n",
    "    optimiser_fm_S.zero_grad()\n",
    "    optimiser_mf_i.zero_grad()\n",
    "    optimiser_mf_u.zero_grad()\n",
    "    loss_source.backward()\n",
    "    optimiser_fm_S.step()\n",
    "    optimiser_mf_i.step()\n",
    "    optimiser_mf_u.step()\n",
    "\n",
    "  '''evaluate the model on the test set'''\n",
    "  rmse_test = evaluation()\n",
    "  # print out the loss of the models and the test rmse\n",
    "  print('Epoch:', epoch, 'Loss_target:', loss_target.item(), 'Loss_transform:', loss_transform.item(), 'Loss_source:', loss_source.item(), 'RMSE_test:', rmse_test)\n",
    "  if rmse_test < best_rmse:\n",
    "    best_rmse = rmse_test\n",
    "    # save_training(data_save_path+'/')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open (f'{analytics_path}/T-GloVe-L2-Meta-DeepFM.txt', 'w') as f:\n",
    "  for r in rmse_arr:\n",
    "    f.write(str(np.round(r, 4)) + '\\n')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.11 ('y3project')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.11"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "9af667c24e890afcb270fe85ca560e1aa788703788da50f20e13467161b45801"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
